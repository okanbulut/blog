[
  {
    "path": "posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/",
    "title": "Lexicon-Based Sentiment Analysis Using R",
    "description": "In this post, we will uncover the power of lexicon-based sentiment analysis using R. I demonstrate how to harness the capabilities of lexicons like NRC and Bing to decipher the emotional pulse of your text data. With practical examples, you'll gain the skills to analyze sentiment scores and extract valuable insights from your textual data sets.\n\n(12 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2024-02-09",
    "categories": [
      "data science",
      "natural language processing",
      "text mining",
      "sentiment"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nLexicon-Based Sentiment Analysis\r\nExample\r\nDescriptive Analysis\r\nSentiment Analysis with tidytext\r\nSentiment Analysis with sentimentr\r\n\r\nConcluding Remarks\r\n\r\nPhoto by Roman Odintsov on PexelsIntroduction\r\nDuring the COVID-19 pandemic, I decided to learn a new statistical technique to keep my mind occupied rather than constantly immersing myself in pandemic-related news. After evaluating several options, I found the concepts related to natural language processing (NLP) particularly captivating. So, I opted to delve deeper into this field and explore one specific technique: sentiment analysis, also known as “opinion mining” in academic literature. This analytical method empowers researchers to extract and interpret the emotions conveyed toward a specific subject within written text. Through sentiment analysis, one can discern the polarity (positive or negative), nature, and intensity of sentiments expressed across various textual formats such as documents, customer reviews, and social media posts.\r\nAmidst the pandemic, I observed a significant trend among researchers who turned to sentiment analysis as a tool to measure public responses to news and developments surrounding the virus. This involved analyzing user-generated content on popular social media platforms such as Twitter, YouTube, and Instagram. Intrigued by this methodology, my colleagues and I endeavored to contribute to the existing body of research by scrutinizing the daily briefings provided by public health authorities. In Alberta, Dr. Deena Hinshaw, who used to be the province’s chief medical officer of health, regularly delivered updates on the region’s response to the ongoing pandemic. Through our analysis of these public health announcements, we aimed to assess Alberta’s effectiveness in implementing communication strategies during this intricate public health crisis. Our investigation, conducted through the lenses of sentiment analysis, sought to shed light on the efficacy of communication strategies employed during this challenging period in public health (Bulut & Poth, 2022; Poth et al., 2021).\r\nIn this post, I aim to walk you through the process of performing sentiment analysis using R. Specifically, I’ll focus on “lexicon-based sentiment analysis,” which I’ll discuss in more detail in the next section. I’ll provide examples of lexicon-based sentiment analysis that we’ve integrated into the publications referenced earlier. Additionally, in future posts, I’ll delve into more advanced forms of sentiment analysis, making use of state-of-the-art pre-trained models accessible on Hugging Face.\r\nLexicon-Based Sentiment Analysis\r\nAs I learned more about sentiment analysis, I discovered that the predominant method for extracting sentiments is lexicon-based sentiment analysis. This approach entails utilizing a specific lexicon, essentially the vocabulary of a language or subject, to discern the direction and intensity of sentiments conveyed within a given text. Some lexicons, like the Bing lexicon (Hu & Liu, 2004), classify words as either positive or negative. Conversely, other lexicons provide more detailed sentiment labels, such as the NRC Emotion Lexicon (Mohammad & Turney, 2013), which categorizes words based on both positive and negative sentiments, as well as Plutchik’s (Plutchik, 1980) psych evolutionary theory of basic emotions (e.g., anger, fear, anticipation, trust, surprise, sadness, joy, and disgust).\r\nLexicon-based sentiment analysis operates by aligning words within a given text with those found in widely-used lexicons such as NRC and Bing. Each word receives an assigned sentiment, typically categorized as positive or negative. The text’s collective sentiment score is subsequently derived by summing the individual sentiment scores of its constituent words. For instance, in a scenario where a text incorporates 50 positive and 30 negative words according to the Bing lexicon, the resulting sentiment score would be 20. This value indicates a predominance of positive sentiments within the text. Conversely, a negative total would imply a prevalence of negative sentiments.\r\nPerforming lexicon-based sentiment analysis using R can be both fun and tricky at the same time. While analyzing public health announcements in terms of sentiments, I found Julia Silge and David Robinson’s book, Text Mining with R, to be very helpful. The book has a chapter dedicated to sentiment analysis, where the authors demonstrate how to conduct sentiment analysis using general-purpose lexicons like Bing and NRC. However, Julia and David also highlight a major limitation of lexicon-based sentiment analysis. The analysis considers only single words (i.e., unigrams) and does not consider qualifiers before a word. For instance, negation words like “not” in “not true” are ignored, and sentiment analysis processes them as two separate words, “not” and “true”. Furthermore, if a particular word (either positive or negative) is repeatedly used throughout the text, this may skew the results depending on the polarity (positive or negative) of this word. Therefore, the results of lexicon-based sentiment analysis should be interpreted carefully.\r\nNow, let’s move to our example where we will conduct lexicon-based sentiment analysis using Dr. Deena Hinshaw’s media briefings during the COVID-19 pandemic. My goal is to showcase two R packages capable of running sentiment analysis 📉.\r\nExample\r\nFor the sake of simplicity, we will focus on the first wave of the pandemic (March 2020 - June 2020). The transcripts of all media briefings were available in the government of Alberta’s COVID-19 pandemic website (https://www.alberta.ca/covid). After importing these transcripts into R, I turned all the text into lowercase and then applied word tokenization using the tidytext (Silge & Robinson, 2016) and tokenizers (Mullen et al., 2018) packages. Word tokenization split the sentences in the media briefings into individual words for each entry (i.e., day of media briefings). Next, I applied lemmatization to the tokens to resolve each word into its canonical form using the textstem package (Rinker, 2018). Finally, I removed common stopwords, such as “my”, “for”, “that”, “with”, and “for, using the stopwords package (Benoit et al., 2021). The final dataset is available here. Now, let’s import the data into R and then review its content.\r\n\r\n\r\nload(\"wave1_alberta.RData\")\r\n\r\nhead(wave1_alberta, 10)\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe dataset has three columns:\r\nmonth (the month of the media briefing)\r\ndate (the exact date of the media briefing), and\r\nword (words or tokens used in media briefing)\r\nDescriptive Analysis\r\nNow, we can calculate some descriptive statistics to better understand the content of our dataset. We will begin by finding the top 5 words (based on their frequency) for each month.\r\n\r\n\r\nlibrary(\"dplyr\")\r\n\r\nwave1_alberta %>%\r\n  group_by(month) %>%\r\n  count(word, sort = TRUE) %>%\r\n  slice_head(n = 5) %>%\r\n  as.data.frame()\r\n\r\n        month      word   n\r\n1  March 2020    health 199\r\n2  March 2020      care 102\r\n3  March 2020  continue 102\r\n4  March 2020    spread  87\r\n5  March 2020      test  86\r\n6  April 2020      test 156\r\n7  April 2020    health 146\r\n8  April 2020      care 145\r\n9  April 2020  continue 135\r\n10 April 2020    spread 129\r\n11   May 2020    health 135\r\n12   May 2020  continue 118\r\n13   May 2020      test 102\r\n14   May 2020    people  78\r\n15   May 2020    public  78\r\n16  June 2020      test 126\r\n17  June 2020    health  93\r\n18  June 2020  continue  69\r\n19  June 2020    people  57\r\n20  June 2020 community  43\r\n\r\nThe output shows that words such as health, continue, and test were commonly used in the media briefings across this 4-month period. We can also expand our list to the most common 10 words and view the results visually:\r\n\r\n\r\nlibrary(\"tidytext\")\r\nlibrary(\"ggplot2\")\r\n\r\nwave1_alberta %>%\r\n  group_by(month) %>%\r\n  count(word, sort = TRUE) %>%\r\n  # Find the top 10 words\r\n  slice_head(n = 10) %>%\r\n  ungroup() %>%\r\n  # Order the words by their frequency within each month\r\n  mutate(word = reorder_within(word, n, month)) %>%\r\n  # Create a bar graph\r\n  ggplot(aes(x = n, y = word, fill = month)) +\r\n  geom_col() +\r\n  scale_y_reordered() +\r\n  facet_wrap(~ month, scales = \"free_y\") +\r\n  labs(x = \"Frequency\", y = NULL) +\r\n  theme(legend.position = \"none\",\r\n        axis.text.x = element_text(size = 11),\r\n        axis.text.y = element_text(size = 11),\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 13))\r\n\r\n\r\n\r\nFigure 1: Most common words based on frequency\r\n\r\n\r\n\r\nSince some words are common across all four months, the plot above may not necessarily show us the important words that are unique to each month. To find such important words, we can use Term Frequency - Inverse Document Frequency (TF-IDF)–a widely used technique in NLP for measuring how important a term is within a document relative to a collection of documents (for more detailed information about TF-IDF, check out my previous blog post). In our example, we will treat media briefings for each month as a document and calculate TF-IDF for the tokens (i.e., words) within each document. The first part of the R codes below creates a new dataset, wave1_tf_idf, by calculating TF-IDF for all tokens and selecting the tokens with the highest TF-IDF values within each month. Next, we use this dataset to create a bar plot with the TF-IDF values to view the common words unique to each month.\r\n\r\n\r\n# Calculate TF-IDF for the words for each month\r\nwave1_tf_idf <- wave1_alberta %>%\r\n  count(month, word, sort = TRUE) %>%\r\n  bind_tf_idf(word, month, n) %>%\r\n  arrange(month, -tf_idf) %>%\r\n  group_by(month) %>%\r\n  top_n(10) %>%\r\n  ungroup\r\n\r\n# Visualize the results\r\nwave1_tf_idf %>%\r\n  mutate(word = reorder_within(word, tf_idf, month)) %>%\r\n  ggplot(aes(word, tf_idf, fill = month)) +\r\n  geom_col(show.legend = FALSE) + \r\n  facet_wrap(~ month, scales = \"free\", ncol = 2) +\r\n  scale_x_reordered() +\r\n  coord_flip() +\r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 13),\r\n        axis.text.x = element_text(size = 11),\r\n        axis.text.y = element_text(size = 11)) +\r\n  labs(x = NULL, y = \"TF-IDF\")\r\n\r\n\r\n\r\nFigure 2: Most common words based on TIF-IDF\r\n\r\n\r\n\r\nThese results are more informative because the tokens shown in the figure reflect unique topics discussed each month. For example, in March 2020, the media briefings were mostly about limiting the travels, returning from crowded conferences, and COVID-19 cases in cruise ships. In June 2020, the focus of the media briefings shifted towards mask requirements, people protesting pandemic-related restrictions, and so on. Before we switch back to the sentiment analysis, let’s take a look at another descriptive variable: the length of each media briefing. This will show us whether the media briefings became longer or shorter over time.\r\n\r\n\r\nwave1_alberta %>%\r\n  mutate(day = substr(date, 9, 10)) %>%\r\n  group_by(month, day) %>%\r\n  summarize(n = n()) %>%\r\n  ggplot(aes(day, n, color = month, shape = month, group = month)) +\r\n  geom_point(size = 2) + \r\n  geom_line() + \r\n  labs(x = \"Days\", y = \"Number of Words\") +\r\n  theme(legend.position = \"none\", \r\n        axis.text.x = element_text(angle = 90, size = 11),\r\n        strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 11),\r\n        axis.text.y = element_text(size = 11)) +\r\n  ylim(0, 800) +\r\n  facet_wrap(~ month, scales = \"free_x\")\r\n\r\n\r\n\r\nFigure 3: Number of words by days\r\n\r\n\r\n\r\nThe figure above shows that the length of media briefings varied quite substantially over time. Especially in March and May, there are larger fluctuations (i.e., very long or short briefings), whereas in June, the daily media briefings are quite similar in terms of length.\r\nSentiment Analysis with tidytext\r\nAfter analyzing the dataset descriptively, we are ready to begin with the sentiment analysis. In the first part, we will use the tidytext package for performing sentiment analysis and computing sentiment scores. We will first import the lexicons into R and then merge them with our dataset. Using the Bing lexicon, we need to find the difference between the number of positive and negative words to produce a sentiment score (i.e., sentiment = the number of positive words - the number of negative words).\r\n\r\n\r\n# From the three lexicons, Bing is already available in the tidytext page\r\n# for AFINN and NRC, install the textdata package by uncommenting the next line\r\n# install.packages(\"textdata\")\r\nget_sentiments(\"bing\") \r\n\r\n# A tibble: 6,786 × 2\r\n   word        sentiment\r\n   <chr>       <chr>    \r\n 1 2-faces     negative \r\n 2 abnormal    negative \r\n 3 abolish     negative \r\n 4 abominable  negative \r\n 5 abominably  negative \r\n 6 abominate   negative \r\n 7 abomination negative \r\n 8 abort       negative \r\n 9 aborted     negative \r\n10 aborts      negative \r\n# ℹ 6,776 more rows\r\n\r\nget_sentiments(\"afinn\") \r\n\r\n# A tibble: 2,477 × 2\r\n   word       value\r\n   <chr>      <dbl>\r\n 1 abandon       -2\r\n 2 abandoned     -2\r\n 3 abandons      -2\r\n 4 abducted      -2\r\n 5 abduction     -2\r\n 6 abductions    -2\r\n 7 abhor         -3\r\n 8 abhorred      -3\r\n 9 abhorrent     -3\r\n10 abhors        -3\r\n# ℹ 2,467 more rows\r\n\r\nget_sentiments(\"nrc\")\r\n\r\n# A tibble: 13,901 × 2\r\n   word        sentiment\r\n   <chr>       <chr>    \r\n 1 abacus      trust    \r\n 2 abandon     fear     \r\n 3 abandon     negative \r\n 4 abandon     sadness  \r\n 5 abandoned   anger    \r\n 6 abandoned   fear     \r\n 7 abandoned   negative \r\n 8 abandoned   sadness  \r\n 9 abandonment anger    \r\n10 abandonment fear     \r\n# ℹ 13,891 more rows\r\n\r\n# We will need the spread function from tidyr\r\nlibrary(\"tidyr\")\r\n\r\n# Sentiment scores with bing (based on frequency)\r\nwave1_alberta %>%\r\n  mutate(day = substr(date, 9, 10)) %>%\r\n  group_by(month, day) %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n  count(month, day, sentiment) %>%\r\n  spread(sentiment, n) %>%\r\n  mutate(sentiment = positive - negative) %>%\r\n  ggplot(aes(day, sentiment, fill = month)) +\r\n  geom_col(show.legend = FALSE) +\r\n  labs(x = \"Days\", y = \"Sentiment Score\") +\r\n  ylim(-50, 50) + \r\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 90)) +\r\n  facet_wrap(~ month, ncol = 2, scales = \"free_x\") +\r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 11),\r\n        axis.text.x = element_text(size = 11),\r\n        axis.text.y = element_text(size = 11)) \r\n\r\n\r\n\r\nFigure 4: Sentiment scores based on the Bing lexicon\r\n\r\n\r\n\r\nThe figure above shows that the sentiments delivered in the media briefings were generally negative, which is not necessarily surprising since the media briefings were all about how many people passed away, hospitalization rates, potential outbreaks, etc. On certain days (e.g., March 24, 2020 and May 4, 2020), the media briefings were particularly more negative in terms of sentiments.\r\nNext, we will use the AFINN lexicon. Unlike Bing that labels words as positive or negative, AFINN assigns a numerical weight to each word. The sign of the weight indicates the polarity of sentiments (i.e., positive or negative) while the value indicates the intensity of sentiments. Now, let’s see if these weighted values produce different sentiment scores.\r\n\r\n\r\nwave1_alberta %>%\r\n  mutate(day = substr(date, 9, 10)) %>%\r\n  group_by(month, day) %>%\r\n  inner_join(get_sentiments(\"afinn\")) %>%\r\n  group_by(month, day) %>%\r\n  summarize(sentiment = sum(value),\r\n            type = ifelse(sentiment >= 0, \"positive\", \"negative\")) %>%\r\n  ggplot(aes(day, sentiment, fill = type)) +\r\n  geom_col(show.legend = FALSE) +\r\n  labs(x = \"Days\", y = \"Sentiment Score\") +\r\n  ylim(-100, 100) + \r\n  facet_wrap(~ month, ncol = 2, scales = \"free_x\") +\r\n  theme(legend.position = \"none\", \r\n        strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 11),\r\n        axis.text.x = element_text(size = 11, angle = 90),\r\n        axis.text.y = element_text(size = 11))\r\n\r\n\r\n\r\nFigure 5: Sentiment scores based on the AFINN lexicon\r\n\r\n\r\n\r\nThe results based on the AFINN lexicon seem to be quite different! Once we take the “weight” of the tokens into account, most media briefings turn out to be positive (see the green bars), although there are still some days with negative sentiments (see the red bars). The two analyses we have done so far have yielded very different for two reasons. First, as I mentioned above, the Bing lexicon focuses on the polarity of the words but ignore the intensity of the words (dislike and hate are considered negative words with equal intensity). Unlike the Bing lexicon, the AFINN lexicon takes the intensity into account, which impacts the calculation of the sentiment scores. Second, the Bing lexicon (6786 words) is fairly larger than the AFINN lexicon (2477 words). Therefore, it is likely that some tokens in the media briefings are included in the Bing lexicon, but not in the AFINN lexicon. Disregarding those tokens might have impacted the results.\r\nThe final lexicon we are going to try using the tidytext package is NRC. As I mentioned earlier, this lexicon uses uses Plutchik’s (Plutchik, 1980) psych evolutionary theory to label the tokens based on basic emotions such as anger, fear, and anticipation. We are going to count the number of words or token associated with each emotion and then visualize the results.\r\n\r\n\r\nwave1_alberta %>%\r\n  mutate(day = substr(date, 9, 10)) %>%\r\n  group_by(month, day) %>%\r\n  inner_join(get_sentiments(\"nrc\")) %>%\r\n  count(month, day, sentiment) %>%\r\n  group_by(month, sentiment) %>%\r\n  summarize(n_total = sum(n)) %>%\r\n  ggplot(aes(n_total, sentiment, fill = sentiment)) +\r\n  geom_col(show.legend = FALSE) +\r\n  labs(x = \"Frequency\", y = \"\") +\r\n  xlim(0, 2000) + \r\n  facet_wrap(~ month, ncol = 2, scales = \"free_x\") +\r\n  theme(strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 11),\r\n        axis.text.x = element_text(size = 11),\r\n        axis.text.y = element_text(size = 11))\r\n\r\n\r\n\r\nFigure 6: Sentiment scores based on the NRC lexicon\r\n\r\n\r\n\r\nThe figure shows that the media briefings are mostly positive each month. Dr. Hinshaw used words associated with “trust”, “anticipation”, and “fear”. Overall, the pattern of these emotions seems to remain very similar over time, indicating the consistency of the media briefings in terms of the type and intensity of the emotions delivered.\r\nSentiment Analysis with sentimentr\r\nAnother package for lexicon-based sentiment analysis is sentimentr (Rinker, 2021). Unlike the tidytext package, this package takes valence shifters (e.g., negation) into account, which can easily flip the polarity of a sentence with one word. For example, the sentence “I am not unhappy” is actually positive but if we analyze it word by word, the sentence may seem to have a negative sentiment due to the words, “not” and “unhappy”. Similarly, “I hardly like this book” is a negative sentence but the analysis of individual words, “hardly” and “like”, may yield a positive sentiment score. The sentimentr package addresses the limitations around sentiment detection with valence shifters (see the package author Tyler Rinker’s Github page for further details on sentimentr: https://github.com/trinker/sentimentr).\r\nTo benefit from the sentimentr package, we need the actual sentences in the media briefings rather than the individual tokens. Therefore, I had to create an untokenized version of the dataset, which is available here. We will first import this dataset into R, get individual sentences for each media briefing using the get_sentences() function, and then calculate sentiment scores by day and month via sentiment_by().\r\n\r\n\r\nlibrary(\"sentimentr\")\r\nlibrary(\"magrittr\")\r\n\r\nload(\"wave1_alberta_sentence.RData\")\r\n\r\n# Calculate sentiment scores by day and month\r\nwave1_sentimentr <- wave1_alberta_sentence %>%\r\n  mutate(day = substr(date, 9, 10)) %>%\r\n  get_sentences() %$%\r\n  sentiment_by(text, list(month, day))\r\n\r\n# View the dataset\r\nhead(wave1_sentimentr, 10)\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn the dataset we created, “ave_sentiment” is the average sentiment score for each day in March, April, May, and June (i.e., days where a media briefing was made). Using this dataset, we can visualize the sentiment scores.\r\n\r\n\r\nwave1_sentimentr %>%\r\n  group_by(month, day) %>%\r\n  ggplot(aes(day, ave_sentiment, fill = ave_sentiment)) +\r\n  scale_fill_gradient(low=\"red\", high=\"blue\") + \r\n  geom_col(show.legend = FALSE) +\r\n  labs(x = \"Days\", y = \"Sentiment Score\") +\r\n  ylim(-0.1, 0.3) +\r\n  facet_wrap(~ month, ncol = 2, scales = \"free_x\") +\r\n  theme(legend.position = \"none\", \r\n        strip.background = element_blank(),\r\n        strip.text = element_text(colour = \"black\", face = \"bold\", size = 11),\r\n        axis.text.x = element_text(size = 11, angle = 90),\r\n        axis.text.y = element_text(size = 11))\r\n\r\n\r\n\r\nFigure 7: Sentiment scores based on sentimentr\r\n\r\n\r\n\r\nIn the figure above, the blue bars represent highly positive sentiment scores, while the red bars depict comparatively lower sentiment scores. The patterns observed in the sentiment scores generated by sentimentr closely resemble those derived from the AFINN lexicon. Notably, this analysis is based on the original media briefings rather than solely tokens, with consideration given to valence shifters in the computation of sentiment scores. The convergence between the sentiment patterns identified by sentimentr and those from AFINN is not entirely unexpected. Both approaches incorporate similar weighting systems and mechanisms that account for word intensity. This alignment reinforces our confidence in the initial findings obtained through AFINN, validating the consistency and reliability of our analyses with sentimentr.\r\nConcluding Remarks\r\nIn conclusion, lexicon-based sentiment analysis in R offers a powerful tool for uncovering the emotional nuances within textual data. Throughout this post, we have explored the fundamental concepts of lexicon-based sentiment analysis and provided a practical demonstration of its implementation using R. By leveraging packages such as sentimentr and tidytext, we have illustrated how sentiment analysis can be seamlessly integrated into your data analysis workflow. As you embark on your journey into sentiment analysis, remember that the insights gained from this technique extend far beyond the surface of text. They provide valuable perspectives on public opinion, consumer sentiment, and beyond. I encourage you to delve deeper into lexicon-based sentiment analysis, experiment with the examples presented here, and unlock the rich insights waiting to be discovered within your own data. Happy analyzing!\r\n\r\n\r\n\r\nBenoit, K., Muhr, D., & Watanabe, K. (2021). Stopwords: Multilingual stopword lists. https://CRAN.R-project.org/package=stopwords\r\n\r\n\r\nBulut, O., & Poth, C. N. (2022). Rapid assessment of communication consistency: Sentiment analysis of public health briefings during the COVID-19 pandemic. AIMS Public Health, 9(2), 293–306. https://doi.org/10.3934/publichealth.2022020\r\n\r\n\r\nHu, M., & Liu, B. (2004). Mining and summarizing customer reviews. Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 168–177.\r\n\r\n\r\nMohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word–emotion association lexicon. Computational Intelligence, 29(3), 436–465.\r\n\r\n\r\nMullen, L. A., Benoit, K., Keyes, O., Selivanov, D., & Arnold, J. (2018). Fast, consistent tokenization of natural language text. Journal of Open Source Software, 3, 655. https://doi.org/10.21105/joss.00655\r\n\r\n\r\nPlutchik, R. (1980). A general psychoevolutionary theory of emotion. In Theories of emotion (pp. 3–33). Elsevier.\r\n\r\n\r\nPoth, C. N., Bulut, O., Aquilina, A. M., & Otto, S. J. G. (2021). Using data mining for rapid complex case study descriptions: Example of public health briefings during the onset of the COVID-19 pandemic. Journal of Mixed Methods Research, 15(3), 348–373. https://doi.org/10.1177/15586898211013925\r\n\r\n\r\nRinker, T. W. (2018). textstem: Tools for stemming and lemmatizing text. http://github.com/trinker/textstem\r\n\r\n\r\nRinker, T. W. (2021). sentimentr: Calculate text polarity sentiment. https://github.com/trinker/sentimentr\r\n\r\n\r\nSilge, J., & Robinson, D. (2016). Tidytext: Text mining and analysis using tidy data principles in r. JOSS, 1(3). https://doi.org/10.21105/joss.00037\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/emoji.jpg",
    "last_modified": "2024-02-09T10:11:54-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-04-introduction-to-psychometric-network-analysis/",
    "title": "Introduction to Psychometric Network Analysis",
    "description": "Psychometric network analysis (PNA), also known as network psychometrics, has emerged as a new framework to analyze the relationships among observed variables to better understand the underlying structure of psychological phenomena. Psychometric network models are particularly useful when dealing with complex and multidimensional data. In this post, I provide a brief introduction to psychometric network models, particularly Gaussian graphical models (GGMs), and demonstrate how to estimate them using R. \n\n(7 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2024-01-04",
    "categories": [
      "psychometrics",
      "network",
      "correlation"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nConclusion\r\n\r\nPhoto by Omar Flores on UnsplashIntroduction\r\nPsychometric Network Analysis (PNA), which is also referred to as “network psychometrics”, is an emerging field that combines concepts from psychometrics and network analysis to study the relationships among psychological variables. Generally, psychometrics involves the measurement of latent traits, while network analysis focuses on modeling and analyzing complex systems of interconnected elements. PNA aims to represent the relationships between psychological constructs, such as symptoms, traits, or behaviors, as a network of interconnected nodes. The figure below shows a network graph for the Big Five Personality Test:\r\n\r\n\r\n\r\nFigure 1: Figure from Epskamp (2017)\r\n\r\n\r\n\r\nEach node represents a specific personality trait, and the edges (connections) between nodes reflect the statistical relationships between those variables. The edges can be weighted to represent the strength (e.g., thicker edges represents stronger associations) and direction of the relationships between the corresponding traits (e.g., green lines for positive and red lines for negative associations).\r\nIn social sciences (especially in psychology and education), researchers often use “partial correlation” coefficients to understand the associations between the variables. A partial correlation is the association between two [quantitative] variables, after conditioning on all other variables in the dataset. Networks based on partial correlations are known as Gaussian Graphical Models (GGMs; Costantini et al. (2015)). A GGM can be mathematically expressed as follows:\r\n\\[Σ=Δ(I−Ω)^{−1}Δ,\\]\r\nwhere \\(Σ\\) is the variance-covariance matrix of the observed variables, \\(Δ\\) is a diagonal matrix, \\(Ω\\) is a matrix containing weights linking observed variables, conditioned on all other variables, and \\(I\\) is the identity matrix.\r\nTo prevent over-interpretation in network structures, we want to limit the number of spurious connections (i.e., weak partial correlations due to sampling variation). It is possible to eliminate spurious connections using statistical regularization techniques, such as “least absolute shrinkage and selection operator” (LASSO; Tibshirani (1996)). LASSO (L1) regularization shrinks partial correlation coefficients when estimating a network model, which means that small coefficients are estimated to be exactly zero.\r\nAs an extension of LASSO, graphical LASSO, or shortly glasso (Friedman et al., 2007), involves a penalty parameter (\\(\\lambda\\)) to remove weak associations within a network. The following figure demonstrates the impact of the penalty parameter.\r\n\r\n\r\n\r\nFigure 2: Figure Adapted from Epskamp and Fried (2017)\r\n\r\n\r\n\r\nWhen estimating network models, glasso is typically combined with the extended Bayesian information criterion (EBIC; Chen & Chen (2008)) for tuning parameter selection, resulting in EBICglasso (Epskamp & Fried, 2018). If the dataset for PNA consists of continuous variables that are multivariate normally distributed, then we can estimate a GGM based on partial correlations with glasso or EBICglasso. GGM can also be used for ordinal data (e.g., Likert-scale data) wherein the network is based on the polychoric correlations instead of partial correlations.\r\nIn this blog post, I want to demonstrate how to perform PNA and visualize resulting network models (specifically, GGMs) using R 🕸 👩‍💻. I highly encourage readers to check out Network Psychometrics with R by Isvoranu et al. (2022) for a comprehensive discussion of network models and their estimation using R.\r\nLet’s get started 💪.\r\nExample\r\nIn our example, we will use a subset of the Synthetic Aperture Personality Assessment (SAPA)–a web based personality assessment project (https://www.sapa-project.org/). The purpose of SAPA is to find patterns among the vast number of ways that people differ from one another in terms of their thoughts, feelings, interests, abilities, desires, values, and preferences (Condon & Revelle, 2014; Revelle et al., 2010). The dataset consists of 16 SAPA items sampled from the full instrument (80 items). These items measure four subskills (i.e., verbal reasoning, letter series, matrix reasoning, and spatial rotations) as part of the general intelligence, also known as g factor. The “sapa_data.csv” dataset is a data frame with 1525 individuals who responded to 16 multiple-choice items in SAPA. The original dataset is included in the psych package (William Revelle, 2023). The dataset can be downloaded from here. Now, let’s import the data into R and then review its content.\r\n\r\n\r\n# Read the data in R\r\nsapa <- read.csv(\"sapa_data.csv\", header = TRUE)\r\n\r\n# Preview the data\r\nhead(data)\r\n\r\n\r\n\r\n\r\n\r\n\r\nNext, we will check out the correlations among the SAPA items. Since the items are dichotomous (i.e., binary), we will compute the tetrachoric correlations and then visualize the matrix using a correlation matrix plot.\r\n\r\n\r\nlibrary(\"psych\")\r\nlibrary(\"ggcorrplot\")  \r\n\r\n# Save the correlation matrix\r\ncormat <- psych::tetrachoric(x = sapa)$rho\r\n\r\n\r\n# Correlation matrix plot\r\nggcorrplot::ggcorrplot(corr = cormat, # correlation matrix\r\n                       type = \"lower\", # print only the lower part of the correlation matrix\r\n                       hc.order = TRUE, # hierarchical clustering\r\n                       show.diag = TRUE, # show the diagonal values of 1\r\n                       lab = TRUE, # add correlation values as labels\r\n                       lab_size = 3) # Size of the labels\r\n\r\n\r\n\r\nFigure 3: Correlation matrix plot of the SAPA items\r\n\r\n\r\n\r\nThe figure above shows that all of the SAPA items are positively correlated with each other. the items associated with the same construct (e.g., rotation) seem to have been clustered together (e.g., see the rotate.4, rotate.3, rotate.6, and rotate.8 as the top cluster). Now, we can go ahead and estimate a GGM based on this correlation matrix.\r\nWe will use the bootnet package (Epskamp, Borsboom, et al., 2018). This package has many methods to estimate GGMs. In our example, we will use “IsingFit” because the Ising model deal with binary data (see Epskamp, Maris, et al. (2018) and Marsman et al. (2018) for more details on the Ising network models). This model combines L1-regularized logistic regression with model selection based on EBIC.\r\n\r\n\r\nlibrary(\"bootnet\")\r\n\r\nnetwork_sapa_1 <- bootnet::estimateNetwork(\r\n  data = sapa, # dataset\r\n  default = \"IsingFit\", # for estimating GGM with the Ising model and EBIC,\r\n  verbose = FALSE # Ignore real-time updates on the estimation process\r\n)\r\n\r\n# View the estimated network\r\nplot(network_sapa_1, layout = \"spring\") \r\n\r\n\r\n\r\nFigure 4: Network plot for the Ising model\r\n\r\n\r\n\r\nFrom the network plot above, we can see that the items associated with the same construct often have stronger edges between the nodes (e.g., see the thick edges between the rotation items). These items also appear to be clustered closely within the network.\r\nWhat if we compute the polychoric correlations for the SAPA items using corMethod = \"cor_auto\" and estimate a GGM with glasso and EBIC?\r\n\r\n\r\nnetwork_sapa_2 <- bootnet::estimateNetwork(\r\n  data = sapa,\r\n  corMethod = \"cor_auto\", # for polychoric correlations\r\n  default = \"EBICglasso\", # for estimating GGM with glasso and EBIC\r\n  verbose = FALSE\r\n)\r\n\r\n# View the estimated network\r\nplot(network_sapa_2, layout = \"spring\") \r\n\r\n\r\n\r\nFigure 5: Network plot for the GGM\r\n\r\n\r\n\r\nBy default, the bootnet package uses tuning = 0.5 for the penalty parameter for EBICglasso. We can increase or decrease this parameter to adjust the penalty on the model (Note: tuning = 0 leads to model selection based on BIC instead of EBIC). In the following example, we will use tuning = 1 to apply a larger penalty parameter to the model.\r\n\r\n\r\nnetwork_sapa_3 <- bootnet::estimateNetwork(\r\n  data = sapa,\r\n  corMethod = \"cor_auto\", # for polychoric correlations\r\n  default = \"EBICglasso\", # for estimating GGM with glasso and EBIC\r\n  tuning = 1, \r\n  verbose = FALSE\r\n)\r\n\r\n# View the estimated network\r\nplot(network_sapa_3, layout = \"spring\") \r\n\r\n\r\n\r\nFigure 6: Network plot for the GGM after tuning\r\n\r\n\r\n\r\nThe final network plot includes fewer edges compared to the previous one with the default tuning parameter of 0.5. As the tuning parameter gets larger, more edges may be eliminated from the network, leading to a more compact and interpretable model.\r\nI also want to demonstrate the same analysis using the psychonetrics package (Epskamp, 2023). This package is more versatile than bootnet as it is capable of estimating both network models and structural equation models.\r\n\r\n\r\nlibrary(\"psychonetrics\")\r\nlibrary(\"dplyr\")\r\n\r\nnetwork_sapa_4 <- psychonetrics::ggm(data = sapa,\r\n                                     omega = \"full\", # or \"empty\" to set all elements to zero\r\n                                     delta = \"full\", # or \"empty\" to set all elements to zero\r\n                                     estimator = \"FIML\", # or \"ML\", \"ULS\", or \"DWLS\"\r\n                                     verbose = FALSE) %>%\r\n  psychonetrics::runmodel() # Run the model\r\n\r\n# View the model parameters\r\nnetwork_sapa_4 %>% \r\n  psychonetrics::parameters()\r\n\r\n\r\n Parameters for group fullsample\r\n    -  mu  \r\n      var1 op var2  est     se        p row col par\r\n  reason.4 ~1      0.64  0.012 < 0.0001   1   1   1\r\n reason.16 ~1      0.70  0.012 < 0.0001   2   1   2\r\n reason.17 ~1      0.70  0.012 < 0.0001   3   1   3\r\n reason.19 ~1      0.62  0.012 < 0.0001   4   1   4\r\n  letter.7 ~1      0.60  0.013 < 0.0001   5   1   5\r\n letter.33 ~1      0.57  0.013 < 0.0001   6   1   6\r\n letter.34 ~1      0.61  0.012 < 0.0001   7   1   7\r\n letter.58 ~1      0.44  0.013 < 0.0001   8   1   8\r\n matrix.45 ~1      0.53  0.013 < 0.0001   9   1   9\r\n matrix.46 ~1      0.55  0.013 < 0.0001  10   1  10\r\n matrix.47 ~1      0.61  0.012 < 0.0001  11   1  11\r\n matrix.55 ~1      0.37  0.012 < 0.0001  12   1  12\r\n  rotate.3 ~1      0.19  0.010 < 0.0001  13   1  13\r\n  rotate.4 ~1      0.21  0.010 < 0.0001  14   1  14\r\n  rotate.6 ~1      0.30  0.012 < 0.0001  15   1  15\r\n  rotate.8 ~1      0.19 0.0099 < 0.0001  16   1  16\r\n\r\n    -  omega (symmetric) \r\n      var1 op      var2     est    se        p row col par\r\n reason.16 --  reason.4   0.086 0.025  0.00072   2   1  17\r\n reason.17 --  reason.4    0.20 0.025 < 0.0001   3   1  18\r\n reason.19 --  reason.4   0.091 0.025  0.00034   4   1  19\r\n  letter.7 --  reason.4   0.064 0.026    0.012   5   1  20\r\n letter.33 --  reason.4  0.0015 0.026     0.95   6   1  21\r\n letter.34 --  reason.4   0.077 0.025   0.0024   7   1  22\r\n letter.58 --  reason.4   0.060 0.026    0.018   8   1  23\r\n matrix.45 --  reason.4    0.10 0.025 < 0.0001   9   1  24\r\n matrix.46 --  reason.4   0.054 0.026    0.033  10   1  25\r\n matrix.47 --  reason.4   0.025 0.026     0.33  11   1  26\r\n matrix.55 --  reason.4   0.013 0.026     0.61  12   1  27\r\n  rotate.3 --  reason.4   0.037 0.026     0.14  13   1  28\r\n  rotate.4 --  reason.4   0.034 0.026     0.18  14   1  29\r\n  rotate.6 --  reason.4   0.035 0.026     0.17  15   1  30\r\n  rotate.8 --  reason.4   0.034 0.026     0.19  16   1  31\r\n reason.17 -- reason.16    0.15 0.025 < 0.0001   3   2  32\r\n reason.19 -- reason.16   0.090 0.025  0.00041   4   2  33\r\n  letter.7 -- reason.16   0.097 0.025  0.00014   5   2  34\r\n letter.33 -- reason.16   0.018 0.026     0.48   6   2  35\r\n letter.34 -- reason.16   0.066 0.026    0.010   7   2  36\r\n letter.58 -- reason.16  0.0081 0.026     0.75   8   2  37\r\n matrix.45 -- reason.16   0.042 0.026    0.098   9   2  38\r\n matrix.46 -- reason.16   0.021 0.026     0.41  10   2  39\r\n matrix.47 -- reason.16   0.073 0.025   0.0039  11   2  40\r\n matrix.55 -- reason.16   0.046 0.026    0.074  12   2  41\r\n  rotate.3 -- reason.16  -0.027 0.026     0.30  13   2  42\r\n  rotate.4 -- reason.16   0.040 0.026     0.12  14   2  43\r\n  rotate.6 -- reason.16   0.043 0.026    0.094  15   2  44\r\n  rotate.8 -- reason.16   0.020 0.026     0.43  16   2  45\r\n reason.19 -- reason.17    0.15 0.025 < 0.0001   4   3  46\r\n  letter.7 -- reason.17   0.049 0.026    0.055   5   3  47\r\n letter.33 -- reason.17   0.055 0.026    0.031   6   3  48\r\n letter.34 -- reason.17   0.037 0.026     0.14   7   3  49\r\n letter.58 -- reason.17   0.068 0.026   0.0079   8   3  50\r\n matrix.45 -- reason.17  0.0052 0.026     0.84   9   3  51\r\n matrix.46 -- reason.17   0.046 0.026    0.075  10   3  52\r\n matrix.47 -- reason.17   0.092 0.025  0.00030  11   3  53\r\n matrix.55 -- reason.17   0.017 0.026     0.50  12   3  54\r\n  rotate.3 -- reason.17  -0.011 0.026     0.66  13   3  55\r\n  rotate.4 -- reason.17  -0.021 0.026     0.40  14   3  56\r\n  rotate.6 -- reason.17   0.092 0.025  0.00030  15   3  57\r\n  rotate.8 -- reason.17   0.012 0.026     0.64  16   3  58\r\n  letter.7 -- reason.19   0.045 0.026    0.078   5   4  59\r\n letter.33 -- reason.19   0.081 0.025   0.0015   6   4  60\r\n letter.34 -- reason.19   0.074 0.025   0.0039   7   4  61\r\n letter.58 -- reason.19   0.054 0.026    0.035   8   4  62\r\n matrix.45 -- reason.19   0.076 0.025   0.0030   9   4  63\r\n matrix.46 -- reason.19 0.00040 0.026     0.99  10   4  64\r\n matrix.47 -- reason.19   0.055 0.026    0.031  11   4  65\r\n matrix.55 -- reason.19   0.036 0.026     0.16  12   4  66\r\n  rotate.3 -- reason.19   0.013 0.026     0.60  13   4  67\r\n  rotate.4 -- reason.19   0.034 0.026     0.19  14   4  68\r\n  rotate.6 -- reason.19   0.014 0.026     0.57  15   4  69\r\n  rotate.8 -- reason.19  0.0054 0.026     0.83  16   4  70\r\n letter.33 --  letter.7    0.14 0.025 < 0.0001   6   5  71\r\n letter.34 --  letter.7    0.18 0.025 < 0.0001   7   5  72\r\n letter.58 --  letter.7    0.13 0.025 < 0.0001   8   5  73\r\n matrix.45 --  letter.7   0.013 0.026     0.60   9   5  74\r\n matrix.46 --  letter.7   0.070 0.025   0.0063  10   5  75\r\n matrix.47 --  letter.7   0.072 0.025   0.0049  11   5  76\r\n matrix.55 --  letter.7  0.0072 0.026     0.78  12   5  77\r\n  rotate.3 --  letter.7  -0.018 0.026     0.48  13   5  78\r\n  rotate.4 --  letter.7   0.060 0.026    0.019  14   5  79\r\n  rotate.6 --  letter.7   0.013 0.026     0.63  15   5  80\r\n  rotate.8 --  letter.7  -0.031 0.026     0.23  16   5  81\r\n letter.34 -- letter.33    0.18 0.025 < 0.0001   7   6  82\r\n letter.58 -- letter.33   0.070 0.025   0.0059   8   6  83\r\n matrix.45 -- letter.33   0.030 0.026     0.25   9   6  84\r\n matrix.46 -- letter.33   0.075 0.025   0.0034  10   6  85\r\n matrix.47 -- letter.33   0.038 0.026     0.14  11   6  86\r\n matrix.55 -- letter.33   0.063 0.026    0.013  12   6  87\r\n  rotate.3 -- letter.33   0.013 0.026     0.61  13   6  88\r\n  rotate.4 -- letter.33   0.029 0.026     0.26  14   6  89\r\n  rotate.6 -- letter.33   0.044 0.026    0.087  15   6  90\r\n  rotate.8 -- letter.33  -0.025 0.026     0.32  16   6  91\r\n letter.58 -- letter.34    0.11 0.025 < 0.0001   8   7  92\r\n matrix.45 -- letter.34   0.023 0.026     0.37   9   7  93\r\n matrix.46 -- letter.34   0.084 0.025  0.00092  10   7  94\r\n matrix.47 -- letter.34    0.11 0.025 < 0.0001  11   7  95\r\n matrix.55 -- letter.34  -0.027 0.026     0.30  12   7  96\r\n  rotate.3 -- letter.34   0.024 0.026     0.35  13   7  97\r\n  rotate.4 -- letter.34   0.014 0.026     0.59  14   7  98\r\n  rotate.6 -- letter.34  -0.014 0.026     0.58  15   7  99\r\n  rotate.8 -- letter.34 -0.0020 0.026     0.94  16   7 100\r\n matrix.45 -- letter.58   0.026 0.026     0.32   9   8 101\r\n matrix.46 -- letter.58   0.021 0.026     0.42  10   8 102\r\n matrix.47 -- letter.58   0.026 0.026     0.31  11   8 103\r\n matrix.55 -- letter.58    0.10 0.025 < 0.0001  12   8 104\r\n  rotate.3 -- letter.58   0.032 0.026     0.21  13   8 105\r\n  rotate.4 -- letter.58   0.046 0.026    0.073  14   8 106\r\n  rotate.6 -- letter.58   0.069 0.026   0.0065  15   8 107\r\n  rotate.8 -- letter.58   0.039 0.026     0.12  16   8 108\r\n matrix.46 -- matrix.45    0.22 0.024 < 0.0001  10   9 109\r\n matrix.47 -- matrix.45   0.089 0.025  0.00049  11   9 110\r\n matrix.55 -- matrix.45    0.10 0.025 < 0.0001  12   9 111\r\n  rotate.3 -- matrix.45   0.018 0.026     0.49  13   9 112\r\n  rotate.4 -- matrix.45   0.017 0.026     0.51  14   9 113\r\n  rotate.6 -- matrix.45  -0.027 0.026     0.29  15   9 114\r\n  rotate.8 -- matrix.45   0.032 0.026     0.21  16   9 115\r\n matrix.47 -- matrix.46   0.063 0.026    0.013  11  10 116\r\n matrix.55 -- matrix.46   0.011 0.026     0.68  12  10 117\r\n  rotate.3 -- matrix.46 -0.0057 0.026     0.82  13  10 118\r\n  rotate.4 -- matrix.46  0.0016 0.026     0.95  14  10 119\r\n  rotate.6 -- matrix.46   0.062 0.026    0.015  15  10 120\r\n  rotate.8 -- matrix.46   0.013 0.026     0.61  16  10 121\r\n matrix.55 -- matrix.47   0.089 0.025  0.00045  12  11 122\r\n  rotate.3 -- matrix.47   0.054 0.026    0.036  13  11 123\r\n  rotate.4 -- matrix.47   0.017 0.026     0.50  14  11 124\r\n  rotate.6 -- matrix.47     ~ 0 0.026      1.0  15  11 125\r\n  rotate.8 -- matrix.47   0.014 0.026     0.58  16  11 126\r\n  rotate.3 -- matrix.55   0.038 0.026     0.14  13  12 127\r\n  rotate.4 -- matrix.55   0.014 0.026     0.59  14  12 128\r\n  rotate.6 -- matrix.55   0.019 0.026     0.45  15  12 129\r\n  rotate.8 -- matrix.55   0.063 0.026    0.013  16  12 130\r\n  rotate.4 --  rotate.3    0.33 0.023 < 0.0001  14  13 131\r\n  rotate.6 --  rotate.3    0.16 0.025 < 0.0001  15  13 132\r\n  rotate.8 --  rotate.3    0.19 0.025 < 0.0001  16  13 133\r\n  rotate.6 --  rotate.4    0.19 0.025 < 0.0001  15  14 134\r\n  rotate.8 --  rotate.4    0.19 0.025 < 0.0001  16  14 135\r\n  rotate.8 --  rotate.6    0.20 0.025 < 0.0001  16  15 136\r\n\r\n    -  delta (diagonal) \r\n      var1  op      var2  est     se        p row col par\r\n  reason.4 ~/~  reason.4 0.41 0.0074 < 0.0001   1   1 137\r\n reason.16 ~/~ reason.16 0.40 0.0073 < 0.0001   2   2 138\r\n reason.17 ~/~ reason.17 0.38 0.0070 < 0.0001   3   3 139\r\n reason.19 ~/~ reason.19 0.42 0.0077 < 0.0001   4   4 140\r\n  letter.7 ~/~  letter.7 0.41 0.0075 < 0.0001   5   5 141\r\n letter.33 ~/~ letter.33 0.43 0.0078 < 0.0001   6   6 142\r\n letter.34 ~/~ letter.34 0.40 0.0073 < 0.0001   7   7 143\r\n letter.58 ~/~ letter.58 0.43 0.0078 < 0.0001   8   8 144\r\n matrix.45 ~/~ matrix.45 0.44 0.0081 < 0.0001   9   9 145\r\n matrix.46 ~/~ matrix.46 0.44 0.0080 < 0.0001  10  10 146\r\n matrix.47 ~/~ matrix.47 0.43 0.0078 < 0.0001  11  11 147\r\n matrix.55 ~/~ matrix.55 0.45 0.0082 < 0.0001  12  12 148\r\n  rotate.3 ~/~  rotate.3 0.32 0.0057 < 0.0001  13  13 149\r\n  rotate.4 ~/~  rotate.4 0.32 0.0058 < 0.0001  14  14 150\r\n  rotate.6 ~/~  rotate.6 0.38 0.0068 < 0.0001  15  15 151\r\n  rotate.8 ~/~  rotate.8 0.33 0.0059 < 0.0001  16  16 152\r\n\r\nWe can also see some information on the model fit, which can be used for making comparisons if we make any changes to the model (e.g., pruning).\r\n\r\n\r\n# Look at the model fit\r\nnetwork_sapa_4 %>% \r\n  psychonetrics::fit()\r\n\r\n           Measure     Value\r\n              logl -13298.10\r\n unrestricted.logl -13298.10\r\n     baseline.logl -15949.45\r\n              nvar        16\r\n              nobs       152\r\n              npar       152\r\n                df       ~ 0\r\n         objective    -11.94\r\n             chisq       ~ 0\r\n            pvalue         1\r\n    baseline.chisq   5302.71\r\n       baseline.df       120\r\n   baseline.pvalue       ~ 0\r\n               nfi         1\r\n              pnfi       ~ 0\r\n               tli          \r\n              nnfi         1\r\n               rfi          \r\n               ifi         1\r\n               rni         1\r\n               cfi         1\r\n             rmsea          \r\n    rmsea.ci.lower       ~ 0\r\n    rmsea.ci.upper       ~ 0\r\n      rmsea.pvalue       ~ 0\r\n            aic.ll  26900.19\r\n           aic.ll2  26934.10\r\n             aic.x       ~ 0\r\n            aic.x2       304\r\n               bic  27710.32\r\n              bic2  27227.45\r\n           ebic.25  28131.75\r\n            ebic.5  28553.18\r\n           ebic.75  28890.33\r\n             ebic1  29396.05\r\n\r\nAnother important feature of the ggm() function is that statistically insignificant edges can be removed to have a more compact model. In the following codes, we will eliminate edges that are not statistically significant at the alpha level of \\(\\alpha = .05\\), after performing a Benjamini and Hochberg (known as “BH” or “fdr”) correction (Benjamini & Hochberg, 1995). We will then compare this model with the previous model.\r\n\r\n\r\n# We can prune this model to remove insignificant edges\r\nnetwork_sapa_5 <- psychonetrics::ggm(data = sapa,\r\n                                     omega = \"full\", # or \"empty\" to set all elements to zero\r\n                                     delta = \"full\", # or \"empty\" to set all elements to zero\r\n                                     estimator = \"FIML\", # or \"ML\", \"ULS\", or \"DWLS\"\r\n                                     verbose = FALSE) %>%\r\n  psychonetrics::runmodel() %>%\r\n  psychonetrics::prune(adjust = \"fdr\", alpha = 0.05)\r\n\r\n# Compare the models\r\ncomparison <- psychonetrics::compare(\r\n  `1. Original model`  = network_sapa_4,\r\n  `2. Compact model after pruning` = network_sapa_5)\r\n\r\nprint(comparison)\r\n\r\n                          model DF      AIC      BIC RMSEA  Chisq\r\n              1. Original model  0 26900.19 27710.32          ~ 0\r\n 2. Compact model after pruning 73 26985.28 27406.33 0.038 231.09\r\n Chisq_diff DF_diff  p_value\r\n                            \r\n     231.09      73 < 0.0001\r\n\r\nNote: Chi-square difference test assumes models are nested.\r\n\r\nThe results show that the compact model with pruning fits the data better based on BIC although AIC shows the opposite result. In the final step, we will visualize the last network model resulting from ggm() using the famous qgraph package (Epskamp et al., 2012):\r\n\r\n\r\nlibrary(\"qgraph\")\r\n\r\n# Obtain the network plot\r\nnet5 <- getmatrix(network_sapa_5, \"omega\")\r\n\r\nqgraph::qgraph(net5, \r\n               layout = \"spring\", \r\n               theme = \"colorblind\",\r\n               labels = colnames(sapa))\r\n\r\n\r\n\r\nFigure 7: Network plot using qgraph\r\n\r\n\r\n\r\nConclusion\r\nThis was a very brief introduction to PNA using R. There are still many things to discuss in PNA. Thus, I plan to write several blog posts on PNA over the next few months. For example, I want to discuss centrality indices and demonstrate how to generate centrality indices for network models using R. I will also write a post on the Ising model for binary data to demonstrate the similarities and differences between an Ising network model and an item response theory (IRT) model.\r\n\r\n\r\n\r\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series B (Methodological), 57(1), 289–300. https://doi.org/https://doi.org/10.1111/j.2517-6161.1995.tb02031.x\r\n\r\n\r\nChen, J., & Chen, Z. (2008). Extended bayesian information criteria for model selection with large model spaces. Biometrika, 95(3), 759–771.\r\n\r\n\r\nCondon, D. M., & Revelle, W. (2014). The international cognitive ability resource: Development and initial validation of a public-domain measure. Intelligence, 43, 52–64.\r\n\r\n\r\nCostantini, G., Epskamp, S., Borsboom, D., Perugini, M., Mõttus, R., Waldorp, L. J., & Cramer, A. O. J. (2015). State of the aRt personality research: A tutorial on network analysis of personality data in r. Journal of Research in Personality, 54, 13–29. https://doi.org/https://doi.org/10.1016/j.jrp.2014.07.003\r\n\r\n\r\nEpskamp, S. (2023). Psychonetrics: Structural equation modeling and confirmatory network analysis. https://CRAN.R-project.org/package=psychonetrics\r\n\r\n\r\nEpskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: A tutorial paper. Behavior Research Methods, 50, 195–212.\r\n\r\n\r\nEpskamp, S., Cramer, A. O. J., Waldorp, L. J., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48(4), 1–18.\r\n\r\n\r\nEpskamp, S., & Fried, E. I. (2018). A tutorial on regularized partial correlation networks. Psychological Methods, 23(4), 617.\r\n\r\n\r\nEpskamp, S., Maris, G., Waldorp, L. J., & Borsboom, D. (2018). Network psychometrics. In The wiley handbook of psychometric testing (pp. 953–986). John Wiley & Sons, Ltd. https://doi.org/https://doi.org/10.1002/9781118489772.ch30\r\n\r\n\r\nFriedman, J., Hastie, T., & Tibshirani, R. (2007). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3), 432–441. https://doi.org/10.1093/biostatistics/kxm045\r\n\r\n\r\nIsvoranu, A.-M., Epskamp, S., Waldorp, L., & Borsboom, D. (2022). Network psychometrics with r: A guide for behavioral and social scientists. Routledge.\r\n\r\n\r\nMarsman, M., Borsboom, D., Kruis, J., Epskamp, S., Bork, R. van van, Waldorp, L., Maas, H. van der, & Maris, G. (2018). An introduction to network psychometrics: Relating ising network models to item response theory models. Multivariate Behavioral Research, 53(1), 15–35.\r\n\r\n\r\nRevelle, W., Wilt, J., & Rosenthal, A. (2010). Individual differences in cognition: New methods for examining the personality-cognition link. In Handbook of individual differences in cognition (pp. 27–49). Springer.\r\n\r\n\r\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. http://www.jstor.org/stable/2346178\r\n\r\n\r\nWilliam Revelle. (2023). Psych: Procedures for psychological, psychometric, and personality research. Northwestern University. https://CRAN.R-project.org/package=psych\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-04-introduction-to-psychometric-network-analysis/network.jpg",
    "last_modified": "2024-01-04T14:25:23-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-text-vectorization-using-python-word2vec/",
    "title": "Text Vectorization Using Python: Word2Vec",
    "description": "In the first two part of this series, we demonstrated how to convert text into numerical representation (i.e., text vectorization) using the term-document matrix and term frequency-inverse document frequency (TF-IDF) approaches. In the last part of the series, we focus on a more advanced approach, Word2Vec, that can capture the meaning and association of words within a text. First, we will briefly explain how Word2Vec works and then demonstrate how to use Word2Vec in Python.\n\n(7 min read)",
    "author": [
      {
        "name": "Sevilay Kilmen",
        "url": "https://akademik.yok.gov.tr/AkademikArama/view/viewAuthor.jsp"
      },
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2022-05-02",
    "categories": [
      "data science",
      "text mining",
      "text vectorization",
      "natural language processing",
      "python"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nWord2Vec\r\n\r\nExample\r\nConclusion\r\n\r\nPhoto by Skitterphoto on PexelsIntroduction\r\nAssume that you want to send a text message to a friend using your smartphone. After you typed the first word of your message, “Happy”, which word would the smart keyboard on your phone suggest for the next word? “Christmas” or “birthday”? It is quite likely that the smart keyboard will recommend the word “birthday”, instead of “Christmas”. Now, you have “Happy birthday” in the message. What would be the following words? At this point, it is not hard to guess that the smart keyword will suggest “to” and then “you” to turn the whole sentence to “Happy birthday to you”. But, how could the smart keyboard predict words one by one and help you create this sentence? How does it associate the word with each other? Or more broadly, when you have a Google search, how does Google come up with the most relevant website about a word or a phrase that you typed? To understand this magic, let’s step into the magical world of Word2Vec ✋.\r\nWord2Vec\r\nAs we explained in the last two posts, computers need numerical representations to analyze textual data. This process is called “text vectorization”. So far, we have talked about two text vectorization methods: term-document matrix and term frequency-inverse document frequency (TF-IDF). Both methods are very simple and easy-to-use when it comes to transforming textual data into numerical representations. In the last part of this series, we will focus on a more advanced approach, Word2Vec. Before we dive into what Word2Vec is and how it works, we need to define an important term, word embedding, which refers to an efficient and dense representation where words or phrases with similar meaning are closer in the vector space. In other words, a word embedding refers to a vector representation of a particular word or phrase in a multidimensional space. The vectorization of words or phrases as word embeddings facilitates the estimation of semantic similarities between different text materials (e.g., documents).\r\nThere are several word embedding techniques that are widely used in natural language processing (NLP) applications, such as Word2Vec, GloVe, and BERT. In this post, we will talk about Word2Vec developed by Tomas Mikolov and other researchers at Google for semantic analysis tasks. In the Word2Vec method, word vectors (i.e., target word) are constructed based on other words (i.e., context words) that are semantically similar to the target word. The number of context words coming before or after the target word is called “window size”. Now, let’s see two examples where there is a single word before and after the target word (i.e., window size = 1) and there are are two words before and after the target word (i.e., window size = 2):\r\n\r\n\r\n\r\nFigure 1: Input and output words based on window size=1\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Input and output words based on window size=2\r\n\r\n\r\n\r\nDepending on whether context words are used as either input or output, Word2Vec offers two neural network-based variants: Continuous Bag of Word (CBOW) and Skip-gram models. In the CBOW, context words are considered as input and the target word as output, whereas in the Skip-gram architecture, the target word is considered as the input and the context words as the output. The following example shows how input and output words are utilized within the CBOW and Skip-gram models.\r\n\r\n\r\n\r\nFigure 3: CBOW and Skipgram models\r\n\r\n\r\n\r\nLet’s assume that the word “two” in the sentence of “My brother is two years older than me” is the target word and the window size is two. In the CBOW model, “two” is considered as output, and the words “brother”, “is”, “years”, and “older” as input. In contrast, in the Skip-gram model, the word “two” is considered as the input, and the other words become the output. To further describe how the Word2Vec algorithm works, we will use real data (i.e., students’ written responses from an automated essay scoring competition) to prepare word embeddings using the Word2Vec algorithm in Python.\r\nExample\r\nIn this example, we will continue using the same Kaggle data set that comes from an automated essay scoring competition funded by the Hewlett Foundation. The data set includes students’ responses to ten different sets of short-answer items and scores assigned by two human raters. The data set is available here as a tab-separated value (TSV) file. The data set contains five variables:\r\nId: A unique identifier for each individual student essay\r\nEssaySet: An id for each set of essays (ranges from 1 to 10)\r\nScore1: Rater1’s score\r\nScore2: Rater2’s score\r\nEssayText: Students’ responses\r\nFor our demonstration, we will use student responses in “Essay Set 2” where students are presented with an investigation procedure to test four different polymer plastics for stretchability and data from that investigation. Students are asked to draw a conclusion based on the research data and describe two ways to improve the experimental design and/or validity of the results.\r\nNow, let’s begin our analysis by importing the Essay Set 2 into Python and format the data in such a way that it contains students’ responses under a single column called “response”.\r\n\r\n# Import required packages\r\nimport gensim\r\nfrom gensim.models import Word2Vec, KeyedVectors\r\nimport pandas as pd \r\n\r\n# Import train_rel_2.tsv into Python\r\nwith open('train_rel_2.tsv', 'r') as f:\r\n    lines = f.readlines()\r\n    columns = lines[0].split('\\t')\r\n    response = []\r\n    for line in lines[1:]:\r\n        temp = line.split('\\t') \r\n        if temp[1] == '2':   # Select the Essay Set 2\r\n            response.append(temp[-1])  # Select \"EssayText\" as a corpus\r\n        else: \r\n            None\r\n            \r\n# Construct a dataframe (\"data\") which includes only response column      \r\ndata = pd.DataFrame(list(zip(response))) \r\ndata.columns = ['response']\r\n\r\nWe can go ahead and review this data set.\r\n\r\nprint(data)\r\n                                               response\r\n0     Changing the type of grafin would improve the ...\r\n1     Concluding from the students data that plastic...\r\n2     Two ways that the stundent could've improved t...\r\n3     A conclusion I can make from this experiment i...\r\n4     a.One conclusion I can draw is that plastic B ...\r\n...                                                 ...\r\n1273  a) We can conclude that plastic B is the most ...\r\n1274  \"a. One conclusion I have fro this data was th...\r\n1275  3. The second trial(12) is not exactly the sam...\r\n1276  A) I have concluded that based on the students...\r\n1277  Plastic type B stretchable most 22mm in T1 & 2...\r\n\r\n[1278 rows x 1 columns]\r\n\r\nThe values at the bottom of the output show that the data set consists of 1278 documents and only one column (i.e., response) which we will use as a corpus in this example. To implement the Word2Vec algorithm, we first need to perform tokenization for all the words in this corpus. Since the corpus is pretty large, we will demonstrate the tokenization process using the first response, and then we will apply tokenization to the entire corpus. Now, let’s have a look at the first response more closely.\r\n\r\ndata.response[0]\r\n\"Changing the type of grafin would improve the student's experiment give a better new at those data. ^P Give the names of each type of plastic type used in this experiment. Each plastic should be the same length. ^P My conclusion is plastic type held up a much stronger than all of the different types.\\n\"\r\n\r\nUsing the simple_preprocess function from gensim.utils, we will tokenize the response:\r\n\r\nprint(gensim.utils.simple_preprocess(\"Changing the type of grafin would improve the student's experiment give a better new at those data. Give the names of each type of plastic type used in this experiment. Each plastic should be the same length. My conclusion is plastic type held up a much stronger than all of the different types\"))\r\n['changing', 'the', 'type', 'of', 'grafin', 'would', 'improve', 'the', 'student', 'experiment', 'give', 'better', 'new', 'at', 'those', 'data', 'give', 'the', 'names', 'of', 'each', 'type', 'of', 'plastic', 'type', 'used', 'in', 'this', 'experiment', 'each', 'plastic', 'should', 'be', 'the', 'same', 'length', 'my', 'conclusion', 'is', 'plastic', 'type', 'held', 'up', 'much', 'stronger', 'than', 'all', 'of', 'the', 'different', 'types']\r\n\r\nAs the output shows, after tokenization, all the words in the response are separated into smaller units (i.e., tokens). In the following analysis, each word in the corpus will be handled separately in the Word2Vec process. Now, let’s repeat the same procedure for the entire corpus.\r\n\r\nresponse_new=data.response.apply(gensim.utils.simple_preprocess)\r\nresponse_new\r\n0       [changing, the, type, of, grafin, would, impro...\r\n1       [concluding, from, the, students, data, that, ...\r\n2       [two, ways, that, the, stundent, could, ve, im...\r\n3       [conclusion, can, make, from, this, experiment...\r\n4       [one, conclusion, can, draw, is, that, plastic...\r\n                              ...                        \r\n1273    [we, can, conclude, that, plastic, is, the, mo...\r\n1274    [one, conclusion, have, fro, this, data, was, ...\r\n1275    [the, second, trial, is, not, exactly, the, sa...\r\n1276    [have, concluded, that, based, on, the, studen...\r\n1277    [plastic, type, stretchable, most, mm, in, mm,...\r\nName: response, Length: 1278, dtype: object\r\n\r\nAfter tokenization is completed for the entire corpus, we can now create a word embedding model using Word2Vec:\r\n\r\n# Model parameters\r\nmodel=gensim.models.Word2Vec(window=5, min_count=2, workers=4, sg=0)\r\n\r\n# Train the model\r\nmodel.build_vocab(response_new, progress_per=1000)\r\nmodel.train(response_new, total_examples=model.corpus_count, epochs=model.epochs)\r\n\r\n# Save the trained model\r\nmodel.save(\"./responses.model\")\r\n\r\nThe hyperparameters used in gensim.models.Word2Vec are as follows:\r\nsize: The number of dimensions of the embeddings (the default is 100).\r\nwindow: The maximum distance between a target word and words around the target word (the default is 5).\r\nmin_count: The minimum count of words to consider when training the model (the default for is 5).\r\nworkers: The number of partitions during training (the default is 3).\r\nsg: The training algorithm, either 0 for CBOW or 1 for skip gram (the default is 0).\r\nAfter the training process is complete, we get a vector for each word. Now, let’s take a look at the vectors of a particular from the corpus (e.g., the word “plastic”) to get an idea of what the generated vector looks like.\r\n\r\nmodel.wv[\"plastic\"]\r\narray([ 0.3236925 ,  0.2548069 ,  0.61545634,  0.1828132 ,  0.31981272,\r\n       -0.8854959 ,  0.10059591,  0.5529515 , -0.14545196, -0.33099753,\r\n       -0.1684745 , -0.80204433, -0.07991576,  0.10517135,  0.29105937,\r\n       -0.08265342,  0.1387488 , -0.44342119, -0.14201172, -1.1230628 ,\r\n        0.93303484,  0.15602377,  0.7197224 , -0.35337123, -0.01448521,\r\n        0.51030767, -0.06602395,  0.30631196, -0.05907682,  0.11381093,\r\n        0.3613567 ,  0.17538303,  0.501223  , -0.46734655, -0.3349126 ,\r\n        0.01602843,  0.51649153,  0.22251019, -0.31913355, -0.42772195,\r\n        0.05480129,  0.28686902, -0.55821824,  0.20228569, -0.01934895,\r\n       -0.4905142 , -0.43356672, -0.40940797,  0.56560874,  0.60450554,\r\n        0.10609645, -0.57371974,  0.09981435, -0.48511255,  0.32300022,\r\n        0.09809875,  0.11661741,  0.00955052, -0.2510347 ,  0.3500143 ,\r\n       -0.27248862,  0.0071716 ,  0.25264668, -0.03935822, -0.13833411,\r\n        0.63956493,  0.02967284,  0.48678428, -0.34669146,  0.22514342,\r\n       -0.32918864,  0.6453007 ,  0.2724857 ,  0.0536106 ,  0.4775878 ,\r\n        0.05614332,  0.5871811 , -0.58996713, -0.26652348, -0.33927533,\r\n       -0.6071714 ,  0.29880825, -0.56886315,  0.6028666 , -0.2625632 ,\r\n       -0.25126255,  0.6333157 ,  0.3476705 ,  0.28564158, -0.01256744,\r\n        1.1550713 ,  0.3539902 ,  0.13358444,  0.30924886,  0.9343885 ,\r\n        0.14220482,  0.03595947,  0.12772141, -0.03011671, -0.03848448],\r\n      dtype=float32)\r\n\r\nThe vector for the word “plastic” is essentially an array of numbers. Once we convert a word to a numerical vector, we can calculate its semantic similarity with other words in the corpus. For example, let’s find the top 2 words that are semantically the closest to the word “experimental” based on the cosine similarity between the vectors of the words in our corpus.\r\n\r\nmodel.wv.most_similar(\"experimental\", topn=2)\r\n[('improved', 0.9936665892601013), ('design', 0.9927185773849487)]\r\n\r\nWe see that the words that are similar to the word “experimental” include “improved” and “design”. We can also calculate the similarity among specific words. In the following example, we will see the similarity of the word “plastic” with two other words, “experiment” and “length”.\r\n\r\nmodel.wv.similarity(w1=\"experiment\", w2=\"plastic\")\r\n0.23159748\r\n\r\n\r\nmodel.wv.similarity(w1=\"length\", w2=\"plastic\")\r\n0.5783539\r\n\r\nThe similarity results show that the word “plastic” seems to be closer to the word “length” than the word “experimental”. We must note that this interesting finding is valid only for the corpus we used in this example (i.e., responses from Essay Set 2). The Gensim library in Python also provides pre-trained models and corpora. A pre-trained model based on a massive data set (e.g. the Google News data set) can be used for exploring semantic similarities as long as the data set is relevant to the domain we are working on.\r\nConclusion\r\nIn this post, we wanted to demonstrate how to use Word2Vec to create word vectors and to calculate semantic similarities between words. Word2Vec transforms individual words or phrases into numerical vectors in a multidimensional semantic space. Word embeddings obtained from Word2Vec can be used for a variety of applications such as generating cloze sentences, query auto-completion, and building a recommendation system. For readers who want to learn more about Word2Vec, we recommend Tensorflow’s Word2Vec tutorial. With this post, we have come to the end of our three-part text vectorization series. We hope that the examples we presented here and in the last two posts help researchers who are interested in using text vectorization for their NLP applications.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-02-text-vectorization-using-python-word2vec/pexels.jpg",
    "last_modified": "2022-05-21T21:31:59-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-16-text-vectorization-using-python-tf-idf/",
    "title": "Text Vectorization Using Python: TF-IDF",
    "description": "In the first part of this text vectorization series, we demonstrated how to transform textual data into a term-document matrix. Although this approach is fairly easy to use, it fails to consider the impact of words occuring frequently across the documents. In the second part of the series, we will focus on term frequency-inverse document frequency (TF-IDF) that can reduce the weight of common words while emphasizing unique words that are more important for each document. First, we will explain how TF-IDF can adjust the weights of the words based on their frequency in the documents and then demonstrate the use of TF-IDF in Python.\n\n(9 min read)",
    "author": [
      {
        "name": "Sevilay Kilmen",
        "url": "https://akademik.yok.gov.tr/AkademikArama/view/viewAuthor.jsp"
      },
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2022-01-16",
    "categories": [
      "data science",
      "text mining",
      "text vectorization",
      "natural language processing",
      "python"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nText Vectorization\r\nTF-IDF\r\n\r\nExample\r\nConclusion\r\n\r\nPhoto by Ri_Ya on PixabayIntroduction\r\nAfter a long (unintentional) hiatus, I am back to sharing more interesting examples of psychometrics and data science on my blog 💪.\r\nIn my last blog post, my colleague Dr. Jinnie Shin from the University of Florida and I had started a three-part series focusing on text vectorization using Python 🐍. In the first part, we explained the term-document matrix. In the second part of this series, we will discuss another text vectorization technique known as TF-IDF. We will explain how TF-IDF works, why it is better than a regular term-document matrix, and then demonstrate how to calculate TF-IDF using Python. I want to thank Dr. Sevilay Kilmen for her detailed work on this blog post, as well as for her encouragement to continue making blog posts.\r\nText Vectorization\r\nBefore we get into the details of TF-IDF, let’s remember what text vectorization means. We use text vectorization to transform textual data (e.g., students’ written responses to essay questions) into a numerical format that computers can understand and process the input. After text vectorization is performed, the resulting numerical data can be used for more advanced linguistic applications (e.g., automated essay scoring).\r\nIn the first part of this series, we demonstrated how to convert textual data into a term-document matrix (bag of words), which is a simple text vectorization method. The term-document matrix is a very simple and easy-to-use approach to transform textual data into numerical vectors. However, this approach simply focuses on the frequency of each word in the document without considering the weights of too frequently occurring words. This may lead to confusing results when it comes to evaluating similarities and differences among documents. For example, two documents may appear quite similar if both documents include similar stop words (e.g., was, is, to, the) that frequently occur in the documents. Thus, it is essential to emphasize distinct words representing the content of each document more accurately.\r\nTF-IDF\r\nNow we know that the term-document matrix (or bag of words) fails to capture distinct or unique words that provide stronger content representation for each document. So, what is a good alternative to using the term-document matrix? The answer is the term frequency-inverse document frequency, or shortly TF-IDF. To describe what TF-IDF is, we first need to explain the meanings of term frequency (TF) and inverse document frequency (IDF).\r\nTerm frequency (or, TF) represents a particular word’s weight in a given document. But, why are we supposed to weight individual words in a document? The main reason is that each document may have different number of words. That is, the length of one document can be very different from the length of another document. For example, assume that we are looking for a particular word in two documents: a document with 22 words and another document with 250 words. Compared with the shorter document, the longer document would be more likely to contain the word. To make these documents more comparable, word weights (i.e., counts) need to be standardized based on the length of each document. TF provides this standardization by dividing the frequency of a word by the total of words in the document.\r\nTF = (Frequency of a word in the document) / (Total number of words in the document)\r\nLet’s see a simple example of how TF is calculated. Assume that we have two documents. One of the documents consists of the following sentence: “John likes apple”. In the document, the word “John” occurs only once. Therefore, the TF value for “John” is \\(1/3=0.333\\). The second document also consists of a single sentence: “Mary likes apple and cherry”. This document consists of five words, and “Mary” is included one time in the document. Therefore, the TF value of the word “Mary” is \\(1/5=0.2\\).\r\nIf a particular word is not included in a document, then its TF value becomes 0 for that document. On the other hand, if the document includes the word but no other words, then its TF value becomes 1 for the document. So, we can see that the TF value ranges between 0 and 1. Words that frequently occur within a document have higher TF values and other words that are not as common.\r\nUnlike TF, inverse document frequency (IDF) represents a particular word’s weight across all documents. The reason for calling it “inverse” is that as the number of documents including a particular word increases, the weight of that word decreases. IDF accomplishes this by calculating the logarithm of the ratio of the total number of documents to the number of documents including the word.\r\nIDF = log(total number of documents / number of documents including the word)\r\nLet’s see another simple example to demonstrate how IDF can be calculated. Assume that there are 1000 documents in a corpus (i.e., a collection of texts). If all documents include a particular word, the IDF value of that word becomes \\(log(1000/1000)=log(1)=0\\). If that word takes a place in 100 documents of 1000 documents, the IDF value of the word becomes log(1000/100)=log(10)=1. If, however, the word occurs only in 10 documents out of 1000 documents, then the IDF value of that word becomes \\(log(1000/10)=log(100)=2\\). This example shows that as the number of documents including the word increases, the IDF value of the word decreases.\r\nNow we know how to calculate TF and IDF but how do we find TF-IDF? To calculate the TF-IDF value of a particular word in a document, we can simply multiply its TF and IDF values.\r\nTF-IDF = (TF * IDF)\r\nThe TF-IDF value depends on the frequency of the word in the document, the total number of words in the document, the total number of documents in the corpus, and the number of documents including the word. If a particular word is included in all documents, its IDF value becomes zero and thus its TF-IDF value also becomes zero. Similarly, if a word is not included in a document, then its TF value for that document becomes zero and thus the TF-IDF value also becomes zero.\r\nIn the following section, we will demonstrate the calculation of TF-IDF in Python. We will use real data (i.e., students’ written responses from an automated essay scoring competition) to prepare text vectors using the TF-IDF algorithm in Python.\r\nExample\r\nIn this example, we will use a data set from one of the popular automated essay scoring competitions funded by the Hewlett Foundation: Short Answer Scoring. The data set includes students’ responses to ten different sets of short-answer items and scores assigned by two human raters. The data set is available here as a tab-separated value (TSV) file. The data set consists of the following variables:\r\nId: A unique identifier for each individual student essay.\r\nEssaySet: An id for each set of essays (ranges from 1 to 10).\r\nScore1: Rater1’s score (ranges from 0 to 2).\r\nScore2: Rater2’s score (ranges from 0 to 2).\r\nEssayText: Student’s response (textual data).\r\nFor our demonstration, we will use “Essay Set 3” where students are asked to explain how pandas in China are similar to koalas in Australia and how they are different from pythons. They also need to support their responses with information from the articles given in the reading passage included in the item. There are three scoring categories (0, 1, or 2 points). Each score category contains a range of student responses which reflect the descriptions given below:\r\nScore 2: The response demonstrates an exploration or development of the ideas presented in the text, a strong conceptual understanding by the inclusion of specific relevant information from the text an extension of ideas that may include extensive and/or insightful inferences, connections between ideas in the text, and references to prior knowledge and/or experiences.\r\nScore 1: The response demonstrates some exploration or development of ideas presented in the text a fundamental understanding by the inclusion of some relevant information from the text an extension of ideas that lacks depth, although may include some inferences, connections between ideas in the text, or references to prior knowledge and/or experiences.\r\nScore 0: The response demonstrates limited or no exploration or development of ideas presented in the text limited or no understanding of the text, may be illogical, vague, or irrelevant possible incomplete or limited inferences, connections between ideas in the text, or references to prior knowledge and/or experiences.\r\nNow, let’s begin our analysis by importing the data into Python and selecting Essay Set 3.\r\n\r\n# Import pandas for dataframe \r\nimport pandas as pd \r\n\r\n# Import train_rel_2.tsv into Python\r\nwith open('train_rel_2.tsv', 'r') as f:\r\n    lines = f.readlines()\r\n    columns = lines[0].split('\\t')\r\n    response = []\r\n    score = []\r\n    for line in lines[1:]:\r\n        temp = line.split('\\t') \r\n        if temp[1] == '3':   # Select the Essay Set 3\r\n            response.append(temp[-1])  # Select EssayText as response\r\n            score.append(int(temp[2])) # Select score1 for human scoring only\r\n        else: \r\n            None\r\n\r\nNow, let’s format the data in such a way that it consists of the necessary columns (two columns: response and score), and then review how many rows and columns the data set consists of.\r\n\r\n# Construct a dataframe (\"data\") which includes response and score column     \r\ndata = pd.DataFrame(list(zip(response, score))) \r\ndata.columns = ['response', 'score'] \r\n\r\n# Print how many rows and columns of the data set consists  \r\nprint(data.shape)\r\n(1808, 2)\r\n\r\nThe values shown above indicate that the data set consists of 1808 rows and two columns (i.e., response and score columns). Now, let’s take a look at the first ten responses.\r\n\r\n# Preview the first ten row in the data set\r\nprint(data.head(10))\r\n                                            response  score\r\n0  China's panda and Australia's koala are two an...      1\r\n1  Pandas and koalas are similar because they are...      1\r\n2  Pandas in China and Koalas in Australia are si...      1\r\n3  Pandas in China only eat bamboo and Koalas in ...      2\r\n4  Pandas in China and koalas from Australia are ...      0\r\n5  Panda's are similar to koala's because they ar...      0\r\n6  Panda's are similar to Koala's by they are bot...      2\r\n7  Pandas in china are similar to koalas in Austr...      1\r\n8  Pandas and koalas are similar because they eat...      1\r\n9  Pandas are similar to koalas due to their very...      1\r\n\r\nEach document includes a set of words contribute to the meaning in the sentence, as well as stop words (e.g., articles, prepositions, pronouns, and conjunctions) that do not add much information to the text. Since stop words are very common and yet they only provide low-level information, removing them from the text can help us highlight words that are more important for each document. In addition, the presence of stop words leads to high sparsity and high dimensionality in the data (see curse of dimensionality). Furthermore, lowercase-uppercase texts and lemmatization are other factors that may impact the vectorization of text. Therefore, before performing TF-IDF text vectorization, a preprocessing process that involves removing stop words, converting uppercase letters to lowercase letters, and lemmatization can be implemented as follow:\r\n\r\n# Import re, nltk, and WordNetLemmatizer\r\nimport re\r\nimport nltk\r\nfrom nltk.stem import WordNetLemmatizer\r\n\r\n# Stopword removal, converting uppercase into lower case, and lemmatization\r\nstopwords = nltk.corpus.stopwords.words('english')\r\nlemmatizer = WordNetLemmatizer()\r\nnltk.download('stopwords')\r\ndata_without_stopwords = []\r\nfor i in range(0, len(response)):\r\n    doc = re.sub('[^a-zA-Z]', ' ', response[i])\r\n    doc = doc.lower()\r\n    doc = doc.split()\r\n    doc = [lemmatizer.lemmatize(word) for word in doc if not word in set(stopwords)]\r\n    doc = ' '.join(doc)\r\n    data_without_stopwords.append(doc)\r\n\r\nTo better understand how preprocessing affects the data, we can print the first student’s response before preprocess.\r\n\r\n# Print first row in the the original data set \r\nprint(data.response[0])    \r\nChina's panda and Australia's koala are two animals that arent predator, pandas eat bamboo and koala's eat eucalyptus leaves. Therefore, they are harmless. They are both different from pythons because pythons are potentialy dangerous considering they can swallow an entire alligator you could conceivably have pythons shacking upto the Potomac\r\n\r\nNow, we will print the same response after preprocessing to see the difference.\r\n\r\n# Print first row in the the data set after preprocessing\r\nprint(data_without_stopwords[0])\r\nchina panda australia koala two animal arent predator panda eat bamboo koala eat eucalyptus leaf therefore harmless different python python potentialy dangerous considering swallow entire alligator could conceivably python shacking upto potomac\r\n\r\nWe can see that after preprocessing, stop words have been removed, all the words have been transformed into lowercase letters, and the words have been lemmatized. Now, we can go ahead and vectorize the responses by using TfidfVectorizer from sklearn.\r\n\r\n# Import Tfidf vectorizer\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nvectorizer = TfidfVectorizer() \r\nvectors = vectorizer.fit_transform(data_without_stopwords)\r\n\r\nLet’s have a look at how many rows and columns of the TF-IDF matrix consists of.\r\n\r\n# Print how many rows and columns of the TF-IDF matrix consists\r\nprint(\"n_samples: %d, n_features: %d\" % vectors.shape)\r\nn_samples: 1808, n_features: 1978\r\n\r\nThe output shows that a new vector consisting of 1978 features belonging to 1808 participants have been created. The TF-IDF matrix is a large matrix, including numerous rows and columns. For the sake of brevity, we will focus on the first five student responses and the most frequent ten words in the TF-IDF matrix.\r\n\r\n# Select the first five documents from the data set\r\ntf_idf = pd.DataFrame(vectors.todense()).iloc[:5]  \r\ntf_idf.columns = vectorizer.get_feature_names()\r\ntfidf_matrix = tf_idf.T\r\ntfidf_matrix.columns = ['response'+ str(i) for i in range(1, 6)]\r\ntfidf_matrix['count'] = tfidf_matrix.sum(axis=1)\r\n\r\n# Top 10 words \r\ntfidf_matrix = tfidf_matrix.sort_values(by ='count', ascending=False)[:10] \r\n\r\n# Print the first 10 words \r\nprint(tfidf_matrix.drop(columns=['count']).head(10))\r\n            response1  response2  response3  response4  response5\r\npython       0.129319   0.124885   0.083783   0.066513   0.467525\r\nkoala        0.079249   0.172196   0.154030   0.061140   0.214879\r\npanda        0.078464   0.170490   0.152505   0.060535   0.212752\r\neat          0.105059   0.076093   0.204196   0.243159   0.000000\r\naustralia    0.056927   0.000000   0.110645   0.087838   0.308710\r\nchina        0.054376   0.000000   0.105687   0.083902   0.294878\r\ngeneralist   0.000000   0.113179   0.000000   0.000000   0.423700\r\nsimilar      0.000000   0.077710   0.104268   0.000000   0.290917\r\ndifferent    0.059878   0.086738   0.000000   0.000000   0.324715\r\nspecialist   0.000000   0.099279   0.000000   0.000000   0.371665\r\n\r\nIn the matrix, we can see that each word has a different weight (TF-IDF value) for each document and that the TF-IDF values of the words not included in the document are zero. For example, the word “specialist” is not included in document 1 (i.e., response 1) and thus its TF-IDF value is zero.\r\nConclusion\r\nIn this post, we wanted to demonstrate how to use the TF-IDF vectorization to create text vectors beyond the term-document matrix (i.e., bag of words). The TF-IDF vectorization transforms textual data into numerical vectors while considering the frequency of each word in the document, the total number of words in the document, the total number of documents, and the number of documents including each unique word. Therefore, unlike the term-document matrix that only shows the presence, absence, or count of a word in a document, it creates more meaningful text vectors focusing on the weight of the words representing their unique contribution to the document. We hope that this post will help you gain a deeper understanding of text vectorization. In the last part of this series, we will discuss word embedding approaches (e.g., Word2Vec) as one of the most popular methods for vectorizing textual data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-16-text-vectorization-using-python-tf-idf/pixabay.jpg",
    "last_modified": "2022-01-16T10:32:54-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-08-text-vectorization-using-python-term-document-matrix/",
    "title": "Text Vectorization Using Python: Term-Document Matrix",
    "description": "Text vectorization is an important step in preprocessing and preparing textual data for advanced analyses of text mining and natural language processing (NLP). With text vectorization, raw text can be transformed into a numerical representation. In this three-part series, we will demonstrate different text vectorization techniques using Python. The first part focuses on the term-document matrix. \n\n(8 min read)",
    "author": [
      {
        "name": "Jinnie Shin",
        "url": "https://coe.ufl.edu/faculty/shin-jinnie/"
      },
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2021-04-08",
    "categories": [
      "data science",
      "text mining",
      "natural language processing"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nTerm-Document Matrix\r\nDocument-Term Vector Visualization\r\nCosine Similarity between Documents\r\n\r\nConclusion\r\n\r\nPhoto by Pixabay on PexelsIntroduction\r\nNatural language processing (NLP) is a subfield of artificial intelligence that focuses on the linguistic interaction between humans and computers. Over the last two decades, NLP has been a rapidly growing field of research across many disciplines, yielding some advanced applications (e.g., automatic speech recognition, automatic translation of text, and chatbots). With the help of evolving machine learning and deep learning algorithms, NLP can help us systematically analyze large volumes of text data. The fundamental idea of NLP in various tasks stems from converting the natural language into a numerical format that computers can understand and process. This process is known as text vectorization. Effective and efficient vectorization of human language and text lays an important foundation to conduct advanced NLP tasks.\r\nIn this three-part series, we will explain text vectorization and demonstrate different methods for vectorizing textual data. First, we will begin with a basic vectorization approach that is widely used in text mining and NLP applications. More specifically, we will focus on how term-document matrices are constructed. In the following example, we will have a brief demonstration of this technique using Python 🐍 (instead of R)1.\r\nExample\r\nIn this example, we will use a data set from one of the popular automated essay scoring competitions funded by the Hewlett Foundation: Short Answer Scoring. The data set includes students’ responses to a set of short-answer items and scores assigned by human raters. On average, each answer is approximately 50 words in length. The data set (train_rel_2.tsv) is available here as a tab-separated value (TSV) file. The data set consists of the following variables:\r\nId: A unique identifier for each individual student essay.\r\nEssaySet: 1-10, an id for each set of essays.\r\nScore1: The human rater’s score for the answer. This is the final score for the answer and the score that you are trying to predict.\r\nScore2: A second human rater’s score for the answer. This is provided as a measure of reliability, but had no bearing on the score the essay received.\r\nEssayText: The ASCII text of a student’s response.\r\nIn this example, we will take a look at “Essay Set 1”, in which students were provided with a prompt describing a science experiment. The students were required to identify the missing information that is important to increase the replicability of the experiment procedures described in the prompt.\r\nWe will begin our analysis by importing the data set into Python.\r\n\r\n# Import pandas for dataframe \r\n# Import pprint for printing the outcomes \r\nimport pandas as pd \r\nfrom pprint import pprint \r\n\r\n# Import train_rel_2.tsv into Python\r\nwith open('train_rel_2.tsv', 'r') as f:\r\n    lines = f.readlines()\r\n    columns = lines[0].split('\\t')\r\n    data = []\r\n    response_id= []\r\n    score = []\r\n    for line in lines[1:]:\r\n        temp = line.split('\\t')\r\n        if temp[1] == '1':\r\n            data.append(temp[-1])\r\n            response_id.append(int(temp[0]))\r\n            score.append(int(temp[2]))\r\n        else: \r\n            None\r\n\r\n# Construct a dataframe (\"doc\") which includes the response_id, responses, and the score        \r\ndoc = pd.DataFrame(list(zip(response_id, data, score)))\r\ndoc.columns = ['id', 'response', 'score']\r\n\r\nNow, we can take a look at some of the student-written responses in the data set.\r\n\r\n# Preview the first response in the data set\r\nprint('Sample response 1:')\r\npprint(doc.response.values[0]) \r\n\r\n# Preview the first 5 lines in the data set\r\ndoc.head(5)\r\n\r\n\r\nSample response 1:\r\n('Some additional information that we would need to replicate the experiment '\r\n 'is how much vinegar should be placed in each identical container, how or '\r\n 'what tool to use to measure the mass of the four different samples and how '\r\n 'much distilled water to use to rinse the four samples after taking them out '\r\n 'of the vinegar.\\n')\r\n   id                                           response  score\r\n0   1  Some additional information that we would need...      1\r\n1   2  After reading the expirement, I realized that ...      1\r\n2   3  What you need is more trials, a control set up...      1\r\n3   4  The student should list what rock is better an...      0\r\n4   5  For the students to be able to make a replicat...      2\r\n\r\nEach student response is associated with a score ranging from 0 to 3, which indicates the overall quality of the response. For example, the first item in the data set focuses on science for Grade 10 students. The item requires students to identify the missing information that would allow replicating the experiment.\r\n\r\n\r\n\r\nFigure 1: A preview of Essay Set 1 in the data set.\r\n\r\n\r\n\r\nEach score (0, 1, 2, or 3 points) category contains a range of student responses which reflect the descriptions given below:\r\nScore 3: The response is an excellent answer to the question. It is correct, complete, and appropriate and contains elaboration, extension, and evidence of higher-order thinking and relevant prior knowledge\r\nScore 2: The response is a proficient answer to the question. It is generally correct, complete, and appropriate, although minor inaccuracies may appear. There may be limited evidence of elaboration, extension, higher-order thinking, and relevant prior knowledge.\r\nScore 1: The response is a marginal answer to the question. While it may contain some elements of a proficient response, it is inaccurate, incomplete, or inappropriate.\r\nScore 0: The response, though possibly on topic, is an unsatisfactory answer to the question. It may fail to address the question, or it may address the question in a very limited way.\r\nBy analyzing students’ responses to this item, we can find important hints from the vocabulary and word choices to better understand the overall quality of their responses. For example, score 3 indicates that the response is correct, complete, and appropriate and contains elaboration. To find out what an elaborate and complete response looks like, we will focus on the semantic structure of the responses.\r\nTerm-Document Matrix\r\nTerm-document matrix represents texts using the frequency of terms or words that appear in a set of documents. While the term-document matrix reveals information regarding most or least common words across multiple texts, little to no information is preserved regarding the order of how the words appear originally in the document (bag-of-words). Still, the term-document matrix provides important insights about the documents (also, it is very easy to construct and understand!)\r\n\r\n\r\n\r\nFigure 2: The illustration of the bag-of-words approach. (Source: http://people.cs.georgetown.edu/nschneid/cosc572/f16/05_classification-NB_slides.pdf).\r\n\r\n\r\n\r\nWe will use CountVectorizer to count how many times each unique word appears in the responses. For analytic simplicity, we will focus on the first five student responses and the top 25 words in this demonstration.\r\n\r\n# Activate CountVectorizer\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\n\r\n# Count Vectorizer\r\nvect = CountVectorizer()  \r\nvects = vect.fit_transform(doc.response)\r\n\r\n# Select the first five rows from the data set\r\ntd = pd.DataFrame(vects.todense()).iloc[:5]  \r\ntd.columns = vect.get_feature_names()\r\nterm_document_matrix = td.T\r\nterm_document_matrix.columns = ['Doc '+str(i) for i in range(1, 6)]\r\nterm_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\r\n\r\n# Top 25 words \r\nterm_document_matrix = term_document_matrix.sort_values(by ='total_count',ascending=False)[:25] \r\n\r\n# Print the first 10 rows \r\nprint(term_document_matrix.drop(columns=['total_count']).head(10))\r\n         Doc 1  Doc 2  Doc 3  Doc 4  Doc 5\r\nthe          5      5      1      3      2\r\nto           5      2      1      0      3\r\nis           1      1      1      2      2\r\nand          1      1      2      1      1\r\nvinegar      2      1      1      0      1\r\nwhat         1      0      1      2      1\r\nyou          0      3      2      0      0\r\nof           2      1      1      0      1\r\nhow          3      0      0      0      1\r\nin           1      1      1      1      0\r\n\r\nAs the name suggests, each row in the term-document matrix indicates a unique word that appeared across the responses, while the columns represent a unique document (e.g., “Doc 1”, “Doc 2”, …) that are the responses for each student. Now let’s take a look at which words were most frequently used across the responses.\r\n\r\nterm_document_matrix['total_count'].plot.bar()\r\n\r\n\r\nFigure 3: A bar plot of most frequent words in the responses.\r\n\r\n\r\n\r\nNot surprisingly, the function words (e.g., “the”, “to”, “is”) appeared more frequently than other words with more contextual information, such as the words “rock”, “use”, and “mass”. Also, this unique distribution should remind us of a very famous distribution in linguistics: Unzipping Zipf’s Law.\r\nDocument-Term Vector Visualization\r\nNow that we represented each document as a unique vector that indicates information regarding word occurrence, we can visualize the relationship between the documents. This can be easily achieved by simply getting a transpose of the term-document matrix (i.e., document-term matrix). Let’s use the two most frequent words “the” and “to” to plot the documents.\r\n\r\n# Locate the and to in the documents\r\nterm_document_matrix.loc[['the', 'to']].T\r\n\r\n# Create a scatterplot of the frequencies\r\n             the  to\r\nDoc 1          5   5\r\nDoc 2          5   2\r\nDoc 3          1   1\r\nDoc 4          3   0\r\nDoc 5          2   3\r\ntotal_count   16  11\r\nterm_document_matrix.drop(columns=['total_count']).T.plot.scatter(x='the', y='to')\r\n\r\n\r\nFigure 4: A scatterplot of frequencies for the and to across the first five documents.\r\n\r\n\r\n\r\nIt is quite difficult to understand which of the documents are highly related to each other just by looking at the relationships using two words. Now, we can plot the documents using the term vector to better understand the similarities (or differences) between their word distributions of the top 25 vocabularies that we selected above.\r\nCosine Similarity between Documents\r\nWe will use cosine similarity that evaluates the similarity between the two vectors by measuring the cosine angle between them. If the two vectors are in the same direction, hence similar, the similarity index yields a value close to 1. The cosine similarity index can be computed using the following formula:\r\n\\[ similarity = cos(\\theta)=\\frac{\\mathbf{A}.\\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\Sigma_{i=1}^nA_iB_i}{\\sqrt{\\Sigma_{i=1}^nA_i^2}\\sqrt{\\Sigma_{i=1}^nB_i^2}} \\]\r\nNow, let’s define a function to calculate cosine similarity between two vectors2:\r\n\r\n# Activate math\r\nimport math\r\n\r\n# Define a cosine similarity function\r\ndef cosine_similarity(a,b):\r\n    \"compute cosine similarity of v1 to v2: (a dot b)/{||a||*||b||)\"\r\n    sumxx, sumxy, sumyy = 0, 0, 0\r\n    for i in range(len(a)):\r\n        x = a[i]; y = b[i]\r\n        sumxx += x*x\r\n        sumyy += y*y\r\n        sumxy += x*y\r\n    return sumxy/math.sqrt(sumxx*sumyy)\r\n\r\nRecall that each student response is associated with a score that represents the overall quality (or accuracy) of the response. We could hypothesize that for students who used similar words in their responses, the scores would be similar.\r\n\r\n# Activate numpy\r\nimport numpy as np \r\n\r\n# Save the similarity index between the documents\r\ndef pair(s):\r\n    for i, v1 in enumerate(s):\r\n        for j in range(i+1, len(s)):\r\n            yield [v1, s[j]]\r\n\r\ndic={} \r\nfor (a,b) in list(pair(['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4', 'Doc 5'])):\r\n  dic[(a,b)] = cosine_similarity(term_document_matrix[a].tolist(), term_document_matrix[b].tolist())\r\n\r\n# Print the cosine similarity index\r\npprint(dic)\r\n{('Doc 1', 'Doc 2'): 0.7003574164133837,\r\n ('Doc 1', 'Doc 3'): 0.5496565719302984,\r\n ('Doc 1', 'Doc 4'): 0.47601655464870696,\r\n ('Doc 1', 'Doc 5'): 0.8463976586173779,\r\n ('Doc 2', 'Doc 3'): 0.6674238124719146,\r\n ('Doc 2', 'Doc 4'): 0.5229577884350213,\r\n ('Doc 2', 'Doc 5'): 0.6510180115869747,\r\n ('Doc 3', 'Doc 4'): 0.4811252243246881,\r\n ('Doc 3', 'Doc 5'): 0.5689945423921311,\r\n ('Doc 4', 'Doc 5'): 0.4927637283262872}\r\n\r\nThe values shown above indicate the similarity index for each document pair (i.e., student pairs). For example, the first row shows that the first document (i.e., student 1’s response) and the second document (i.e., student 2’s response) had a similarity index of 0.70. To see all the similarity indices together, we can create a heatmap that shows the cosine similarity index for each pair of documents.\r\n\r\ndocuments= ['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4', 'Doc 5']\r\nfinal_df = pd.DataFrame(np.asarray([[(dic[(x,y)] if (x,y) in dic else 0) for y in documents] for x in documents]))\r\nfinal_df.columns =  documents\r\nfinal_df.index = documents \r\n\r\nimport matplotlib.pyplot as plt\r\nfig, ax = plt.subplots()\r\nax.set_xticks(np.arange(len(documents)))\r\nax.set_yticks(np.arange(len(documents)))\r\nax.set_xticklabels(documents)\r\nax.set_yticklabels(documents)\r\nax.matshow(final_df, cmap='seismic')\r\nfor (i, j), z in np.ndenumerate(final_df):\r\n  if z != 0 :\r\n    ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',\r\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\r\n  else:\r\n    None\r\nfig.suptitle('Cosine Similarity Index between the Documents')\r\nplt.show()\r\n\r\n\r\n\r\n\r\nFigure 5: A heatmap of the cosine similarity indices across the five documents.\r\n\r\n\r\n\r\nIn our example, documents 1, 2, and 3 were scored as 1 point, document 4 was scored as 0 point, and document 5 was scored as 2 points. As shown in Figure 5, the highest similarity (0.85) occurs between Documents 1 and 5. This might be a surprising finding because these two documents were given different scores (1 and 2 points, respectively). In terms of the lowest similarity, we can see that document 4 is quite different from the rest of the documents with a relatively lower cosine similarity index (0.48, 0.52, and 0.48). This finding is not necessarily surprising because document 4 represents the only response scored as 0 points.\r\nConclusion\r\nIn this post, we demonstrated how we could convert text documents (e.g., a student’s written responses to an item) into a term-document matrix. Term-document vectorization is often called the “bag-of-words” representation (or shortly, BoW) as it focuses on considering the frequency of words in the document to understand and preserve its semantic representations. We also attempted to understand how we could use this vectorization approach to measure the similarities between the documents. In the next post, we will review how we can suppress the weights of “too frequently” occurring words, such as function words (e.g., “the”, “to”, “is”), using different vectorization approaches.\r\n\r\nI plan to make a post showing the same analyses using R.↩︎\r\nThe original stackoverflow discussion with the cosine similarity function: https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists↩︎\r\n",
    "preview": "posts/2021-04-08-text-vectorization-using-python-term-document-matrix/pexels.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-23-visualizing-machine-learning-models/",
    "title": "Visualizing Machine Learning Models",
    "description": "Data visualization plays an important role when evaluating the performance of machine learning models. In this post, we demonstrate how to use the **DALEX** package for visualizing machine learning models effectively. Visualizations with **DALEX** can facilitate the comparison of machine learning models and help researchers understand which model works better and why.\n \n (10 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      },
      {
        "name": "Seyma Nur Yildirim-Erbasli",
        "url": "https://www.ualberta.ca"
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "machine learning",
      "classification",
      "data visualization"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nModel Performance\r\nVariable Importance\r\nPartial Dependence Plot\r\nAccumulated Local Effects Plot\r\nInstance Level Explanation\r\nCeteris Paribus Profiles\r\n\r\nConclusion\r\n\r\nPhoto by Gerd Altmann on pixabayIntroduction\r\nOver the last few years, advanced machine learning algorithms have been widely utilized in different contexts of education. The literature shows that educational researchers typically perform machine learning models for classification (or prediction) problems, such as student engagement (e.g., Hew et al., 2018), performance (e.g., Xu et al., 2017), and dropout (e.g., Tan & Shao, 2015). Researchers often try different classification algorithms and select the most accurate model based on model evaluation metrics (e.g., recall, precision, accuracy, and area under the curve). However, the comparison and evaluation of machine learning models based on these evaluation metrics are not necessarily easy to use for most researchers.\r\nIn this post, we demonstrate a versatile R package that can be used to visualize and interpret machine learning models: DALEX (Biecek, 2018). DALEX package stands for moDel Agnostic Language for Exploration and eXplanation. It can be used for both regression and classification tasks in machine learning. With the DALEX package, we can examine residual diagnostics, feature importance, the relationship between features and the outcome variable, the accuracy of future predictions, and many other things. Using real data from a large-scale assessment, we will review some of the data visualization tools available in DALEX.\r\nNow, let’s get started 📈.\r\nExample\r\nIn this example, we will use student data from the OECD’s Programme for International Student Assessment (PISA). PISA is an international. large-scale assessment that measures 15-year-old students’ competency in reading, mathematics and science. Using the Turkish sample of the PISA 2015 database, we will build a binary classification model that predicts students’ reading performance (i.e., high vs. low performance) and then use the DALEX package to evaluate and compare different machine learning algorithms. The data set is available here. The variables in the data set are shown below:\r\nVariable\r\nDescription\r\ngender\r\nGender\r\ngrade\r\nGrade\r\ncomputer\r\nHaving a vomputer at home\r\ninternet\r\nHaving Internet at home\r\ndesk\r\nHaving a study desk at home?\r\nown.room\r\nOwning a room at home\r\nquiet.study\r\nOwning a quiet study area at home\r\nbook.sch\r\nHaving school books\r\ntech.book\r\nHaving technical books\r\nart.book\r\nHaving art books\r\nreading\r\nStudents’ reading scores in PISA 2015\r\nFirst, we will import the data into R and preview its content:\r\n\r\n\r\npisa <- read.csv(\"PISA_Turkey.csv\", header = TRUE)\r\nhead(pisa)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSecond, we will remove missing cases from the data.\r\n\r\n\r\npisa <- na.omit(pisa)\r\n\r\n\r\n\r\nNext, we will convert gender and grade to numeric variables. Also, in the DALEX package, the outcome variable needs to be a numeric vector for both regression and classification tasks. Thus, we will transform students’ reading scores into a binary variable based on the average reading score: 1 (i.e., high performance) vs. 0 (i.e., low performance).\r\n\r\n\r\n# Convert gender to a numeric variable\r\npisa$gender = (as.numeric(sapply(pisa$gender, function(x) {\r\n  if(x==\"Female\") \"1\"\r\n  else if (x==\"Male\") \"0\"})))\r\n\r\n# Convert grade to a numeric variable\r\npisa$grade = (as.numeric(sapply(pisa$grade, function(x) {\r\n  if(x==\"Grade 7\") \"7\"\r\n  else if (x==\"Grade 8\") \"8\"\r\n  else if (x==\"Grade 9\") \"9\"\r\n  else if (x==\"Grade 10\") \"10\"\r\n  else if (x==\"Grade 11\") \"11\"\r\n  else if (x==\"Grade 12\") \"12\"})))\r\n\r\n# Convert reading performance to a binary variable based on the average score \r\n# 1 represents high performance and 0 represents low performance\r\npisa$reading <- factor(ifelse(pisa$reading >= mean(pisa$reading), 1, 0))\r\n\r\n# View the frequencies for high and low performance groups\r\ntable(pisa$reading)\r\n\r\n\r\n\r\n   0    1 \r\n2775 2722 \r\n\r\nNow, we will build a machine learning model using three different algorithms: random forest, logistic regression, and support vector machines. Since the focus of our post is on how to visualize machine learning models, we will build the machine learning models without additional hyperparameter tuning. We use the createDataPartition() function from the caret package (Kuhn, 2020) to create training (70%) and testing (30%) sets.\r\n\r\n\r\n# Activate the caret package\r\nlibrary(\"caret\")\r\n\r\n# Set the seed to ensure reproducibility\r\nset.seed(1)\r\n\r\n# Split the data into training and testing sets\r\nindex <- createDataPartition(pisa$reading, p = 0.7, list = FALSE)\r\ntrain <- pisa[index, ]\r\ntest  <- pisa[-index, ]\r\n\r\n\r\n\r\nNext, we use the train() function from the caret package to create three classification models through 5-fold cross-validation. In each model, reading ~ . indicates that the outcome variable is reading (1 = high performance, 0 = low performance) and the remaining variables are the predictors.\r\n\r\n\r\n# 5-fold cross-validation\r\ncontrol = trainControl(method=\"repeatedcv\", number = 5, savePredictions=TRUE)\r\n\r\n# Random Forest\r\nmod_rf = train(reading ~ .,\r\n               data = train, method='rf', trControl = control)\r\n\r\n# Generalized linear model (i.e., Logistic Regression)\r\nmod_glm = train(reading ~ .,\r\n                data = train, method=\"glm\", family = \"binomial\", trControl = control)\r\n\r\n# Support Vector Machines\r\nmod_svm <- train(reading ~.,\r\n                 data = train, method = \"svmRadial\", prob.model = TRUE, trControl=control)\r\n\r\n\r\n\r\nNow, we are ready to explore the DALEX package. The first step of using the DALEX package is to define explainers for machine learning models. For this, we write a custom predict function with two arguments: model and newdata. This function returns a vector of predicted probabilities for each class of the binary outcome variable.\r\nIn the second step, we create an explainer for each machine learning model using the explainer() function from the DALEX package, the testing data set, and the predict function. When we convert machine learning models to an explainer object, they contain a list of the training and metadata on the machine learning model.\r\n\r\n\r\n# Activate the DALEX package\r\nlibrary(\"DALEX\")\r\n\r\n# Create a custom predict function\r\np_fun <- function(object, newdata){\r\n  predict(object, newdata=newdata, type=\"prob\")[,2]\r\n  }\r\n\r\n# Convert the outcome variable to a numeric binary vector\r\nyTest <- as.numeric(as.character(test$reading))\r\n\r\n# Create explainer objects for each machine learning model\r\nexplainer_rf  <- explain(mod_rf, label = \"RF\",\r\n                         data = test, y = yTest,\r\n                         predict_function = p_fun,\r\n                         verbose = FALSE)\r\n\r\nexplainer_glm <- explain(mod_glm, label = \"GLM\",\r\n                         data = test, y = yTest,\r\n                         predict_function = p_fun,\r\n                         verbose = FALSE)\r\n\r\nexplainer_svm <- explain(mod_svm, label = \"SVM\",\r\n                         data = test, y = yTest,\r\n                         predict_function = p_fun,\r\n                         verbose = FALSE)\r\n\r\n\r\n\r\nModel Performance\r\nWith the DALEX package, we can analyze model performance based on the distribution of residuals. Here, we consider the differences between observed and predicted probabilities as residuals. The model_performance() function calculates predictions and residuals for the testing data set.\r\n\r\n\r\n# Calculate model performance and residuals\r\nmp_rf  <- model_performance(explainer_rf)\r\nmp_glm <- model_performance(explainer_glm)\r\nmp_svm <- model_performance(explainer_svm)\r\n\r\n# Random Forest\r\nmp_rf\r\n\r\n\r\nMeasures for:  classification\r\nrecall     : 0.663 \r\nprecision  : 0.6558 \r\nf1         : 0.6594 \r\naccuracy   : 0.6608 \r\nauc        : 0.7165\r\n\r\nResiduals:\r\n     0%     10%     20%     30%     40%     50%     60%     70% \r\n-1.0000 -0.9646 -0.3952 -0.2440 -0.0580  0.0000  0.0020  0.0160 \r\n    80%     90%    100% \r\n 0.2340  0.6840  1.0000 \r\n\r\n# Logistic Regression\r\nmp_glm\r\n\r\n\r\nMeasures for:  classification\r\nrecall     : 0.6924 \r\nprecision  : 0.6479 \r\nf1         : 0.6694 \r\naccuracy   : 0.6614 \r\nauc        : 0.7165\r\n\r\nResiduals:\r\n      0%      10%      20%      30%      40%      50%      60% \r\n-0.94870 -0.63986 -0.48616 -0.38661 -0.20636 -0.04374  0.28757 \r\n     70%      80%      90%     100% \r\n 0.35729  0.41568  0.58303  0.98097 \r\n\r\n# Support Vector Machines\r\nmp_svm\r\n\r\n\r\nMeasures for:  classification\r\nrecall     : 0.6556 \r\nprecision  : 0.6613 \r\nf1         : 0.6585 \r\naccuracy   : 0.6632 \r\nauc        : 0.7025\r\n\r\nResiduals:\r\n     0%     10%     20%     30%     40%     50%     60%     70% \r\n-0.7026 -0.6870 -0.3824 -0.2882 -0.2835 -0.1896  0.3129  0.3129 \r\n    80%     90%    100% \r\n 0.3346  0.6912  0.8474 \r\n\r\nBased on the performance measures of these three models (i.e., recall, precision, f1, accuracy, and AUC) from the above output, we can say that the models seem to perform very similarly. However, when we check the residual plots, we see how similar or different they are in terms of the residuals. Residual plots show the cumulative distribution function for absolute values from residuals and they can be generated for one or more models. Here, we use the plot() function to generate a single plot that summarizes all three models. This plot allows us to make an easy comparison of absolute residual values across models.\r\n\r\n\r\n# Activate the ggplot2 package\r\nlibrary(\"ggplot2\")\r\n\r\np1 <- plot(mp_rf, mp_glm, mp_svm)\r\np1\r\n\r\n\r\n\r\n\r\nFigure 1: Plot of reserve cumulative distribution of residuals\r\n\r\n\r\n\r\nFrom the reverse cumulative of the absolute residual plot, we can see that there is a higher number of residuals in the left tail of the SVM residual distribution. It shows a higher number of large residuals compared to the other two models. However, RF has a higher number of large residuals than the other models in the right tail of the residual distribution.\r\nIn addition to the cumulative distributions of absolute residuals, we can also compare the distribution of residuals with boxplots by using geom = “boxplot” inside the plot function.\r\n\r\n\r\np2 <- plot(mp_rf, mp_glm, mp_svm, geom = \"boxplot\")\r\np2\r\n\r\n\r\n\r\n\r\nFigure 2: Boxplots of residuals\r\n\r\n\r\n\r\nFigure 2 shows that RF has the lowest median absolute residual value. Although the GLM model has the highest AUC score, the RF model performs best when considering the median absolute residuals. We can also plot the distribution of residuals with histograms by using geom=“histogram” and the precision recall curve by using geom=“prc.”\r\n\r\n\r\n# Activate the patchwork package to combine plots\r\nlibrary(\"patchwork\")\r\n\r\np1 <- plot(mp_rf, mp_glm, mp_svm, geom = \"histogram\") \r\np2 <- plot(mp_rf, mp_glm, mp_svm, geom = \"prc\") \r\np1 + p2\r\n\r\n\r\n\r\n\r\nFigure 3: Histograms for residuals and precision-recall curve\r\n\r\n\r\n\r\nVariable Importance\r\nWhen using machine learning models, it is important to understand which predictors are more influential on the outcome variable. Using the DALEX package, we can see which variables are more influential on the predicted outcome. The variable_importance() function computes variable importance values through permutation, which then can be visually examined using the plot function.\r\n\r\n\r\nvi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square)\r\nvi_glm <- variable_importance(explainer_glm, loss_function = loss_root_mean_square)\r\nvi_svm <- variable_importance(explainer_svm, loss_function = loss_root_mean_square)\r\n\r\nplot(vi_rf, vi_glm, vi_svm)\r\n\r\n\r\n\r\n\r\nFigure 4: Feature importance plots\r\n\r\n\r\n\r\nIn Figure 4, the width of the interval bands (i.e., lines) corresponds to variable importance, while the bars indicate RMSE loss after permutations. Overall, the GLM model seems to have the lowest RMSE, whereas the RF model has the highest RMSE. The results also show that if we list the first two most influential variables on the outcome variable, grade and having school books seem to influence all three models significantly.\r\nAnother function that calculates the importance of variables using permutations is model_parts(). We will use the default loss_fuction - One minus AUC - and set show_boxplots = FALSE this time. Also, we limit the number of variables on the plot with max_vars to show make the plots more readable if there is a large number of predictors in the model.\r\n\r\n\r\nvip_rf  <- model_parts(explainer = explainer_rf,  B = 50, N = NULL)\r\nvip_glm  <- model_parts(explainer = explainer_glm,  B = 50, N = NULL)\r\nvip_svm <- model_parts(explainer = explainer_svm, B = 50, N = NULL)\r\n\r\nplot(vip_rf, vip_glm, vip_svm, max_vars = 4, show_boxplots = FALSE) +\r\n  ggtitle(\"Mean variable-importance over 50 permutations\", \"\") \r\n\r\n\r\n\r\n\r\nFigure 5: Mean variable importance for some predictors\r\n\r\n\r\n\r\nAfter identifying the influential variables, we can show how the machine learning models perform based on different combinations of the predictors.\r\nPartial Dependence Plot\r\nWith the DALEX package, we can also create explainers that show the relationship between a predictor and model output through Partial Dependence Plots (PDP) and Accumulated Local Effects (ALE). These plots show whether or not the relationship between the outcome variable and a predictor is linear and how each predictor affects the prediction process. Therefore, these plots can be created for one predictor at a time. The model_profile() function with the parameter type = “partial” calculates PDP. We will use the grade variable to create a partial dependence plot.\r\n\r\n\r\npdp_rf <- model_profile(explainer_rf, variable = \"grade\", type = \"partial\")\r\npdp_glm <- model_profile(explainer_glm, variable = \"grade\", type = \"partial\")\r\npdp_svm <- model_profile(explainer_svm, variable = \"grade\", type = \"partial\")\r\n\r\nplot(pdp_rf, pdp_glm, pdp_svm)\r\n\r\n\r\n\r\n\r\nFigure 6: Partial dependence of grade in the models\r\n\r\n\r\n\r\nFigure 6 can helps us understand how grade affects the classification of reading performance. The plot shows that the probability (see the y-axis) is low until grade 9 (see the x-axis) but then increases for all of the models. However, it decreases after grade 10 for the RF and SVM models.\r\nAccumulated Local Effects Plot\r\nALE plots are the extension of PDP, which is more suited for correlated variables. The model_profile() function with the parameter type = “accumulated” calculates the ALE curve. Compared with PDP plots, ALE plots are more useful because predictors in machine learning models are often correlated to some extent, and ALE plots take the correlations into account.\r\n\r\n\r\nale_rf  <- model_profile(explainer_rf, variable = \"grade\", type = \"accumulated\")\r\nale_glm  <- model_profile(explainer_glm, variable = \"grade\", type = \"accumulated\")\r\nale_svm  <- model_profile(explainer_svm, variable = \"grade\", type = \"accumulated\")\r\n\r\nplot(ale_rf, ale_glm, ale_svm)\r\n\r\n\r\n\r\n\r\nFigure 7: Accumulated local effect of grade in the models\r\n\r\n\r\n\r\nInstance Level Explanation\r\nUsing DALEX, we can also see how the models behave for a single observation. We can select a particular observation from the data set or define a new observation. We investigate this using the predict_parts() function. This function is a special case of the model_parts(). It calculates the importance of the variables for a single observation while model_parts() calculates it for all observations in the data set.\r\nWe show this single observation level explanation by using the RF model. We could also create the plots for each model and compare the importance of a selected variable across the models. We will use an existing observation (i.e., student 1) from the testing data set.\r\n\r\n\r\nstudent1 <- test[1, 1:11]\r\npp_rf <- predict_parts(explainer_rf, new_observation = student1)\r\n\r\nplot(pp_rf)\r\n\r\n\r\n\r\n\r\nFigure 8: Prediction results for student 1\r\n\r\n\r\n\r\nFigure 8 shows that the prediction probability for the selected observation is 0.34. Also, grade seems to be the most important predictor. Next, we will define a hypothetical student and investigate how the RF model behaves for this student.\r\n\r\n\r\nnew_student <- data.frame(gender = 0,\r\n                          grade = 10,\r\n                          computer = 0,\r\n                          internet = 0,\r\n                          desk=1,\r\n                          own.room=1,\r\n                          quiet.study=1,\r\n                          book.sch = 1,\r\n                          tech.book=1,\r\n                          art.book=1)\r\n\r\npp_rf_new <- predict_parts(explainer_rf, new_observation = new_student)\r\nplot(pp_rf_new)\r\n\r\n\r\n\r\n\r\nFigure 9: Prediction results for a hypothetical student\r\n\r\n\r\n\r\nFor the new student we have defined, the most important variable that affects the prediction is computer. Setting type=“shap,” we can inspect the contribution of the predictors for a single observation.\r\n\r\n\r\npp_rf_shap <- predict_parts(explainer_rf, new_observation = new_student, type = \"shap\")\r\nplot(pp_rf_shap)\r\n\r\n\r\n\r\n\r\nFigure 10: Contributions of the predictors to the prediction process\r\n\r\n\r\n\r\nCeteris Paribus Profiles\r\nIn the previous section, we have discussed the PDP plots. Ceteris Paribus Profiles (CPP) is the single observation level version of the PDP plots. To create this plot, we can use predict_profile() function in the DALEX package. In the following example, we select two predictors for the same observation (i.e., student 1) and create a CPP plot for the RF model. In the plot, blue dots represent the original values for the selected observation.\r\n\r\n\r\nselected_variables <- c(\"grade\", \"gender\")\r\ncpp_rf <- predict_profile(explainer_rf, student1, variables = selected_variables)\r\n\r\nplot(cpp_rf, variables = selected_variables)\r\n\r\n\r\n\r\n\r\nFigure 11: CPP plot for student 1\r\n\r\n\r\n\r\nConclusion\r\nIn this post, we wanted to demonstrate how to use data visualizations to evaluate the performance machine learning models beyond the conventional performance measures. Data visualization tools in the DALEX package enable residual diagnostics of the machine learning models, a comparison of variable importance, and a comprehensive evaluation of the relationship between each predictor and the outcome variable. Also, the package offers tools for visualizing the machine learning models based on a particular observation (either real or hypothetical). We hope that these features of the DALEX package will help you in the comparison and interpretation of machine learning models. More examples of DALEX are available on the DALEX authors’ book (Biecek & Burzykowski, 2021), which is available online at http://ema.drwhy.ai/.\r\n\r\n\r\n\r\nBiecek, P. (2018). DALEX: Explainers for complex predictive models in r. Journal of Machine Learning Research, 19(84), 1–5. https://jmlr.org/papers/v19/18-416.html\r\n\r\n\r\nBiecek, P., & Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/\r\n\r\n\r\nHew, K. F., Qiao, C., & Tang, Y. (2018). Understanding student engagement in large-scale open online courses: A machine learning facilitated analysis of student’s reflections in 18 highly rated MOOCs. International Review of Research in Open and Distributed Learning, 19(3), 70–93. https://doi.org/10.19173/irrodl.v19i3.3596\r\n\r\n\r\nKuhn, M. (2020). Caret: Classification and regression training. https://CRAN.R-project.org/package=caret\r\n\r\n\r\nTan, M., & Shao, P. (2015). Prediction of student dropout in e-learning program through the use of machine learning method. International Journal of Emerging Technologies in Learning, 10(1), 11–17. https://doi.org/10.3991/ijet.v10i1.4189\r\n\r\n\r\nXu, J., Moon, K. H., & Van Der Schaar, M. (2017). A machine learning approach for tracking and predicting student performance in degree programs. IEEE Journal of Selected Topics in Signal Processing, 11(5), 742–753. https://doi.org/10.1109/JSTSP.2017.2692560\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-23-visualizing-machine-learning-models/algorithm.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-5-ways-to-effectively-visualize-survey-data/",
    "title": "5 Ways to Effectively Visualize Survey Data",
    "description": "When presented visually, survey results become much more interesting than some numbers squeezed into a boring table. Data visualizations can help your audience view and understand key insights in the results. There are many data visualization tools to present survey results visually, including bar charts, pie charts, and line charts. In this post, I demonstrate 5 alternative ways to visualize survey results. \n\n(13 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2021-03-04",
    "categories": [
      "survey",
      "data visualization"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nCorrelation Matrix Plot\r\nUpSet Plot\r\nStacked Bar Chart\r\nAlluvial Plot\r\nItem-Person Map\r\n\r\nConclusion\r\n\r\nPhoto by 200degrees on pixabayIntroduction\r\nAs a visual learner, I often use data visualization to present the results of surveys and other types of self-report measures (e.g., psychological scales). In the 2019 annual meeting of the Canadian Society for the Study of Education (CSSE), I gave a half-day workshop on how to visualize assessment and survey results effectively (the workshop slides are available on my GitHub page)1. My goal was to demonstrate readily-available tools that can be used for creating effective visualizations with survey and assessment data. Since my CSSE workshop in 2019, I have come across several other tools that can be quite useful for presenting survey results visually. So, I decided to share them in a blog post.\r\nThere are several ways to visualize data, depending on the type of variables as well as the message to be conveyed to the audience. Figure 1 presents some guidelines regarding which type of visualization to choose depending on the purpose of visualization, the type of variables, and the number of variables. Similar guidelines for data visualization are available on the Venngage website. Also, I definitely recommend you to check out Darkhorse Analytics’ blog post “Data Looks Better Naked.” The authors demonstrate how to create effective visualizations by eliminating redundancy in figures and charts.\r\n\r\n\r\n\r\nFigure 1: Types of data visualization (Source: https://extremepresentation.com/)\r\n\r\n\r\n\r\nIn this post, I will use real data from a questionnaire included in OECD’s Programme for International Student Assessment (PISA). PISA is an international, large-scale assessment administered to 15-year-old students across many countries and economies around the world. In 2015, nearly 540,000 students from 72 countries participated in PISA. After finishing reading, math, and science assessments, students also complete a questionnaire with several items focusing on their learning in school, their attitudes towards different aspects of learning, and non-cognitive/metacognitive constructs. Using students’ responses in the questionnaire, I will demonstrate five alternative tools to visualize survey data.\r\nLet’s get started 📈.\r\nExample\r\nIn this example, we will use a subset of the PISA 2015 dataset that includes students’ responses to some survey items and some demographic variables (e.g., age, gender, and immigration status). The sample only consists of students who participated in PISA 2015 from Alberta, Canada (\\(n = 2,133\\)). The data set is available in a .csv format here.\r\nThere are 10 Likert-type survey items potentially measuring students’ attitudes towards teamwork. The first eight items share the same question statement: “To what extent do you disagree or agree about yourself?” while the last two items are independent. Now let’s see all of the items included in the data.\r\n\r\n\r\nQuestion ID\r\n\r\n\r\nQuestion Statement\r\n\r\n\r\nST082Q01\r\n\r\n\r\nI prefer working as part of a team to working alone.\r\n\r\n\r\nST082Q02\r\n\r\n\r\nI am a good listener.\r\n\r\n\r\nST082Q03\r\n\r\n\r\nI enjoy seeing my classmates be successful.\r\n\r\n\r\nST082Q08\r\n\r\n\r\nI take into account what others are interested in.\r\n\r\n\r\nST082Q09\r\n\r\n\r\nI find that teams make better decisions than individuals.\r\n\r\n\r\nST082Q12\r\n\r\n\r\nI enjoy considering different perspectives.\r\n\r\n\r\nST082Q13\r\n\r\n\r\nI find that teamwork raises my own efficiency.\r\n\r\n\r\nST082Q14\r\n\r\n\r\nI enjoy cooperating with peers.\r\n\r\n\r\nST034Q02\r\n\r\n\r\nI make friends easily at school.\r\n\r\n\r\nST034Q05\r\n\r\n\r\nOther students seem to like me.\r\n\r\n\r\nStudents can answer the items by selecting one of the following response options or skip them without choosing a response option (missing responses are labeled as 999 in the data):\r\n1 = Strongly disagree\r\n2 = Disagree\r\n3 = Agree\r\n4 = Strongly agree\r\nWe will begin the analysis by reading the data in R and previewing the first few rows.\r\n\r\n\r\n# Read the data in R\r\ndata <- read.csv(\"PISA_Alberta.csv\", header = TRUE, na.strings = 999)\r\n\r\n# Preview the data\r\nhead(data)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn the following sections, I will demonstrate how to visualize the items in the PISA data set. The visualizations focus on either individual items or the relationship among the items.\r\nCorrelation Matrix Plot\r\nWe will begin the visualization process by creating a correlation matrix plot. This plot takes the correlation matrix of a group of items as an input and transforms it into a colored table similar to a heatmap. Negative and positive correlations are represented by different colors. Also, the darkness (or lightness) of the colors indicates the strength of pairwise correlations. Using this plot, we can:\r\nunderstand the direction and strength of the relationships among the items,\r\ndetect problematic items (i.e., items that are weakly correlated with the rest of the items), and\r\nidentify the items that may require reverse-coding due to inconsistent phrasing (e.g., negative phrasing in some items).\r\nTo create a correlation matrix plot, we can use either corrplot (Wei & Simko, 2017) or ggcorrplot (Kassambara, 2019). First, we will select the survey items (those starting with ST082 and ST034 in the data). Next, we will save pairwise correlations among the items (i.e., a 10x10 matrix of correlations). Finally, we will create a correlation matrix plot using the two packages above. For preparing the data for data visualizations, we will use the dplyr package (Wickham et al., 2020).\r\n\r\n\r\n# Activate the dplyr package\r\nlibrary(\"dplyr\")\r\n\r\n# Correlation matrix of items\r\ncormat <- data %>%\r\n  select(starts_with(c(\"ST082\", \"ST034\"))) %>%\r\n  cor(., use = \"pairwise.complete.obs\")\r\n\r\n\r\n\r\nNow, let’s create our correlation matrix plot using corrplot. In the corrplot function, order = \"hclust\" applies hierarchical clustering to group the items together based on their correlations with each other. The other option, addrect = 2, refers to the number of rectangles that we want to draw around the item clusters. In our example, we suspect that there may be two clusters of items: one for the first 8 items (focusing on teamwork) and another for the last 2 items (focusing on being liked by friends). Item clusters in Figure 2 seem to confirm our suspicion.\r\n\r\n\r\n# Activate the corrplot package\r\nlibrary(\"corrplot\")\r\n\r\n# Correlation matrix plot\r\ncorrplot(cormat, # correlation matrix\r\n         order = \"hclust\", # hierarchical clustering of correlations\r\n         addrect = 2) # number of rectangles to draw around clusters\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Correlation matrix plot of the PISA survey items (using corrplot)\r\n\r\n\r\n\r\nWe will also use ggcorrplot to create a correlation matrix plot. The procedure is very similar. By using type = \"lower\", we will only visualize the lower diagonal part of the correlation matrix.\r\n\r\n\r\n# Activate the ggcorrplot package\r\nlibrary(\"ggcorrplot\")\r\n\r\n# Correlation matrix plot\r\nggcorrplot(cormat, # correlation matrix\r\n           type = \"lower\", # print the lower part of the correlation matrix\r\n           hc.order = TRUE, # hierarchical clustering of correlations\r\n           lab = TRUE) # add correlation values as labels\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Correlation matrix plot of the PISA survey items (using ggcorrplot)\r\n\r\n\r\n\r\nBoth correlation matrix plots confirm our initial suspicion that the last two items (i.e., ST034Q02 and ST034Q05) are correlated with each other, but not with the rest of the survey items. If we are building a scale focusing on “teamwork,” we can probably exclude these two items since they would not contribute to the scale sufficiently.\r\nUpSet Plot\r\nThe next visualization tool that we will try is called “UpSet plot.” UpSet plots can show the intersections of multiple sets of data. These plots allow users to visualize a complex set of relationships among variables from different data sets. This visualization method enables users to combine different plots (e.g., Venn diagrams, bar charts, etc.) based on the intersections in the data. You can watch this nice video to better understand the design and use of UpSet plots2. In R, the UpSetR package (Gehlenborg, 2019) can be used to generate UpSet plots easily. The authors of the package also have a Shiny app and a web version (https://vcg.github.io/upset/) that help users generate UpSet plots without any coding in R.\r\nIn our example, we will use the naniar package (Tierney et al., 2020) to create an UpSet plot for visualizing missing data. The naniar package offers several tools for visualizing missing data, including UpSet plots3. Using this package, we will create an UpSet plot that will show us missing response patterns in our survey data. First, we will select the survey items in the data and then send the items to the gg_miss_upset function to create an UpSet plot. In the function, nsets = 10 refers to the number of item sets that we want to visualize. So, we use 10 to visualize all of the ten items in the data. In addition, we can use nintersects argument (e.g., nintersects = 10) to change the number of intersections to be shown in the plot.\r\n\r\n\r\n# Activate the naniar package\r\nlibrary(\"naniar\")\r\n\r\ndata %>%\r\n  # Select the survey items\r\n  select(starts_with(c(\"ST082\", \"ST034\"))) %>%\r\n  # Create an UpSet plot\r\n  gg_miss_upset(., nsets = 10)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 4: An UpSet plot showing missing data patterns among the PISA survey items\r\n\r\n\r\n\r\nWe can use Figure 4 to understand the number of missing responses per item. For example, if we look at the first row (ST082Q01), we can see that there are 83 missing responses based on the sum of the three frequencies marked for this item (i.e., 79 + 3 + 1). In addition, we can use the plot to identify missing response patterns. For example, the plot shows that 79 students skipped all of the survey items without selecting an answer (see the first vertical bar), while 13 students skipped either ST034Q05 or ST082Q14 (see the second and third vertical bars). Each column represents a different combination of items with missing responses (i.e., those with black marks). Overall, the UpSet plot can be quite useful for identifying problematic items in terms of missingness.\r\nStacked Bar Chart\r\nThe next visualization tool is the stacked bar chart. Stacked bar charts are not necessarily very exciting forms of data visualization but they are widely used in survey research to visualize Likert-type items (e.g., items with response options of strongly disagree, disagree, agree, and strongly agree). The likert package (Bryer & Speerschneider, 2016) offers a relatively straightforward way to create stacked bar charts for survey items. Without diving into all the options to customize these charts, I will demonstrate how to create a simple stacked bar chart using the first eight items focusing on teamwork. Since the likert package relies on the plyr package, we will have to activate both packages. The resulting plot shows the percentages of responses for each response option across the eight items.\r\n\r\n\r\n# Activate likert and plyr\r\nlibrary(\"likert\")\r\nlibrary(\"plyr\")\r\n\r\n# Select only the first 8 items in the survey\r\nitems <- select(data, starts_with(c(\"ST082\")))\r\n\r\n# Rename the items so that the question statement becomes the name\r\nnames(items) <- c(\r\n  ST082Q01=\"I prefer working as part of a team to working alone.\",\r\n  ST082Q02=\"I am a good listener.\",\r\n  ST082Q03=\"I enjoy seeing my classmates be successful.\",\r\n  ST082Q08=\"I take into account what others are interested in.\",\r\n  ST082Q09=\"I find that teams make better decisions than individuals.\",\r\n  ST082Q12=\"I enjoy considering different perspectives.\",\r\n  ST082Q13=\"I find that teamwork raises my own efficiency.\",\r\n  ST082Q14=\"I enjoy cooperating with peers.\")\r\n\r\n# A custom function to recode numerical responses into ordered factors\r\nlikert_recode <- function(x) {\r\n  y <- ifelse(is.na(x), NA,\r\n              ifelse(x == 1, \"Strongly disagree\",\r\n                     ifelse(x == 2, \"Disagree\",\r\n                            ifelse(x == 3, \"Agree\", \"Strongly agree\"))))\r\n  \r\n  y <- factor(y, levels = c(\"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\"))\r\n  \r\n  return(y)\r\n}\r\n\r\n# Transform the items into factors and save the data set as a likert object\r\nitems_likert <- items %>%\r\n  mutate_all(likert_recode) %>%\r\n  likert()\r\n\r\n# Create a stacked bar chart\r\nplot(items_likert, \r\n     # Group the items alphabetically\r\n     group.order=names(items),\r\n     # Plot the percentages for each response category\r\n     plot.percents = TRUE,\r\n     # Plot the total percentage for negative responses\r\n     plot.percent.low = FALSE,\r\n     # Plot the total percentage for positive responses\r\n     plot.percent.high = FALSE,\r\n     # Whether response categories should be centered\r\n     # This is only helpful when there is a middle response\r\n     # option such as \"neutral\" or \"neither agree nor disagree\"\r\n     centered = FALSE,\r\n     # Wrap label text for item labels\r\n     wrap=30)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 5: A stacked bar chart of the PISA survey items\r\n\r\n\r\n\r\nAlluvial Plot\r\nAlluvial plots (or diagrams) are similar to flow diagrams that represent changes over time or between different groups. Using an alluvial plot, we can demonstrate the flow of survey responses between different items or between different groups of respondents. You can check out the package vignette for more examples. In this example, we will use the ggalluvial package (Brunson, 2020). We will choose one of the survey items and create an alluvial plot using gender, immigration status, and responses given to the item. To transform the data from wide format to long format, we will use the reshape2 package (Wickham, 2007).\r\n\r\n\r\n# Activate reshape2 and ggalluvial\r\nlibrary(\"reshape2\")\r\nlibrary(\"ggalluvial\")\r\n\r\n# Transform the data from wide format to long format\r\ndata_long <- mutate(data,\r\n                    # Create categorical gender and immigration variables\r\n                    gender = ifelse(ST004D01T == 1, \"Female\", \"Male\"),\r\n                    immigration = factor(\r\n                      ifelse(is.na(IMMIG), NA, \r\n                                         ifelse(IMMIG == 1, \"Native\", \r\n                                                ifelse(IMMIG == 2, \"Second-Generation\", \r\n                                                       \"First-Generation\"))),\r\n                      levels = c(\"Native\", \"First-Generation\", \"Second-Generation\")\r\n                    )) %>%\r\n  # Drop old variables\r\n  select(-ST004D01T, -IMMIG) %>%\r\n  # Make the variable names easier\r\n  rename(grade = ST001D01T,\r\n         student = CNTSTUID,\r\n         age = AGE) %>%\r\n  # Melt data to long format\r\n  melt(data = .,\r\n       id.vars = c(\"student\", \"grade\", \"age\", \"gender\", \"immigration\"),\r\n       variable.name = \"question\",\r\n       value.name = \"response\") %>%\r\n  mutate(\r\n    # Recode numerical responses to character strings\r\n    response2 = (ifelse(is.na(response), \"Missing\",\r\n                        ifelse(response == 1, \"Strongly disagree\",\r\n                               ifelse(response == 2, \"Disagree\",\r\n                                      ifelse(response == 3, \"Agree\", \"Strongly agree\"))))),\r\n    # Reorder factors\r\n    response3 = factor(response2, levels = c(\"Missing\", \"Strongly disagree\", \"Disagree\", \r\n                                             \"Agree\", \"Strongly agree\"))\r\n    \r\n  ) \r\n\r\n\r\n# Create a frequency table for one of the items\r\nST082Q01 <- data_long %>%\r\n  filter(question == \"ST082Q01\") %>%\r\n  group_by(gender, immigration, response3) %>%\r\n  summarise(Freq = n())\r\n  \r\n# Create an alluvial plot\r\nggplot(ST082Q01,\r\n       aes(y = Freq, axis1 = gender, axis2 = immigration, axis3 = response3)) +\r\n  geom_alluvium(aes(fill = gender), width = 1/12) +\r\n  geom_stratum(width = 1/12, fill = \"black\", color = \"grey\") +\r\n  geom_label(stat = \"stratum\", aes(label = after_stat(stratum))) +\r\n  scale_x_discrete(limits = c(\"gender\", \"immigration\", \"response3\"), expand = c(.1, .1)) +\r\n  scale_fill_brewer(type = \"qual\", palette = \"Set1\") +\r\n  ggtitle(\"I prefer working as part of a team to working alone.\")\r\n\r\n\r\n\r\n\r\nFigure 6: Alluvial plot of the item ST082Q01, gender, and immigration status\r\n\r\n\r\n\r\nItem-Person Map\r\nThe last data visualization tool that we will review is called the item-person map (also known as the Wright map). The Partial Credit Model (PCM), which is an extension of the Rasch model for polytomous item responses, can be used for survey development and validation (Green & Frantom, 2002). We can analyze survey items using PCM and obtain item thresholds for each item. These thresholds indicate the magnitude of the latent trait required to select a particular response option (e.g., selecting disagree over strongly disagree). Using an item-person map, we can see thresholds for the items as well as the distribution of the latent trait for the respondents. This plot is quite useful for analyzing the alignment (or match) between the items and respondents answering the items.\r\nPCM can be estimated using a variety of R packages, such as mirt (Chalmers, 2012) and eRm (Mair et al., 2020). In the Handbook of Educational Measurement and Psychometrics Using R, we provide a step-by-step demonstration of how to estimate PCM using the mirt package. To create an item-person map, users can use the plotPImap function in the eRm package. Since I typically use the mirt package instead of eRm, I wrote my own function to create an item-person map using PCM results returned from the mirt package. My itempersonmap function is available here.\r\nAfter downloading the R codes for the itempersonmap function, we can use the source function to import it into R. First, we will select the eight items focusing on teamwork (remember that the other two items did not seem to work well with the rest of the items and thus they are excluded here). Next, we will recode the responses from 1-2-3-4 to 0-1-2-34. Finally, we will estimate the item parameters using PCM. In the mirt function, itemtype = \"Rasch\" applies the Rasch model to binary items and PCM to polytomous items. Since our survey responses are polytomous, PCM will be selected for estimating the item thresholds. Note that we add technical = list(removeEmptyRows=TRUE) because there are 79 respondents who skipped all of the items on the survey. Therefore, we have to remove them before estimating the model.\r\nWe are saving the results of the model estimation as “mod.” If the model converges properly, all the information that we need to create an item-person map will be saved in this mirt object. After the estimation is complete, we can simply use itempersonmap(mod) to create an item-person map. In the plot, red points indicate item thresholds for each item (three thresholds separating four response categories) and the asterisk indicates the average value of the thresholds for each item. The higher these thresholds, the more “teamwork” is necessary. On the left-hand side of the plot, we also see the distribution of the latent trait (i.e., the construct of teamwork).\r\n\r\n\r\nsource(\"itempersonmap.R\")\r\n\r\n# Save the items as a separate dataset\r\nitems <- select(data, starts_with(c(\"ST082\"))) %>%\r\n  # Change responses from 1-2-3-4 to 0-1-2-3 by subtracting 1\r\n  apply(., 2, function(x) x-1)\r\n\r\n# Activate the mirt package\r\nlibrary(\"mirt\")\r\n\r\n# Estimate the Partial Credit Model\r\nmod <- mirt(items, 1, itemtype = \"Rasch\", \r\n            technical = list(removeEmptyRows=TRUE),\r\n            verbose = FALSE)\r\n\r\n\r\n# Create the item-person map\r\nitempersonmap(mod)\r\n\r\n\r\n\r\n\r\nFigure 7: An item-person map of the PISA survey items\r\n\r\n\r\n\r\nConclusion\r\nIn this post, I wanted to demonstrate five alternative ways to visualize survey data effectively. Some of the visualizations (e.g., stacked bar chart and alluvial plot) can be used for presenting the survey results, while the other plots (e.g., UpSet plot and correlation matrix plot) can be used for identifying problematic items in the survey. The item-person map is also a very effective tool that can be used for evaluating the overall quality of the survey (i.e., the alignment between items and respondents) and determining whether more items are necessary to measure the latent trait precisely.\r\n\r\n\r\n\r\nBrunson, J. C. (2020). ggalluvial: Layered grammar for alluvial plots. Journal of Open Source Software, 5(49), 2017. https://doi.org/10.21105/joss.02017\r\n\r\n\r\nBryer, J., & Speerschneider, K. (2016). Likert: Analysis and visualization likert items. https://CRAN.R-project.org/package=likert\r\n\r\n\r\nChalmers, R. P. (2012). mirt: A multidimensional item response theory package for the R environment. Journal of Statistical Software, 48(6), 1–29. https://doi.org/10.18637/jss.v048.i06\r\n\r\n\r\nGehlenborg, N. (2019). UpSetR: A more scalable alternative to venn and euler diagrams for visualizing intersecting sets. https://CRAN.R-project.org/package=UpSetR\r\n\r\n\r\nGreen, K. E., & Frantom, C. G. (2002). Survey development and validation with the rasch model. International Conference on Questionnaire Development, Evaluation, and Testing, Charleston, SC, 14–17.\r\n\r\n\r\nKassambara, A. (2019). Ggcorrplot: Visualization of a correlation matrix using ’ggplot2’. https://CRAN.R-project.org/package=ggcorrplot\r\n\r\n\r\nMair, P., Hatzinger, R., & Maier, M. J. (2020). eRm: Extended Rasch Modeling. https://cran.r-project.org/package=eRm\r\n\r\n\r\nTierney, N., Cook, D., McBain, M., & Fay, C. (2020). Naniar: Data structures, summaries, and visualisations for missing data. https://CRAN.R-project.org/package=naniar\r\n\r\n\r\nWei, T., & Simko, V. (2017). R package \"corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot\r\n\r\n\r\nWickham, H. (2007). Reshaping data with the reshape package. Journal of Statistical Software, 21(12), 1–20. http://www.jstatsoft.org/v21/i12/\r\n\r\n\r\nWickham, H., François, R., Henry, L., & Müller, K. (2020). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr\r\n\r\n\r\nThe other training materials are available at https://github.com/okanbulut/dataviz.↩︎\r\nSee Johannes Kepler University’s Visual Data Science Lab for further information.↩︎\r\nCheck out the package’s GitHub page: https://github.com/njtierney/naniar↩︎\r\nThis is what the mirt function requires.↩︎\r\n",
    "preview": "posts/2021-03-04-5-ways-to-effectively-visualize-survey-data/survey.png",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 853
  },
  {
    "path": "posts/2021-02-20-building-a-computerized-adaptive-version-of-psychological-scales/",
    "title": "Building a Computerized Adaptive Version of Psychological Scales",
    "description": "Computerized adaptive testing (CAT) is a sophisticated methodology to create measurement instruments that are highly accurate and efficient. In this post, I explain how to evaluate the feasibility of creating a computerized adaptive version of a psychological instrument. \n\n(12 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2021-02-20",
    "categories": [
      "digital assessments",
      "psychological scales",
      "CAT"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nComputerized Adaptive Testing\r\nExample\r\nConclusion\r\n\r\nPhoto by Glenn Carstens-Peters on UnsplashIntroduction\r\nPsychologists, counselors, educators, and other practitioners often rely on educational and psychological instruments–such as tests, questionnaires, and inventories–to make informed decisions about individuals. Most educational and psychological instruments in use today follow the traditional approach of asking as many items as possible via a paper-and-pencil assessment. Although this approach is likely to increase the internal consistency of the instrument, it may lead to some unintended consequences, such as having low-quality responses due to test fatigue and test-taking disengagement. Therefore, many researchers have proposed systematic ways to shorten educational and psychological instruments (Sandy et al., 2014; Yang et al., 2010; Yarkoni, 2010).\r\nIn my previous posts, I demonstrated how to shorten measurement instruments using psychometric methods such as automated test assembly and data science methods such as the ant colony optimization. These methods can help researchers and practitioners build a shorter version of an instrument and thereby increasing measurement efficiency. However, as Weiss (2004) pointed out, conventional assessments with fixed items (i.e., the same items being used for everyone) tend to yield accurate results for individuals whose trait levels are around the mean of the target population but yield poor measurement results for those whose latent trait levels deviate from the mean.\r\nA promising solution to creating assessments with high measurement accuracy for all individuals is the use of adaptive testing. Adaptive testing follows the idea of adapting an assessment to each individual’s latent trait level by administering a customized set of items, instead of administering the same set of items to all individuals. In this post, I will briefly explain how adaptive testing works and then demonstrate how to create a computerized adaptive version of an existing psychological instrument.\r\nComputerized Adaptive Testing\r\nComputerized adaptive testing (CAT) is a sophisticated method of delivering computerized assessments with high measurement precision and efficiency (Thompson & Weiss, 2011; Weiss, 2004). The primary goal of CAT is to customize the assessment for each individual by selecting the most suitable items based on their responses to the previously administered questions. To design and implement a CAT, the following five components are necessary (Thompson & Weiss, 2011):\r\nItem bank: A large pool of items calibrated with a particular item response theory (IRT) model\r\nStarting point: The point where the test begins (e.g., administering a particular item to everyone or assuming \\(\\theta = 0\\) for all individuals)\r\nItem selection algorithm: How CAT will determine which items should be administered (e.g., Maximum Fisher Information for selecting the most informative item)\r\nScoring algorithm: How latent trait levels will be estimated after each response (e.g., Maximum Likelihood or Expected a Posteriori)\r\nTest termination criteria: How CAT will terminate the test (e.g., answering the maximum number of items or reaching a particular level of measurement precision)\r\nIn a typical CAT, each individual begins to receive items at a particular level (e.g., \\(\\theta = 0\\)). Then, depending on the answer, the next item becomes less or more difficult. For example, if the individual answers an item with moderate difficulty correctly, then the CAT assumes that this individual’s latent trait level is above the difficulty level of the question and thus it presents a more difficult question in the next round. If, however, the individual is not able to answer the item correctly, then she/he is administered an easier question in the next round. this iterative process continues until a test termination criterion is met (e.g., answering the maximum number of questions).\r\nBefore implementing a CAT, it is important to obtain enough evidence to support its use in operational settings. For example, if researchers aim to redesign a conventional, non-adaptive instrument as a CAT, then a series of post-hoc simulations can be conducted using previously collected data. A post-hoc simulation is a psychometric procedure to evaluate the effect of different CAT algorithms under specific conditions (Seo & Choi, 2018). Using real data from a conventional instrument taken by a large group of individuals, one can create a hypothetical CAT scenario and evaluate the impact of various CAT elements (e.g., different algorithms for item selection or scoring) on measurement accuracy and efficiency. The results of these simulations can help researchers and practitioners determine the feasibility and applicability of the CAT approach for an existing instrument.\r\nExample\r\nIn this example, we will use real data from a sample of respondents (\\(n = 4474\\)) who responded to the items in the Taylor Manifest Anxiety Scale (Taylor, 1953) and build a CAT version of this instrument through post-hoc simulations. The Taylor Manifest Anxiety Scale is typically used for measuring anxiety as a personality trait. The scale consists of fifty statements about different indicators of anxiety. For each item, individuals select either true or false. The higher the score, the higher the anxiety level. Because some items on the scale are negatively phrased, they must be reverse-coded1. A clean version of the data (including reverse-coding) is available here. Now let’s import the data and check out its content.\r\n\r\n\r\n# Import the TMA data into R\r\ndata <- read.csv(\"tma_data.csv\", header = TRUE)\r\n\r\n# Preview the data\r\nhead(data)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBefore we start using the data for conducting simulations, we will check psychometric properties of the items. Specifically, we will check the inter-item correlations and point-biserial correlations (i.e., item discrimination) to ensure that the items contribute to the scale sufficiently.\r\n\r\n\r\nlibrary(\"DataExplorer\")\r\n\r\nplot_correlation(data)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: Inter-item Correlations in the Taylor Manifest Anxiety Scale\r\n\r\n\r\n\r\nFigure 1 shows that item 15 has a very low correlation with the rest of the items on the scale. Therefore, we will check item discrimination (i.e., point-biserial correlations) to examine whether item 5, as well as the remaining items, might be problematic. We will use \\(r = 0.20\\) as a threshold to identify problematic items.\r\n\r\n\r\nlibrary(\"psych\")\r\nlibrary(\"ggplot2\")\r\n\r\nitem_summary <- data.frame(Items = factor(colnames(data), levels = colnames(data)),\r\n                           Discrimination = psych::alpha(data)$item.stats$r.cor)\r\n\r\nggplot(data = item_summary, \r\n       aes(x = Items, y = Discrimination, colour = Discrimination > 0.20)) +\r\n  geom_point(size = 3) + ylim(0, 1) +\r\n  geom_hline(yintercept = 0.20, linetype=\"dashed\", colour = \"red\", size = 1) +\r\n  labs(x = \"Items\", \"Discrimination\") +\r\n  coord_flip() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Item Discrimination Values in the Taylor Manifest Anxiety Scale\r\n\r\n\r\n\r\nFigure 2 suggests that item 15 is weakly correlated with the rest of the items on the scale and thus its discrimination value is below 0.20. We can also check the internal consistency of the scale, with and without this item, to see the impact of removing this particular item from the scale.\r\n\r\n\r\ncat(\"Coefficient alpha with all items:\", psych::alpha(data)$total$raw_alpha)\r\n\r\n\r\nCoefficient alpha with all items: 0.902\r\n\r\ncat(\"Coefficient alpha without item 15:\", psych::alpha(data)$alpha.drop$raw_alpha[15])\r\n\r\n\r\nCoefficient alpha without item 15: 0.9045\r\n\r\nWe can see that removing item 15 does not decrease the internal consistency of the scale; but rather it slightly increases the coefficient alpha. Therefore, we will leave item 15 out of the subsequent analyses.\r\n\r\n\r\ndata <- data[,-c(15)]\r\n\r\n\r\n\r\nSince the CAT requires IRT-based item parameters, we will calibrate the items using a particular IRT model. The Taylor Manifest Anxiety Scale is not an achievement test and thus guessing would not be a concern. Thus, we will choose the 2-parameter logistic (2PL) model to calibrate the items and then save the estimated parameters as well as the theta (i.e., anxiety) scores. We will use the mirt package (Chalmers, 2012) for the IRT estimation.\r\n\r\n\r\nlibrary(\"mirt\")\r\n\r\n# Model estimation\r\nmod <- 'F = 1-49' \r\nmodel.2pl <- mirt(data=data, model=mod, itemtype=\"2PL\", SE=TRUE)\r\n\r\n# Saving item parameters as a matrix\r\nparameters <- coef(model.2pl, IRTpars = TRUE, simplify=TRUE)$items\r\nparameters <- as.matrix(parameters)\r\ncolnames(parameters) <- c(\"a\",\"b\",\"c\",\"d\")\r\nhead(parameters)\r\n\r\n# Saving theta values as a matrix\r\ntheta <- fscores(model.2pl, method = \"EAP\")\r\ntheta <- as.matrix(theta)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow we can check out the test information function for the entire scale. Figure 3 indicates that the scale is mostly informative between \\(\\theta = -3\\) and \\(\\theta = 2\\).\r\n\r\n\r\nmirt::plot(model.2pl, type = 'infoSE', theta_lim=c(-5,5), \r\n           auto.key = list(points=TRUE, lines=TRUE, columns=2, space = \"top\"),\r\n           par.settings = list(superpose.line = list(col = \"black\", lty = 1:2, lwd = 2)))\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Test Information Function and Standard Error of Measurement\r\n\r\n\r\n\r\nUsing the item parameters and theta values we have estimated above, we will design a hypothetical CAT scenario. Our goal is to check the feasibility of using the Taylor Manifest Anxiety Scale as a computerized adaptive scale. Therefore, we will run post-hoc simulations using real responses in the data and let the CAT system select the optimal items for each person. Then, we will compare the final theta estimates from the CAT with the original calibration based on all the items.\r\nTo run post hoc simulations, we will use the catR package (Magis & Barrada, 2017).\r\n\r\n\r\nlibrary(\"catR\")\r\n\r\n\r\n\r\nIn the simulation, we will use the following design:\r\nthe item bank consists of 49 items,\r\nthe starting theta level is \\(\\theta = 0\\) for all individuals,\r\nthe Maximum Fisher Information (MFI) will be used for selecting the items,\r\nExpected a Posteriori (EAP) will be used for estimating both interim theta values and final theta values, and\r\neach person will be able to answer up to 10 items in the scale but if a person’s theta estimate is precise enough (i.e., SEM < 0.3), CAT will terminate the test before reaching 10 items.\r\nBefore starting the simulation with the whole data, we will test our CAT system using a single iteration. We will use the randomCAT function to simulate responses for a hypothetical person who takes the Taylor Manifest Anxiety Scale. We will assume that the person’s true theta level is \\(\\theta = 0\\).\r\n\r\n\r\nsim1 <- randomCAT(trueTheta = 0, \r\n                  # Item bank\r\n                  itemBank = parameters,\r\n                  # Set the seed to ensure reproducibility\r\n                  genSeed = 666,\r\n                  # Starting rules for CAT \r\n                  start = list(startSelect=\"MFI\"), \r\n                  # Test administration rules for CAT\r\n                  test = list(method=\"EAP\", itemSelect=\"MFI\"),\r\n                  # Stopping rule for CAT\r\n                  stop = list(rule = c(\"precision\", \"length\"), thr = c(0.3, 10)), \r\n                  # Final ability estimation\r\n                  final = list(method = \"EAP\"),\r\n                  # Save all interim theta and SEM values\r\n                  allTheta = TRUE)\r\n\r\n\r\n\r\nNow we can see how the response process worked for this hypothetical person. The plot_cat function will create an animated plot that shows how theta estimates and 95% SEM band change after administering each item. A function to create a static version of the same plot is available here2.\r\n\r\n\r\nplot_cat <- function(model) {\r\n  require(\"ggplot2\")\r\n  require(\"gganimate\")\r\n  \r\n  # Summarize the results\r\n  cat_summary <- data.frame(\r\n    items = factor(model$testItems, levels = model$testItems),\r\n    thetas = model$thetaProv,\r\n    theta_lb = model$thetaProv - 1.96 * model$seProv,\r\n    theta_ub = model$thetaProv + 1.96 * model$seProv,\r\n    wr = factor(model$pattern, levels=c(0, 1), labels=c(\"Wrong\", \"Right\")))\r\n  \r\n  # Prepare the animated dot plot\r\n  p1 <- ggplot(data=cat_summary, aes(x=items, y=thetas, color=wr)) + \r\n    geom_point(aes(group = seq_along(items)), size = 5) +\r\n    geom_linerange(aes(ymin=theta_lb, ymax=theta_ub), size = 1.5, alpha = 0.4) +\r\n    geom_point(aes(x=tail(items, n = 1), y=tail(thetas, n = 1)), color=\"black\", pch=4, size=5) +\r\n    geom_hline(aes(yintercept = model$trueTheta), \r\n               color=\"#808080\", linetype=\"dotted\", size = 1.5, alpha = 0.7) +\r\n    transition_states(items)+\r\n    coord_cartesian(ylim=c(-4, 4)) + \r\n    scale_size_continuous(range=c(1, 3)) +\r\n    labs(x = \"Items\", y = expression(paste(\"Estimated \", theta)), color = \"Response\",\r\n         caption = \"Note: The dotted horizontal line represents the true theta level.\") +\r\n    guides(size=F, alpha=F) + \r\n    theme_bw(base_size = 19) + \r\n    theme(legend.key=element_blank(),\r\n          plot.caption = element_text(hjust = 0))\r\n  \r\n  # Turn it into an animation\r\n  p2 <- animate(p1, \r\n                nframes = 250,\r\n                fps = 25, \r\n                width = 900, \r\n                height = 600)\r\n  \r\n  return(p2)\r\n}\r\n\r\n\r\nplot_cat(sim1)\r\n\r\n\r\n\r\n\r\nFigure 4: Interim Theta and SEM Estimates in CAT\r\n\r\n\r\n\r\nFigure 4 shows that the 95% SEM band around the theta estimate is getting narrower after administering each item, indicating that the theta estimate is becoming more precise. The final theta estimate is shown with “X” on the right-hand side of the plot. After 10 items, the final theta estimate appears to be very close to the true theta value (see the dotted horizontal line).\r\nTo run a post hoc CAT simulation with real responses in the data, we will use the simulateRespondents function in the catR package. To keep our example simple, we will choose a random sample of 1000 persons (out of 4474 persons) and test the CAT system with this sample.\r\n\r\n\r\n# Set the seed to ensure reproducibility\r\nset.seed(2021)\r\n\r\n# A random sample of 1000 persons\r\npersons <- sample.int(1000, n = nrow(theta))\r\n\r\n# Post hoc simulation #1\r\nsim2 <- simulateRespondents(thetas = theta[persons,],\r\n                            # Item bank\r\n                            itemBank = parameters, \r\n                            # Real responses in the data\r\n                            responsesMatrix = data[persons,],\r\n                            # Starting rules for CAT  \r\n                            start = list(startSelect=\"MFI\"),\r\n                            # Test administration rules for CAT\r\n                            test = list(method=\"EAP\", itemSelect=\"MFI\"),\r\n                            # Stopping rule for CAT\r\n                            stop = list(rule = c(\"precision\", \"length\"), thr = c(0.3, 10)),\r\n                            # Final ability estimation\r\n                            final = list(method = \"EAP\")\r\n)\r\n\r\n\r\n\r\nNow, let’s see the simulation output.\r\n\r\n\r\n# Statistical summary of results\r\nprint(sim2)\r\n\r\n\r\n\r\n ** Post-hoc simulation of multiple examinees ** \r\n \r\nSimulation time: 57.25 seconds \r\n \r\nNumber of simulees: 1000 \r\nItem bank size: 49 items \r\nIRT model: Two-Parameter Logistic model \r\n \r\nItem selection criterion: MFI \r\n Stopping rules: \r\n    Stopping criterion 1: precision of ability estimate\r\n       Maximum SE value: 0.3 \r\n    Stopping criterion 2: length of test\r\n       Maximum test length: 10 \r\n rmax: 1 \r\n\r\nMean test length: 10 items \r\nCorrelation(assigned thetas,CAT estimated thetas): 0.9128 \r\nRMSE: 0.3859 \r\nBias: 0.0093 \r\nMaximum exposure rate: 1 \r\nNumber of item(s) with maximum exposure rate: 1 \r\nMinimum exposure rate: 0 \r\nNumber of item(s) with minimum exposure rate: 22 \r\nItem overlap rate: 0.6013 \r\n \r\nConditional results \r\n                        Measure     D1     D2     D3     D4     D5\r\n                     Mean Theta -1.746 -1.001 -0.608 -0.346 -0.104\r\n                           RMSE  0.371  0.295   0.38  0.368  0.403\r\n                      Mean bias  0.141  0.101  0.115  0.157  0.086\r\n               Mean test length     10     10     10     10     10\r\n            Mean standard error  0.354  0.331  0.369  0.407  0.433\r\n Proportion stop rule satisfied      1      1      1      1      1\r\n             Number of simulees    100    100    100    100    100\r\n    D6    D7    D8     D9    D10\r\n 0.161 0.405 0.644  0.978  1.554\r\n 0.432 0.329  0.32  0.382  0.529\r\n 0.076 0.048 -0.11 -0.118 -0.403\r\n    10    10    10     10     10\r\n 0.474 0.509 0.522  0.573  0.613\r\n     1     1     1      1      1\r\n   100   100   100    100    100\r\n\r\nThese results can be saved by setting 'save.output' to TRUE \r\n  in the 'simulateRespondents' function \r\n\r\nThe simulateRespondents function returns a detailed output of the simulation results. The correlation between the true (i.e., theta based on all 49 items) and estimated theta values is \\(r = 0.912\\). This is is a good result because even with 10 items, individuals’ estimated theta levels are highly correlated with their true theta levels. We can also see that RMSE is 0.386 and bias is 0.009. We are not going to focus on the item exposure results since we did not apply any method to control item exposure given the small size of our item bank.\r\nNext, we will review the “Condition Results” section. The output is split by the latent trait groups (i.e., D1 to D10). This allows us to evaluate the performance of the CAT system across different latent trait levels. We can see that both RMSE and bias are quite high for D1 and D10 (i.e., very low anxiety and very high anxiety groups). This is not a surprising finding given the small size of the item bank. Mean test length indicates that all individuals finished the scale with 10 items. That is, SEM could not be minimized below 0.3 and thus there was no early termination. The row of “Mean standard error” also confirms that the average SEM was higher than 0.3 for all latent trait groups.\r\nWe can also take a look at the visual summary of the results using the plot function.\r\n\r\n\r\n# Visual summary of results\r\nplot(sim2)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 5: Simulation Results\r\n\r\n\r\n\r\nFigure 5 consists of several plots. The accuracy plot shows the alignment between estimated and true theta values. The following figures show a breakdown of bias and RMSE across different theta levels. The test length plot shows that all examinees responded to 10 items, confirming the results we have seen earlier. Lastly, the conditional standard error plot shows that SEM becomes quite high for individuals whose theta level is around \\(\\theta = 0.5\\) or higher.\r\nIn the next simulation, we will keep all the design elements the same but increase the maximum number of items to 15. We will see if administering up to 15 items will reduce bias and RMSE and increase the correlation between true and estimated theta values.\r\n\r\n\r\n# Post hoc simulation #2\r\nsim3 <- simulateRespondents(thetas = theta[persons,],\r\n                            # Item bank\r\n                            itemBank = parameters, \r\n                            # Real responses in the data\r\n                            responsesMatrix = data[persons,],\r\n                            # Starting rules for CAT  \r\n                            start = list(startSelect=\"MFI\"),\r\n                            # Test administration rules for CAT\r\n                            test = list(method=\"EAP\", itemSelect=\"MFI\"),\r\n                            # Stopping rule for CAT\r\n                            stop = list(rule = c(\"precision\", \"length\"), thr = c(0.3, 15)),\r\n                            # Final ability estimation\r\n                            final = list(method = \"EAP\")\r\n)\r\n\r\n\r\n\r\nNow, let’s see the simulation output.\r\n\r\n\r\n# Statistical summary of results\r\nprint(sim3)\r\n\r\n\r\n\r\n ** Post-hoc simulation of multiple examinees ** \r\n \r\nSimulation time: 1.391 minutes \r\n \r\nNumber of simulees: 1000 \r\nItem bank size: 49 items \r\nIRT model: Two-Parameter Logistic model \r\n \r\nItem selection criterion: MFI \r\n Stopping rules: \r\n    Stopping criterion 1: precision of ability estimate\r\n       Maximum SE value: 0.3 \r\n    Stopping criterion 2: length of test\r\n       Maximum test length: 15 \r\n rmax: 1 \r\n\r\nMean test length: 14.68 items \r\nCorrelation(assigned thetas,CAT estimated thetas): 0.9472 \r\nRMSE: 0.3031 \r\nBias: 0.0042 \r\nMaximum exposure rate: 1 \r\nNumber of item(s) with maximum exposure rate: 1 \r\nMinimum exposure rate: 0 \r\nNumber of item(s) with minimum exposure rate: 13 \r\nItem overlap rate: 0.628 \r\n \r\nConditional results \r\n                        Measure     D1     D2     D3     D4     D5\r\n                     Mean Theta -1.746 -1.001 -0.608 -0.346 -0.104\r\n                           RMSE  0.285   0.22  0.279  0.292  0.327\r\n                      Mean bias  0.095  0.049  0.059   0.09  0.062\r\n               Mean test length  13.96  13.31  14.62  14.95  14.98\r\n            Mean standard error  0.326  0.299  0.321  0.351  0.379\r\n Proportion stop rule satisfied      1      1      1      1      1\r\n             Number of simulees    100    100    100    100    100\r\n    D6    D7     D8     D9    D10\r\n 0.161 0.405  0.644  0.978  1.554\r\n 0.331 0.279  0.279  0.339  0.372\r\n 0.018 0.041 -0.089 -0.059 -0.225\r\n    15    15     15     15     15\r\n 0.412 0.451  0.468  0.517  0.568\r\n     1     1      1      1      1\r\n   100   100    100    100    100\r\n\r\nThese results can be saved by setting 'save.output' to TRUE \r\n  in the 'simulateRespondents' function \r\n\r\nThe new output shows that the correlation between the estimated and true levels increased: \\(r = 0.947\\). Furthermore, both RMSE (0.303) and bias (0.004) decreased. Overall, increasing the maximum test length from 10 items to 15 items appears to improve the precision of the estimation. The mean test length is around 14.68 items, indicating that the CAT terminated early with less than 15 items at least for some individuals. When we review bias, RMSE, and mean standard error across the 10 latent trait groups, we see that increasing the maximum test length improved the precision of theta estimates for all groups.\r\nFinally, we can check out the visual summary of the simulation. Figure 6 so that approximately 20% of the individuals answered fewer than 15 items, whereas the remaining individuals answered 15 items before the CAT was terminated. Individuals whose theta levels are between -2 and -0.5 mostly answer fewer than 15 items.\r\n\r\n\r\n# Visual summary of results\r\nplot(sim3)\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 6: Simulation Results\r\n\r\n\r\n\r\nConclusion\r\nThe post-hoc simulation results show that the Taylor Manifest Anxiety Scale can be used as a computerized adaptive instrument. The CAT can shorten the length of the scale without compromising measurement precision. With up to 15 items, the correlation between the estimated and full-scale theta values becomes very high. However, it should be noted that especially for high levels of anxiety, the CAT does not seem to yield highly accurate results. This is directly related to the quality and size of the item bank (only 49 items). For the Taylor Manifest Anxiety Scale to be very accurate in measuring anxiety, the item bank requires additional items that could measure higher levels of anxiety. For a more detailed investigation of the item bank, the randomCAT function could be used to design Monte Carlo simulations based on hypothetical individuals with high levels of anxiety. This simulation can further help researchers determine what type of items they need if they decide to expand the item bank.\r\n\r\n\r\n\r\nChalmers, R. P. (2012). mirt: A multidimensional item response theory package for the R environment. Journal of Statistical Software, 48(6), 1–29. https://doi.org/10.18637/jss.v048.i06\r\n\r\n\r\nMagis, D., & Barrada, J. R. (2017). Computerized adaptive testing with R: Recent updates of the package catR. Journal of Statistical Software, Code Snippets, 76(1), 1–19. https://doi.org/10.18637/jss.v076.c01\r\n\r\n\r\nSandy, C. J., Gosling, S. D., & Koelkebeck, T. (2014). Psychometric comparison of automated versus rational methods of scale abbreviation: An illustration using a brief measure of values. Journal of Individual Differences, 35(4), 221. https://doi.org/10.1027/1614-0001/a000144\r\n\r\n\r\nSeo, D. G., & Choi, J. (2018). Post-hoc simulation study of computerized adaptive testing for the korean medical licensing examination. Journal of Educational Evaluation for Health Professions, 15. https://doi.org/10.3352/jeehp.2018.15.14\r\n\r\n\r\nTaylor, J. A. (1953). A personality scale of manifest anxiety. The Journal of Abnormal and Social Psychology, 48(2), 285. https://doi.org/10.1037/h0056264\r\n\r\n\r\nThompson, N. A., & Weiss, D. A. (2011). A framework for the development of computerized adaptive tests. Practical Assessment, Research, and Evaluation, 16(1), 1. https://doi.org/10.7275/wqzt-9427\r\n\r\n\r\nWeiss, D. J. (2004). Computerized adaptive testing for effective and efficient measurement in counseling and education. Measurement and Evaluation in Counseling and Development, 37(2), 70–84. https://doi.org/10.1080/07481756.2004.11909751\r\n\r\n\r\nYang, C., Nay, S., & Hoyle, R. H. (2010). Three approaches to using lengthy ordinal scales in structural equation models: Parceling, latent scoring, and shortening scales. Applied Psychological Measurement, 34(2), 122–142. https://doi.org/10.1177/0146621609338592\r\n\r\n\r\nYarkoni, T. (2010). The abbreviation of personality, or how to measure 200 personality scales with 200 items. Journal of Research in Personality, 44(2), 180–198. https://doi.org/10.1016/j.jrp.2010.01.002\r\n\r\n\r\nItems 1, 3, 4, 9, 12, 15, 18, 20, 29, 32, 38, and 50 were reverse-coded.↩︎\r\nThis function has been inspired by the plot.cat function in the xxIRT package.↩︎\r\n",
    "preview": "posts/2021-02-20-building-a-computerized-adaptive-version-of-psychological-scales/computer.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-12-a-polytomous-scoring-approach-based-on-item-response-time/",
    "title": "A Polytomous Scoring Approach Based on Item Response Time",
    "description": "In this post, we introduce a polytomous scoring approach based on the optimal use of item response time. This approach provides an easy and practical way to deal with not-reached items in low-stakes assessments. First, we describe how the polytomous scoring approach works and then demonstrate how to implement this approach using R.\n\n(12 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      },
      {
        "name": "Guher Gorgun",
        "url": {}
      }
    ],
    "date": "2021-02-12",
    "categories": [
      "psychometrics",
      "response time",
      "IRT"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nPolytomous Scoring\r\nHow Does It Work?\r\n\r\nExample\r\nConclusion\r\n\r\nPhoto by Veri Ivanova on UnsplashIntroduction\r\nLow-stakes assessments (e.g., formative assessments and progress monitoring measures in K-12) usually have no direct consequences for students. Therefore, some students may not show effortful response behavior when attempting the items on such assessments and leave some items unanswered. These items are typically referred to as not-reached items. For example, some students may try to answer all of the items rapidly and complete the assessment in unrealistically short amounts of time. Oppositely, some students may spend unrealistically long amounts of time on each item and thus fail to finish answering all of the items within the allotted time. Furthermore, students may leave items unanswered due to test speededness-the situation where the allotted time does not allow a large number of students to fully consider all items on the assessment (Lu & Sireci, 2007).\r\nIn practice, not-reached items are often treated as either incorrect or not-administered (i.e., NA) when estimating item and person parameters. However, when the proportion of not-reached items is high, these approaches may yield biased parameter estimates and thereby threatening the validity of assessment results. To date, researchers proposed various model-based approaches to deal with not-reached items, such as modeling valid responses and not-reached items jointly in a tree-based item response theory (IRT) model (Debeer et al., 2017) or modeling proficiency and tendency to omit items as distinct latent traits (Pohl et al., 2014). However, these are typically complex models that would not be easy to use in operational settings.\r\nResponse time spent on each item in an assessment is often considered as a strong proxy for students’ engagement with the items (Kuhfeld & Soland, 2020; Pohl et al., 2019; Rios et al., 2017). Several researchers demonstrated the utility of response times in reducing the effects of non-effortful response behavior such as rapid guessing (e.g., Kuhfeld & Soland (2020), Pohl et al. (2019), Wise & Kong (2005)). By identifying and removing responses where rapid guessing occurred, the accuracy of item and person parameter estimates can be improved, without having to apply a complex model-based approach.\r\nIn this post, we will demonstrate an alternative scoring method that considers not only students with rapid guessing behavior but also students who spend too much time on each item and thereby leaving many items unanswered. In the following sections, we will briefly describe how our scoring approach works and then demonstrate the approach using R.\r\nPolytomous Scoring\r\nIn our recent study (Gorgun & Bulut, 2021)1, we have proposed a new scoring approach that utilizes item response time to transform dichotomous responses into polytomous responses. With our scoring approach, students are able to receive a partial credit on their responses depending on the optimality of their response behavior in terms of response time. This approach combines the speed and accuracy in the scoring process to alleviate the negative impact of not-reached items on the estimation of item and person parameters.\r\nTo conceptualize our scoring approach, we introduce the term of optimal time that refers to spending a reasonable amount of time when responding to an item. Optimal time allows us to make a distinction between students who spend optimal time but miss the item and students who spend too much time on the item and yet answer it incorrectly. By using item response time, we group students into three categories:\r\nOptimal time users who answer the item in a reasonable amount of time,\r\nRapid guessers who answer the item in an unrealistically short amount of time, and\r\nSlow respondents who answer the item in an unrealistically long amount of time.\r\nIf an assessment is timed, students are expected to adjust their speed to attempt as many items as possible within the allotted time. Therefore, spending too little time (rapid guessers) or too much time (slow respondents) on a given item can be considered an outcome of disengaged response behavior. Our scoring approach enables assigning partial credit to optimal time users who answer the item incorrectly but spend optimal time when attempting the item. These students use the time optimally so that they can answer most (or all) of the items.\r\nHow Does It Work?\r\nThe polytomous scoring approach can be implemented using the following steps:\r\nWe separate response time for correct and incorrect responses and then find two median response times for each item: one for correct responses and another for incorrect responses. The median response time is used to avoid the outliers in the response time distribution.\r\nWe use the normative threshold (NT) approach (Wise & Kong, 2005) to find two cut-off values that divide the response time distribution into three regions: optimal time users, rapid guessers, and slow respondents. For example, we can use 25% and 175% of the median response times to specify the optimal time interval2.\r\nAfter finding the cut-off values for the response time distributions for each item, we select a scoring range of 0 to 3 points or 0 to 4 points.\r\nIf the scoring range is 0 to 4:\r\nBased on the correct response time distribution, we assign 4 points to the fastest students, 3 points to the middle region, a.k.a optimal time users, and 2 points to the slowest students.\r\nBased on the incorrect response time distribution, we assign 0 points to rapid guessers and slow respondents and 1 point to the optimal time users.\r\n\r\nIf the scoring range is 0 to 3, the same scoring rule applies for students with incorrect responses. However, for the correct response time distribution, 2 points are given to both rapid guessers and slow respondents, and 3 points are assigned to optimal time users who are in the middle region of the correct response time distribution.\r\n\r\nWe determine how to deal with not-reached items. We can choose to treat not-reached items as either not-administered (i.e, NA) or incorrect.\r\nNow, let’s see how the polytomous scoring approach works in R.\r\nExample\r\nTo illustrate the polytomous scoring approach, we use response data from a sample of 5000 students who participated in a hypothetical assessment with 40 items. In the response data,\r\ncorrect responses are scored as 1,\r\nincorrect responses are scored as 0,\r\nnot-reach items are scored with 9, and\r\nnot-answered items are scored with 83.\r\nThe data also includes students’ response time (in seconds) for each item. The data as a comma-separated-values file (dichotomous_data.csv) is available here.\r\nNow let’s import the data into R and view its content.\r\n\r\n\r\ndata <- read.csv(\"dichotomous_data.csv\", header = TRUE)\r\n\r\n# Item responses\r\nhead(data[,1:40])\r\n\r\n# Item response times\r\nhead(data[,41:80])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNext, we create a scoring function to transform dichotomous responses into polytomous responsed based on the polytomous scoring approach described above. The polyscore function requires the following arguments:\r\nresponse: A vector of students’ responses to an item\r\ntime: A vector of students’ response times on the same item\r\nmax.score: Maximum score for polytomous items (3 for 0-1-2-3 or 4 for 0-1-2-3-4)\r\nnot.reached: Response value for not-reached items (in our data, it is “9”)\r\nnot.answered: Response value for not-answered items (in our data, it is “8”). These responses are automatically recoded as 0 (i.e., incorrect).\r\nna.handle: Treatment of not-reached responses. If “IN,” not-reached responses become 0 (i.e., incorrect); if “NA,” then not-reached responses become NA (i.e., missing).\r\ncorrect.cut: The cut-off proportions to identify rapid guessers and slow respondents who answered the item correctly. The default cut-off proportions are 0.25 and 1.75 for the bottom 25% and the top 75% of the median response time.\r\nincorrect.cut: The cut-off proportions to identify rapid guessers and slow respondents who answered the item incorrectly. The default cut-off proportions are 0.25 and 1.75 for the bottom 25% and the top 75% of the median response time.\r\n\r\n\r\npolyscore <- function(response, time, max.score, na.handle = \"NA\", not.reached, not.answered,  \r\n                      correct.cut = c(0.25, 1.75), incorrect.cut = c(0.25, 1.75)) {\r\n  \r\n  # Find response time thresholds\r\n  median.time.correct1 <- median(time[which(response==1)], na.rm = TRUE)*correct.cut[1]\r\n  median.time.correct2 <- median(time[which(response==1)], na.rm = TRUE)*correct.cut[2]\r\n  median.time.incorrect1 <- median(time[which(response==0)], na.rm = TRUE)*incorrect.cut[1]\r\n  median.time.incorrect2 <- median(time[which(response==0)], na.rm = TRUE)*incorrect.cut[2]\r\n  \r\n  # Recode dichotomous responses as polytomous\r\n  if(max.score == 3) {\r\n    response <- ifelse(response == 1 & time < median.time.correct1, 2,\r\n                       ifelse(response == 1 & time > median.time.correct2, 2,\r\n                              ifelse(response == 1 & \r\n                                       time > median.time.correct1 & \r\n                                       time < median.time.correct2, 3, response)))  \r\n    response <- ifelse(response == 0 & time < median.time.incorrect1, 0,\r\n                       ifelse(response == 0 & time > median.time.incorrect2, 0, \r\n                              ifelse(response == 0 & \r\n                                       time > median.time.incorrect1 & \r\n                                       time < median.time.incorrect2, 1, response)))\r\n  } else if (max.score == 4)  {\r\n    response <- ifelse(response == 1 & time < median.time.correct1, 4,\r\n                       ifelse(response == 1 & time > median.time.correct2, 2, \r\n                              ifelse(response == 1 & \r\n                                       time > median.time.correct1 & \r\n                                       time < median.time.correct2, 3, response)))\r\n    response <- ifelse(response == 0 & time < median.time.incorrect1, 0,\r\n                       ifelse(response == 0 & time > median.time.incorrect2, 0, \r\n                              ifelse(response == 0 & \r\n                                       time > median.time.incorrect1 & \r\n                                       time < median.time.incorrect2, 1, response)))\r\n  }\r\n  \r\n  # Set not-answered responses as incorrect\r\n  if(!is.null(not.answered)) {\r\n    response.recoded <- ifelse(response == not.answered, 0, response)\r\n  } else {\r\n    response.recoded <- response\r\n  }\r\n  \r\n  # Set not-reached responses as NA or incorrect\r\n  if(na.handle == \"IN\") {\r\n    response.recoded <- ifelse(response.recoded == not.reached, 0, response.recoded)\r\n  } else {\r\n    response.recoded <- ifelse(response.recoded == not.reached, NA, response.recoded)\r\n  }\r\n  \r\n  return(response.recoded)\r\n} \r\n\r\n\r\n\r\nBefore we start using the polyscore function, let’s see how rapid guessers, slow respondents, and optimal time users are identified using one of the items (item 1).\r\n\r\n\r\nlibrary(\"patchwork\")\r\nlibrary(\"ggplot2\")\r\n\r\n# Response time distribution for correct\r\np1 <- ggplot(data = data[data$item_1==1, ], \r\n             aes(x = rt_1)) +\r\n  geom_histogram(color = \"white\", \r\n                 fill = \"steelblue\", \r\n                 bins = 40) + ylim(0, 250) + \r\n  geom_vline(xintercept = median(data[data$item_1==1, \"rt_1\"])*0.25, \r\n             linetype=\"dashed\", color = \"red\", size = 1) +\r\n  geom_vline(xintercept = median(data[data$item_1==1, \"rt_1\"])*1.75, \r\n             linetype=\"dashed\", color = \"red\", size = 1) +\r\n  labs(x = \"Response time for item 1 (Correct)\") + \r\n  annotate(geom=\"text\", x = -1, y = 200, label=\"Rapid\\n guessers\") + \r\n  annotate(geom=\"text\", x = 8, y = 200, label=\"Optimal\\n time users\") +\r\n  annotate(geom=\"text\", x = 15, y = 200, label=\"Slow\\n respondents\") +\r\n  theme_bw()\r\n\r\n# Response time distribution for incorrect\r\np2 <- ggplot(data = data[data$item_1==0, ], \r\n             aes(x = rt_1)) +\r\n  geom_histogram(color = \"white\", \r\n                 fill = \"steelblue\", \r\n                 bins = 40) + ylim(0, 250) + \r\n  geom_vline(xintercept = median(data[data$item_1==0, \"rt_1\"])*0.25, \r\n             linetype=\"dashed\", color = \"red\", size = 1) +\r\n  geom_vline(xintercept = median(data[data$item_1==0, \"rt_1\"])*1.75, \r\n             linetype=\"dashed\", color = \"red\", size = 1) +\r\n  labs(x = \"Response time for item 1 (Incorrect)\") +\r\n  annotate(geom=\"text\", x = -1, y = 200, label=\"Rapid\\n guessers\") + \r\n  annotate(geom=\"text\", x = 5, y = 200, label=\"Optimal\\n time users\") +\r\n  annotate(geom=\"text\", x = 15, y = 200, label=\"Slow\\n respondents\") +\r\n  theme_bw()\r\n\r\n# Print the plots together\r\n(p1 / p2)\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow we can go ahead and apply polytomous scoring to our data. First, we separate the response and response time portions of the data.\r\n\r\n\r\nresp_data <- data[, 1:40]\r\ntime_data <- data[, 41:80]\r\n\r\n\r\n\r\nNext, we implement the polytomous scoring approach using different combinations of max.score and na.handle. For each combination, we will apply the polyscore function to the items through a loop.\r\npolydata3_NA: Scores of 0-1-2-3 and not-reached items = NA\r\npolydata3_IN: Scores of 0-1-2-3 and not-reached items = IN\r\npolydata4_NA: Scores of 0-1-2-3-4 and not-reached items = NA\r\npolydata4_IN: Scores of 0-1-2-3-4 and not-reached items = IN\r\n\r\n\r\n# Define empty matrices to store polytomous responses\r\npolydata3_NA <- matrix(NA, nrow = nrow(resp_data), ncol = ncol(resp_data))\r\npolydata3_IN <- matrix(NA, nrow = nrow(resp_data), ncol = ncol(resp_data))\r\npolydata4_NA <- matrix(NA, nrow = nrow(resp_data), ncol = ncol(resp_data))\r\npolydata4_IN <- matrix(NA, nrow = nrow(resp_data), ncol = ncol(resp_data))\r\n\r\n# Run the function within a loop\r\nfor(i in 1:ncol(resp_data)) {\r\n  \r\n  polydata3_NA[,i] <- polyscore(resp_data[,i], time_data[,i], max.score = 3, \r\n                                na.handle = \"NA\", not.reached = 9, not.answered = 8,  \r\n                                correct = c(0.25, 1.75), incorrect = c(0.25, 1.75))\r\n  \r\n  polydata3_IN[,i] <- polyscore(resp_data[,i], time_data[,i], max.score = 3, \r\n                                na.handle = \"IN\", not.reached = 9, not.answered = 8,  \r\n                                correct = c(0.25, 1.75), incorrect = c(0.25, 1.75))\r\n  \r\n  polydata4_NA[,i] <- polyscore(resp_data[,i], time_data[,i], max.score = 4, \r\n                                na.handle = \"NA\", not.reached = 9, not.answered = 8,  \r\n                                correct = c(0.25, 1.75), incorrect = c(0.25, 1.75))\r\n  \r\n  polydata4_IN[,i] <- polyscore(resp_data[,i], time_data[,i], max.score = 4, \r\n                                na.handle = \"IN\", not.reached = 9, not.answered = 8,  \r\n                                correct = c(0.25, 1.75), incorrect = c(0.25, 1.75))\r\n} \r\n\r\n# Save the data as a data.frame and rename the columns\r\npolydata3_NA <- as.data.frame(polydata3_NA)\r\nnames(polydata3_NA) <- names(resp_data)\r\n\r\npolydata3_IN <- as.data.frame(polydata3_IN)\r\nnames(polydata3_IN) <- names(resp_data)\r\n\r\npolydata4_NA <- as.data.frame(polydata4_NA)\r\nnames(polydata4_NA) <- names(resp_data)\r\n\r\npolydata4_IN <- as.data.frame(polydata4_IN)\r\nnames(polydata4_IN) <- names(resp_data)\r\n\r\n\r\n\r\nLet’s quickly see how one of the polytomous data sets looks like after applying polytomous scoring.\r\n\r\n\r\nhead(polydata3_NA)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn this example, we will use polytomously-scored items with Item Response Theory (IRT) to estimate item and person parameters. However, it can also be used with Classical Test Theory (CTT) to calculate raw scores. Regardless of whether IRT or CTT is used, speededness is no longer a nuisance variable because it is used in the operationalization of ability (Tijmstra & Bolsinova, 2018).\r\nNow, we will activate the mirt package (Chalmers, 2012) before conducting IRT analysis.\r\n\r\n\r\nlibrary(\"mirt\")\r\n\r\n\r\n\r\nNext, we will define a unidimensional model with 40 items and then fit the Graded Response Model (Samejima, 1997) to each polytomous data set.\r\n\r\n\r\nmodel <- 'F = 1-40' \r\n\r\n### Apply GRM to each data set\r\nresults3_NA <- mirt(data=polydata3_NA, model=model, itemtype=\"graded\", SE=TRUE, verbose=FALSE)\r\nresults3_IN <- mirt(data=polydata3_IN, model=model, itemtype=\"graded\", SE=TRUE, verbose=FALSE)\r\nresults4_NA <- mirt(data=polydata4_NA, model=model, itemtype=\"graded\", SE=TRUE, verbose=FALSE)\r\nresults4_IN <- mirt(data=polydata4_IN, model=model, itemtype=\"graded\", SE=TRUE, verbose=FALSE)\r\n\r\n\r\n\r\nNow, we can see the ability distributions from the four models. The figure shows that the ability distributions become narrower (with high kurtosis) when not-reached items are scored as incorrect, whereas the distribution looks more normal when not-reached items are scored as NA.\r\n\r\n\r\n# Save theta estimates\r\ntheta3_NA <- fscores(results3_NA, method = \"EAP\")\r\ntheta3_IN <- fscores(results3_IN, method = \"EAP\")\r\ntheta4_NA <- fscores(results4_NA, method = \"EAP\")\r\ntheta4_IN <- fscores(results4_IN, method = \"EAP\")\r\n\r\n# Combine theta estimates in a data frame\r\ntheta_estimates <- data.frame(\r\n  GRM3_NA = theta3_NA[,1],\r\n  GRM3_IN = theta3_IN[,1],\r\n  GRM4_NA = theta4_NA[,1],\r\n  GRM4_IN = theta4_IN[,1]\r\n)\r\n\r\n# Create a histogram for each model\r\np1 <- ggplot(data = theta_estimates) +\r\n  geom_histogram(aes(x = GRM3_NA), bins = 50, fill = \"blue\", alpha = 0.5) +\r\n  xlab(\"Theta - GRM3_NA\") + xlim(-5.5, 5.5) + ylim(0, 600) +\r\n  theme_bw()\r\n\r\np2 <- ggplot(data = theta_estimates) +\r\n  geom_histogram(aes(x = GRM3_IN), bins = 50, fill = \"green\", alpha = 0.5) +\r\n  xlab(\"Theta - GRM3_IN\") + xlim(-5.5, 5.5) + ylim(0, 600) +\r\n  theme_bw()\r\n\r\np3 <- ggplot(data = theta_estimates) +\r\n  geom_histogram(aes(x = GRM4_NA), bins = 50, fill = \"yellow\", alpha = 0.5) +\r\n  xlab(\"Theta - GRM4_NA\") + xlim(-5.5, 5.5) + ylim(0, 600) +\r\n  theme_bw()\r\n\r\np4 <- ggplot(data = theta_estimates) +\r\n  geom_histogram(aes(x = GRM4_IN), bins = 50, fill = \"red\", alpha = 0.5) +\r\n  xlab(\"Theta - GRM4_IN\") + xlim(-5.5, 5.5) + ylim(0, 600) +\r\n  theme_bw()\r\n\r\n# Combine the histograms\r\n(p1 | p2)/(p3 | p4)\r\n\r\n\r\n\r\n\r\nWe can also examine test information function (TIF) based on the estimated person parameters using the polytomous scoring approach. The figure shows that GRM3-NA and GRM4-NA are more informative than the other two models where not-reached items were scored as incorrect. Furthermore, compared with GRM3-IN and GRM4-IN, GRM3-NA and GRM4-NA cover a wider range of ability (i.e., theta).\r\n\r\n\r\nTheta <- matrix(seq(-4, 4, .01))\r\n\r\ntif_summary <- data.frame(Theta = Theta,\r\n                          tif = c(testinfo(results3_NA, Theta = Theta),\r\n                                  testinfo(results3_IN, Theta = Theta),\r\n                                  testinfo(results4_NA, Theta = Theta),\r\n                                  testinfo(results4_IN, Theta = Theta)),\r\n                          scoring = c(rep(\"GRM3-NA\", length(Theta)),\r\n                                      rep(\"GRM3-IN\", length(Theta)),\r\n                                      rep(\"GRM4-NA\", length(Theta)),\r\n                                      rep(\"GRM4-IN\", length(Theta))))\r\n\r\nggplot(data = tif_summary, \r\n       aes(x = Theta, y = tif, linetype = scoring, color = scoring)) +\r\n  geom_line(size = 1) + labs(x = \"Theta\", y = \"TIF\") +\r\n  theme_bw() +\r\n  theme(legend.title=element_blank())\r\n\r\n\r\n\r\n\r\nConclusion\r\nPolytomous scoring is an alternative approach when scoring students’ responses in a low-stakes assessment. This approach takes test engagement into account by considering the optimality of response time. Using polytomous scoring can encourage students to avoid response behaviors such as rapid guessing and spending too much time on some items. It should be noted that for this approach to work accurately and effectively, response time cut-off values should be obtained from a large group of students that is representative of the target student population.\r\n\r\n\r\n\r\nChalmers, R. P. (2012). mirt: A multidimensional item response theory package for the R environment. Journal of Statistical Software, 48(6), 1–29. https://doi.org/10.18637/jss.v048.i06\r\n\r\n\r\nDebeer, D., Janssen, R., & De Boeck, P. (2017). Modeling skipped and not-reached items using IRTrees. Journal of Educational Measurement, 54(3), 333–363.\r\n\r\n\r\nGorgun, G., & Bulut, O. (2021). A polytomous scoring approach to handle not-reached items in low-stakes assessments. Educational and Psychological Measurement. https://doi.org/10.1177/0013164421991211\r\n\r\n\r\nKuhfeld, M., & Soland, J. (2020). Using assessment metadata to quantify the impact of test disengagement on estimates of educational effectiveness. Journal of Research on Educational Effectiveness, 13(1), 147–175.\r\n\r\n\r\nLu, Y., & Sireci, S. G. (2007). Validity issues in test speededness. Educational Measurement: Issues and Practice, 26(4), 29–37. https://doi.org/10.1111/j.1745-3992.2007.00106.x\r\n\r\n\r\nPohl, S., Gräfe, L., & Rose, N. (2014). Dealing with omitted and not-reached items in competence tests: Evaluating approaches accounting for missing responses in item response theory models. Educational and Psychological Measurement, 74(3), 423–452.\r\n\r\n\r\nPohl, S., Ulitzsch, E., & Davier, M. von. (2019). Using response times to model not-reached items due to time limits. Psychometrika, 84(3), 892–920.\r\n\r\n\r\nRios, J. A., Guo, H., Mao, L., & Liu, O. L. (2017). Evaluating the impact of careless responding on aggregated-scores: To filter unmotivated examinees or not? International Journal of Testing, 17(1), 74–104.\r\n\r\n\r\nSamejima, F. (1997). Graded response model. In Handbook of modern item response theory (pp. 85–100). Springer.\r\n\r\n\r\nTijmstra, J., & Bolsinova, M. (2018). On the importance of the speed-ability trade-off when dealing with not reached items. Frontiers in Psychology, 9, 964.\r\n\r\n\r\nWise, S. L., & Kong, X. (2005). Response time effort: A new measure of examinee motivation in computer-based tests. Applied Measurement in Education, 18(2), 163–183.\r\n\r\n\r\nAn open-access version of our paper is available:https://journals.sagepub.com/doi/10.1177/0013164421991211.↩︎\r\nSmaller percentages can be used for obtaining more conservative cut-off values.↩︎\r\nThese are items that students view but skip without selecting a valid response option.↩︎\r\n",
    "preview": "posts/2021-02-12-a-polytomous-scoring-approach-based-on-item-response-time/clock1.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-19-how-to-shorten-a-measurement-instrument-automatically-part-ii/",
    "title": "How to Shorten a Measurement Instrument Automatically (Part II)",
    "description": "Do you think there are too many questions on your survey? Are you worried that participants \nmay get tired of responding to the questions in the middle of the survey? In this two-part series, I \ndemonstrate how to shorten measurement instruments such as surveys automatically in R. The second part \nfocuses on the use of two optimization algorithms (genetic algorithm and ant colony optimization) for\nreducing the number of questions in surveys and similar instruments.\n\n(13 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      },
      {
        "name": "Hatice Cigdem Bulut",
        "url": {}
      }
    ],
    "date": "2021-01-19",
    "categories": [
      "psychometrics",
      "machine learning",
      "optimization"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nOptimization with GA and ACO\r\n\r\nExample\r\nAutomatic Selection of Items\r\nGenetic Algorithm\r\nAnt Colony Optimization\r\n\r\nConclusion\r\n\r\nPhoto by Poranimm Athithawatthee on PexelsIntroduction\r\nIn the social and behavioral sciences, researchers often use surveys, questionnaires, or scales to collect data from a sample of participants. Such instruments provide an efficient and effective way to collect information about a large group of individuals.\r\n\r\nSurveys are used to collect information from or about people to describe, compare, or explain their knowledge, feelings, values, and behaviors. (Fink, 2015)\r\n\r\nSometimes respondents may get tired of answering the questions during the survey-taking process—especially if the survey is very long. This is known as survey taking fatigue. The presence of survey taking fatigue can affect response quality significantly. When respondents get tired, they may skip questions, provide inaccurate responses due to insufficient effort responding, or even abandon the survey completely. To alleviate this issue, it is important to reduce survey length properly.\r\nIn the first part of this series, I demonstrated how to use automated test assembly and recursive feature elimination to automatically shorten educational assessments (e.g., multiple-choice exams, tests, and quizzes). In the second part, I will demonstrate how to use the following optimization algorithms for creating shorter versions of surveys and similar instruments:\r\nGenetic algorithm (GA; 🧬)\r\nAnt colony optimization (ACO; 🐜)\r\nOptimization with GA and ACO\r\nIn computer science, a genetic algorithm (GA) is essentially a search heuristic that mimics Charles Darwin’s theory of natural evolution. The algorithm reflects the process of natural selection by iteratively selecting and mating strong individuals (i.e., solutions) that are more likely to survive and eliminating weak individuals from generating more weak individuals. This process continues until GA finds an optimal or acceptable solution.\r\n\r\nGenetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. (Mitchell, 1998)\r\n\r\nIn the context of survey abbreviation, GA can be used as an optimization tool for finding a subset of questions that maximally captures the variance (i.e., \\(R^2\\)) in the original data. Yarkoni (2010) proposed the following cost function for scale abbreviation:\r\n\\[Cost = Ik + \\Sigma^s_i w_i(1-R^2_i)\\] where \\(I\\) represents a user-specified fixed item cost, \\(k\\) represents the number of items to be retained by GA, \\(s\\) is the number of subscales in the measure, \\(w_i\\) are the weights associated with each subscale, and \\(R^2\\) is the amount of variance in the \\(i^{th}\\) subscale explained by its items. If the cost of retaining a particular item is larger than the loss in \\(R^2\\), then the item is dropped from its subscale (i.e., GA returns a shorter subscale). Yarkoni (2010) demonstrated the use of GA in abbreviating lengthy personality scales and thereafter many researchers have used GA to abbreviate psychological scales (e.g., Crone et al. (2020), Eisenbarth et al. (2015), Sahdra et al. (2016))1.\r\nLike GA, the ant colony optimization (ACO) is also an optimization method. ACO was first inspired by the collective behavior of Argentine ants called iridomyrmex humilis (Goss et al., 1989). While searching for food, these ants drop pheromone on the ground and follow pheromone previously dropped by other ants. Since the shortest path is more likely to retain pheromone, ants can follow this path and find promising food sources more quickly. Figure 1 illustrates this process.\r\nFigure 1: How real ants find a shortest path. (a) Ants arrive at a decision point. (b) Some ants choose the upper path and some the lower path. The choice is random. (c) Since ants move at approximately a constant speed, the ants which choose the lower, shorter, path reach the opposite decision point faster than those which choose the upper, longer, path. (d) Pheromone accumulates at a higher rate on the shorter path. The number of dashed lines is approximately proportional to the amount of pheromone deposited by ants (Obtained from Dorigo et al. (1996), page 54)Engineers decided to use the way Argentine ant colonies function as an analogy to solve the shortest path problem and created the ACO algorithm (Dorigo et al., 1996; Dorigo & Gambardella, 1997). Then, G. A. Marcoulides & Drezner (2003) applied ACO to model specification searches in structural equation modeling (SEM). The goal of this approach is to automate the model fitting process in SEM by starting with a user-specified model and then fitting alternative models to fix missing paths or parameters. This iterative process continues until an optimal model (e.g., a model with good model-fit indices) is identified. Leite et al. (2008) used the ACO algorithm for the development of short forms of scales and found that ACO outperformed traditionally used methods of item selection. In a more recent study, K. M. Marcoulides & Falk (2018) demonstrated how to use ACO for model specification searches in R.\r\nExample\r\nIn this example, we will use the Experiences in Close Relationships (ECR) scale (Brennan et al., 1998). The ECR scale consists of 36 items measuring two higher-order attachment dimensions for adults (18 items per dimension): avoidance and anxiety (see Figure 2)2. The items are based on a 5-point Likert scale (i.e., 1 = strongly disagree to 5 = strongly agree). For each subscale (i.e., dimension), higher scores indicate higher levels of avoidance (or anxiety). Individuals who score high on either or both of these dimensions are assumed to have an insecure adult attachment orientation (Wei et al., 2007).\r\nFigure 2: The two-dimensional model of individual differences in adult attachment (Adapted from the figure at http://labs.psychology.illinois.edu/~rcfraley/measures/measures.html)Wei et al. (2007) developed a 12-item, short form of the ERC scale using traditional methods (e.g., dropping items with low item-total correlation and keeping items with the highest factor loadings). In our example, we will use the GA and ACO algorithms to automatically select the best items for the two subscales of the ECR scale. The original data set for the ERC scale is available on the Open-Source Psychometric Project website. For demonstration purposes, we will use a subset (\\(n = 10,798\\)) of the original data set based on the following rules:\r\nRespondents must participate in the survey from the United States, and\r\nRespondents must be between 18 and 30 years of age.\r\nIn the ERC scale, some items are positively phrased and thus indicate lower avoidance (or anxiety) for respondents. Therefore, these items (items 3, 15, 19, 22, 25, 29, 31, 33, 35) have been reverse-coded3. Lastly, the respondents with missing responses have been eliminated from the data set. The final data set is available here.\r\nNow, let’s import the data into R and then preview its content.\r\n\r\n\r\necr <- read.csv(\"ecr_data.csv\", header = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNext, we will review inter-item correlations for each subscale to confirm that the items within each subscale are in the same direction. The odd-numbered items belong to the Avoidance subscale, while the even-numbered items belong to the Anxiety subscale.\r\n\r\n\r\nlibrary(\"DataExplorer\")\r\n\r\n# Avoidance subscale\r\nplot_correlation(ecr[, seq(1, 35, by = 2)])\r\n\r\n\r\n\r\n# Anxiety subscale\r\nplot_correlation(ecr[, seq(2, 36, by = 2)])\r\n\r\n\r\n\r\n\r\nThe two correlation matrix plots above indicate that the items within each subscale are positively correlated with each other (i.e., the responses are in the same direction).\r\nLastly, we will check the reliability (i.e, coefficient alpha) for each subscale based on the original number of items (i.e., 18 items per subscale).\r\n\r\n\r\n# Avoidance subscale\r\npsych::alpha(ecr[, seq(1, 35, by = 2)])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N       ase  mean     sd\r\n    0.9438    0.9443  0.9511    0.4848 16.94 0.0007861 2.558 0.8587\r\n median_r\r\n   0.4654\r\n\r\n# Anxiety subscale\r\npsych::alpha(ecr[, seq(2, 36, by = 2)])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N      ase  mean     sd\r\n     0.914    0.9141  0.9263    0.3716 10.64 0.001207 3.117 0.7912\r\n median_r\r\n   0.3375\r\n\r\nNow, we will go ahead and shorten the instrument using GA and ACO.\r\nAutomatic Selection of Items\r\nGoal: Using both GA and ACO, we aim to select six items for each subscale. That is, we want to create a short version of the ECR scale with 12 items in total.\r\nAssumptions: We will assume that: (1) the items that belong to each subscale measure the same latent trait (i.e., either avoidance or anxiety), and the subscales do not exhibit any psychometric issues (e.g., differential item functioning).\r\nMethodology: We will use the GA and ACO algorithms to automatically select the items from the ECR subscale. Although these algorithms aim to shorten the subscales, they follow selection mechanisms based on different criteria. These will be explained in the following sections.\r\nGenetic Algorithm\r\nWe will use the GAabbreviate function from the GAabbreviate package (Scrucca & Sahdra, 2016) to implement GA for scale abbreviation.\r\n\r\n\r\nlibrary(\"GAabbreviate\")\r\n\r\n\r\n\r\nStep 1: We will create scale scores by summing up the item scores for each dimension4. As mentioned earlier, the odd-numbered items define “Avoidance” and the even-numbered items define “Anxiety.”\r\n\r\n\r\nscales = cbind(rowSums(ecr[, seq(1, 35, by = 2)]), \r\n               rowSums(ecr[, seq(2, 36, by = 2)])\r\n               )\r\n\r\n\r\n\r\nStep 2: We will transform each item response into an integer and then save the response data set as a matrix.\r\n\r\n\r\necr <- as.data.frame(sapply(ecr, as.integer))\r\necr <- matrix(as.integer(unlist(ecr)), nrow=nrow(ecr))\r\n\r\n\r\n\r\nStep 3: We will use the GAabbreviate function to execute the GA algorithm. In the function, a few parameters need to be determined by trial and error. For example, itemCost refers to the fitness cost of each item. The default value in the function is 0.05. By lowering or increasing the cost, we can check whether the function is able to shorten the scale (or subscales) based on the target length (which is 12 items in our example; 6 items per subscale). Similarly, the maximum number of iterations (maxiter) can be increased if the function is not able to find a solution within the default number of iterations (100).\r\n\r\n\r\necr_GA = GAabbreviate(items = ecr, # Matrix of item responses\r\n                      scales = scales, # Scale scores\r\n                      itemCost = 0.01, # The cost of each item\r\n                      maxItems = 6, # Max number of items per dimension\r\n                      maxiter = 1000, # Max number of iterations\r\n                      run = 100, # Number of runs\r\n                      crossVal = TRUE, # Cross-validation\r\n                      seed = 2021) # Seed for reproducibility\r\n\r\n\r\n\r\n\r\n\r\n\r\nOnce the GAabbreviate function begins to run, the iteration information during the search is displayed:\r\n\r\nStarting GA run...\r\nIter = 1  | Mean = 0.4265  | Best = 0.3455 \r\nIter = 2  | Mean = 0.4238  | Best = 0.3455 \r\nIter = 3  | Mean = 0.4204  | Best = 0.3334 \r\nIter = 4  | Mean = 0.4101  | Best = 0.3255 \r\nIter = 5  | Mean = 0.4028  | Best = 0.3255 \r\nIter = 6  | Mean = 0.3983  | Best = 0.3175 \r\nIter = 7  | Mean = 0.3913  | Best = 0.3175 \r\nIter = 8  | Mean = 0.3983  | Best = 0.3175 \r\nIter = 9  | Mean = 0.3926  | Best = 0.3175 \r\nIter = 10 | Mean = 0.3927  | Best = 0.3175 \r\nIter = 11 | Mean = 0.3898  | Best = 0.3175 \r\n.\r\n.\r\n.\r\n.\r\n\r\nStep 4: If the GAabbreviate function finds an optimal solution before the maximum number of iterations, it automatically stops; otherwise, it continues until the maximum number of iterations is reached. Then, we can view the results as follows:\r\n\r\n\r\necr_GA$measure\r\n\r\n\r\n$items\r\n x2  x7 x10 x11 x14 x15 x16 x18 x23 x27 x29 x30 \r\n  2   7  10  11  14  15  16  18  23  27  29  30 \r\n\r\n$nItems\r\n[1] 12\r\n\r\n$key\r\n      [,1] [,2]\r\n [1,]    0    1\r\n [2,]    1    0\r\n [3,]    0    1\r\n [4,]    1    0\r\n [5,]    0    1\r\n [6,]    1    0\r\n [7,]    0    1\r\n [8,]    0    1\r\n [9,]    1    0\r\n[10,]    1    0\r\n[11,]    1    0\r\n[12,]    0    1\r\n\r\n$nScaleItems\r\n[1] 6 6\r\n\r\n$alpha\r\n         A1     A2\r\nalpha 0.832 0.7908\r\n\r\n$ccTraining\r\n[1] 0.9644 0.9529\r\n\r\n$ccValidation\r\n[1] 0.9635 0.9530\r\n\r\nThe output above shows a list of which items have been selected (items) and which of those items belong to each dimension (key). To combine the two pieces together, we can use the following:\r\n\r\n\r\navoidance <- which(ecr_GA$measure$key[,1]==1)\r\nanxiety <- which(ecr_GA$measure$key[,2]==1)\r\n\r\n# Avoidance items\r\necr_GA$measure$items[avoidance]\r\n\r\n\r\n x7 x11 x15 x23 x27 x29 \r\n  7  11  15  23  27  29 \r\n\r\n# Anxiety items\r\necr_GA$measure$items[anxiety]\r\n\r\n\r\n x2 x10 x14 x16 x18 x30 \r\n  2  10  14  16  18  30 \r\n\r\nBased on the selected items, we can see the estimated reliability coefficients (i.e., alpha) for each subscale. For both subscales, reliability levels seem to be acceptable but much lower than those we estimated for the original ECR scale earlier.\r\n\r\n\r\necr_GA$measure$alpha\r\n\r\n\r\n         A1     A2\r\nalpha 0.832 0.7908\r\n\r\nLastly, we will see a visual summary of the search process using the plot function:\r\n\r\n\r\nplot(ecr_GA)\r\n\r\n\r\n\r\n\r\nThe diagnostic plots on the left-hand side show how total cost, the length of the ECR scale, and the mean \\(R^2\\) value have changed during the search process. The plot in the middle shows the amount of variance explained (i.e., \\(R^2\\)) for the best solution—nearly \\(R^2=.90\\) for each subscale. The plot on the right-hand side shows which items have been selected during the search process. Those at the bottom of the plot (e.g., 2, 7, 10) indicate the set of items included in the optimal solution.\r\nAnt Colony Optimization\r\nWe will use the antcolony.lavaan function from the ShortForm package (Raborn & Leite, 2020) to implement ACO for scale abbreviation.\r\n\r\n\r\nlibrary(\"ShortForm\")\r\n\r\n\r\n\r\nStep 1: We will save the response data set as a data.frame because the antcolony.lavaan function requires the data to be in a data frame format. This step is necessary because, for the GAabbreviate function, we have transformed our data set into a matrix. So, we will need to change the format back to a data frame and rename the columns as before.\r\n\r\n\r\necr <- as.data.frame(ecr)\r\nnames(ecr) <- paste0(\"Q\", 1:36)\r\n\r\n\r\n\r\nStep 2: We will define the factorial structure underlying the ECR scale. That is, there are two dimensions (Avoidance and Anxiety) and each dimension is defined by 18 items. To define the factorial model, we need to use the language of the lavaan package5.\r\n\r\n\r\nmodel <- 'Avoidance =~ Q1+Q3+Q5+Q7+Q9+Q11+Q13+Q15+Q17+Q19+Q21+Q23+Q25+Q27+Q29+Q31+Q33+Q35\r\n          Anxiety =~ Q2+Q4+Q6+Q8+Q10+Q12+Q14+Q16+Q18+Q20+Q22+Q24+Q26+Q28+Q30+Q32+Q34+Q36'\r\n\r\n\r\n\r\nStep 3: Next, we will define which items can be used for each dimension. Although we have already defined the model above, this step is still necessary to tell the function which items are candidate items for each dimension during the item selection process. So, we will create a list of item names for each factor.\r\n\r\n\r\nitems <- list(c(paste0(\"Q\", seq(1, 35, by = 2))),\r\n              c(paste0(\"Q\", seq(2, 36, by = 2))))\r\n\r\n\r\n\r\nStep 4: We will put everything together to implement the ACO algorithm. When preparing the antcolony.lavaan function, we will use the default values. However, some parameters, such as ants, evaporation, and steps, could be modified to find an optimal result (or reduce the computation time). The only parameter we will change is the estimator for the model. By default, lavaan uses maximum likelihood but we should select the weighted least square mean and variance adjusted (WLSMV) estimator because it is more robust to non-normality in the data (which is quite likely given the Likert scale established for the ECR scale). By default, the antcolony.lavaan function uses Hu & Bentler (1999)’s guidelines for fit indices to evaluate model fit: (1) Comparative fit index (CFI) > .95; Tucker-Lewis index (TLI) > .95; and root mean square error of approximation (RMSEA) < .06.\r\n\r\n\r\necr_ACO <- antcolony.lavaan(data = ecr, # Response data set\r\n                            ants = 20, # Number of ants\r\n                            evaporation = 0.9, #  % of the pheromone retained after evaporation\r\n                            antModel = model, # Factor model for ECR\r\n                            list.items = items, # Items for each dimension\r\n                            full = 36, # The total number of unique items in the ECR scale\r\n                            i.per.f = c(6, 6), # The desired number of items per dimension\r\n                            factors = c('Avoidance','Anxiety'), # Names of dimensions\r\n                            # lavaan settings - Change estimator to WLSMV\r\n                            lavaan.model.specs = list(model.type = \"cfa\", auto.var = T, estimator = \"WLSMV\",\r\n                                                      ordered = NULL, int.ov.free = TRUE, int.lv.free = FALSE, \r\n                                                      auto.fix.first = TRUE, auto.fix.single = TRUE, \r\n                                                      auto.cov.lv.x = TRUE, auto.th = TRUE, auto.delta = TRUE,\r\n                                                      auto.cov.y = TRUE, std.lv = F),\r\n                            steps = 50, # The number of ants in a row for which the model does not change\r\n                            fit.indices = c('cfi', 'tli', 'rmsea'), # Fit statistics to use\r\n                            fit.statistics.test = \"(cfi > 0.95)&(tli > 0.95)&(rmsea < 0.06)\",\r\n                            max.run = 1000) # The maximum number of ants to run before the algorithm stops\r\n\r\n\r\n\r\nStep 5: In the final step, we will review the results. First, we will see the model fit indices and which items have been selected by ACO.\r\n\r\n\r\necr_ACO[[1]]\r\n\r\n\r\n        cfi    tli   rmsea mean_gamma Q1 Q3 Q5 Q7 Q9 Q11 Q13 Q15 Q17\r\n[1,] 0.9814 0.9768 0.05723      0.755  0  1  1  1  0   0   1   0   1\r\n     Q19 Q21 Q23 Q25 Q27 Q29 Q31 Q33 Q35 Q2 Q4 Q6 Q8 Q10 Q12 Q14 Q16\r\n[1,]   0   0   1   0   0   0   0   0   0  1  1  1  1   1   0   0   0\r\n     Q18 Q20 Q22 Q24 Q26 Q28 Q30 Q32 Q34 Q36\r\n[1,]   0   0   1   0   0   0   0   0   0   0\r\n\r\n# Uncomment to print the entire model output\r\n# print(ecr_AC) \r\n\r\n\r\n\r\nNext, we will check which items have been selected for each dimension. The following will return the lavaan syntax that can be used to estimate the same two-factor model with 12 items:\r\n\r\n\r\ncat(ecr_ACO$best.syntax)\r\n\r\n\r\nAvoidance =~ Q7 + Q17 + Q13 + Q9 + Q5 + Q3\r\nAnxiety =~ Q2 + Q10 + Q6 + Q22 + Q8 + Q4\r\n\r\nWe can also visualize the results returned from antcolony.lavaan. For example, we can see changes in the amount of variance explained in the model across each iteration of the algorithm.\r\n\r\n\r\nplot(ecr_ACO, type = \"variance\")\r\n\r\n\r\n$Pheromone\r\nNULL\r\n\r\n$Gamma\r\nNULL\r\n\r\n$Beta\r\nNULL\r\n\r\n$Variance\r\n\r\n# Other alternative plots\r\n# plot(ecr_ACO, type = \"gamma\")\r\n# plot(ecr_ACO, type = \"pheromone\")\r\n\r\n\r\n\r\nLastly, we will check the reliability levels of the shortened subscales based on the items recommended by ACO.\r\n\r\n\r\n# Avoidance subscale\r\npsych::alpha(ecr[, c(\"Q7\", \"Q17\", \"Q13\", \"Q9\", \"Q5\", \"Q3\")])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N      ase  mean    sd\r\n    0.8995    0.8996  0.8852    0.5989 8.958 0.001497 2.572 1.016\r\n median_r\r\n   0.5934\r\n\r\n# Anxiety subscale\r\npsych::alpha(ecr[, c(\"Q2\", \"Q10\", \"Q6\", \"Q22\", \"Q8\", \"Q4\")])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N      ase  mean     sd\r\n    0.8724    0.8726  0.8642     0.533 6.847 0.001917 3.409 0.9806\r\n median_r\r\n   0.5303\r\n\r\nFor both subscales of the ECR scale, the reliability levels are quite high based on the items recommended by ACO. These reliability estimates are closer to those we have obtained from the original ECR scale.\r\nConclusion\r\nIn our example, both GA and ACO could recommend a shorter version of the ECR scale with 6 items per subscale. GA is relatively easier to use because we only needed to feed the scale score information and the target number of items per subscale into the GAabbreviate function but we did not have to specify which items belong to each subscale. The GAabbreviate function runs and finds the solution quickly—though the performance of the function would depend on the number of items in the original instrument (i.e., the one that we want to shorten) and the quality of response data. Also, finding the ideal value for item cost (i.e., itemCost) by trial and error might take longer in some cases. In terms of reliability, the short versions of the Avoidance and Anxiety subscales yielded acceptable levels of reliability, although the reliability coefficients seem to be much lower than those we have estimated for the original ECR subscales. Unlike GA, ACO requires multiple user inputs, such as the factor model underlying the data, a list of the items to be used for each dimension in the factor model, and additional lavaan settings. However, specifying these parameters correctly enables better control of the search process and thereby yielding more accurate results for the shortened subscales. The antcolony.lavaan function finds the optimal solution quickly (a bit slower than GAabbreviate). The speed of the search process depends on the number of ants, evaporation rate, model fit indices, and the cut-off values established for these indices. For example, the cut-off values can be modified (e.g., CFI > .90, TLI > .90) to speed up the search process. Finally, the shortened subscales returned from antcolony.lavaan yielded high reliability values (and these values are higher than those we have obtained from GAabbreviate).\r\n\r\n\r\n\r\nBrennan, K. A., Clark, C. L., & Shaver, P. (1998). Self-report measures of adult romantic attachment. Attachment Theory and Close Relationships, 46–76.\r\n\r\n\r\nCrone, D. L., Rhee, J. J., & Laham, S. M. (2020). Developing brief versions of the moral foundations vignettes using a genetic algorithm-based approach. Behavior Research Methods, 1–9.\r\n\r\n\r\nDorigo, M., & Gambardella, L. M. (1997). Ant colony system: A cooperative learning approach to the traveling salesman problem. IEEE Transactions on Evolutionary Computation, 1(1), 53–66.\r\n\r\n\r\nDorigo, M., Maniezzo, V., & Colorni, A. (1996). Ant system: Optimization by a colony of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 26(1), 29–41.\r\n\r\n\r\nEisenbarth, H., Lilienfeld, S. O., & Yarkoni, T. (2015). Using a genetic algorithm to abbreviate the psychopathic personality inventory–revised (PPI-r). Psychological Assessment, 27(1), 194.\r\n\r\n\r\nFink, A. (2015). How to conduct surveys: A step-by-step guide. Sage Publications.\r\n\r\n\r\nGoss, S., Aron, S., Deneubourg, J.-L., & Pasteels, J. M. (1989). Self-organized shortcuts in the argentine ant. Naturwissenschaften, 76(12), 579–581.\r\n\r\n\r\nHu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal, 6(1), 1–55.\r\n\r\n\r\nLeite, W. L., Huang, I.-C., & Marcoulides, G. A. (2008). Item selection for the development of short forms of scales using an ant colony optimization algorithm. Multivariate Behavioral Research, 43(3), 411–431.\r\n\r\n\r\nMarcoulides, G. A., & Drezner, Z. (2003). Model specification searches using ant colony optimization algorithms. Structural Equation Modeling, 10(1), 154–164.\r\n\r\n\r\nMarcoulides, K. M., & Falk, C. F. (2018). Model specification searches in structural equation modeling with r. Structural Equation Modeling: A Multidisciplinary Journal, 25(3), 484–491. https://doi.org/10.1080/10705511.2017.1409074\r\n\r\n\r\nMitchell, M. (1998). An introduction to genetic algorithms. MIT press.\r\n\r\n\r\nRaborn, A., & Leite, W. (2020). ShortForm: Automatic short form creation. https://CRAN.R-project.org/package=ShortForm\r\n\r\n\r\nSahdra, B. K., Ciarrochi, J., Parker, P., & Scrucca, L. (2016). Using genetic algorithms in a large nationally representative american sample to abbreviate the multidimensional experiential avoidance questionnaire. Frontiers in Psychology, 7, 189. https://doi.org/10.3389/fpsyg.2016.00189\r\n\r\n\r\nScrucca, L., & Sahdra, B. K. (2016). GAabbreviate: Abbreviating items measures using genetic algorithms. https://CRAN.R-project.org/package=GAabbreviate\r\n\r\n\r\nWei, M., Russell, D. W., Mallinckrodt, B., & Vogel, D. L. (2007). The experiences in close relationship scale (ECR)-short form: Reliability, validity, and factor structure. Journal of Personality Assessment, 88(2), 187–204.\r\n\r\n\r\nYarkoni, T. (2010). The abbreviation of personality, or how to measure 200 personality scales with 200 items. Journal of Research in Personality, 44(2), 180–198.\r\n\r\n\r\nSee Yarkoni’s blog post on the use of GA at https://www.talyarkoni.org/blog/2010/03/31/abbreviating-personality-measures-in-r-a-tutorial/↩︎\r\nSee http://labs.psychology.illinois.edu/~rcfraley/measures/measures.html for more information.↩︎\r\nSee the full list of questions at http://labs.psychology.illinois.edu/~rcfraley/measures/brennan.html.↩︎\r\nInstead of raw scale scores, we could also obtain factor scores or IRT-based scores.↩︎\r\nSee https://lavaan.ugent.be/ for more information about lavaan.↩︎\r\n",
    "preview": "posts/2021-01-19-how-to-shorten-a-measurement-instrument-automatically-part-ii/pexels-poranimm.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-how-to-shorten-a-measurement-instrument-automatically-part-i/",
    "title": "How to Shorten a Measurement Instrument Automatically (Part I)",
    "description": "Reducing the number of items in an existing measurement instrument (e.g., tests, surveys,\nquestionnaires) is almost as tedious as creating a new instrument. Going through all of \nthe items one by one and choosing the appropriate ones based on personal judgment could be highly \nlaborious and inefficient. In this two-part series, I will demonstrate how to shorten a\nmeasurement instrument automatically in R. The first part focuses on the use of automated test\nassembly and recursive feature elimination for the automatic selection of items.\n\n(9 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2021-01-04",
    "categories": [
      "psychometrics",
      "machine learning",
      "test development"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nAutomatic Selection of Items\r\nAutomated Test Assembly\r\nRecursive Feature Elimination\r\n\r\nConclusion\r\n\r\nPhoto by Maxime Agnelli on UnsplashIntroduction\r\nWith the COVID-19 pandemic, many educators around the world have begun to use online assessments because students are not able to physically attend classes in order to avoid the spread of the virus. When assessing student learning with online assessments, educators are recommended to avoid long, heavily-weighted exams and instead administer shorter exams more frequently throughout the semester (Kuhfeld et al., 2020). Although this sounds like a good idea in theory, it is easier said than done in practice.\r\nTo build shorter exams, educators first need to determine which items should be removed from their existing exams. In addition, they need to ensure that the reliability and other psychometric qualities (e.g., content distribution) of the shortened exams are acceptable. However, making such adjustments manually could be tedious and time-consuming. In this two-part series, I want to demonstrate how to shorten exams (or, any measurement instrument) by automatically selecting the most appropriate items.\r\nIn Part I, I will show how to utilize automated test assembly and recursive feature elimination as alternative methods to automatically build shorter versions of educational assessments (e.g., multiple-choice exams, tests, and quizzes).\r\nIn Part II, I will demonstrate how to use more advanced algorithms, such as the ant colony optimization (ACO; 🐜) and genetic algorithm (GA; 🧬), for creating short forms of other types of instruments (e.g., psychological scales and surveys).\r\nLet’s get started 💪.\r\nExample\r\nIn this example, we will create a hypothetical exam with 80 dichotomously-scored items (i.e., 0 = Incorrect, 1 = Correct). We will assume that the items on the exam are associated with four content domains labeled as ‘A,’ ‘B,’ ‘C,’ and ‘D’ (with 20 items per content domain). We will also assume that a sample of 500 students responded to the items thus far. Because this is a hypothetical exam with no real data, we will need to simulate item responses. The item difficulty distribution will be \\(b \\sim N(0, 0.7)\\) and the ability distribution will be \\(\\theta \\sim N(0, 1)\\). We will simulate student responses based on the Rasch model with the xxIRT package (Luo, 2019)1.\r\n\r\n\r\nlibrary(\"xxIRT\")\r\n\r\n# Set the seed so the simulation can be reproduced\r\nset.seed(2021)\r\n\r\n# Generate item parameters, abilities, and responses\r\ndata <- model_3pl_gendata(\r\n  n_p = 500, # number of students\r\n  n_i = 80, # number of items\r\n  t_dist = c(0, 1), # theta distribution as N(0, 1)\r\n  a = 1, # fix discrimination to 1\r\n  b_dist = c(0, 0.7), # difficulty distribution\r\n  c = 0 # fix guessing to zero (i.e., no guessing)\r\n)\r\n\r\n# Save the item parameters as a separate data set\r\nitems <- with(data, data.frame(id=paste0(\"item\", 1:80), a=a, b=b, c=c))\r\n\r\n# Randomly assign four content domains (A, B, C, or D) to the items\r\nitems$content <- sample(LETTERS[1:4], 80, replace=TRUE)\r\n\r\n\r\n\r\nLet’s see the item bank that we have created.\r\n\r\n\r\n\r\n\r\nBased on the generated items, we can check out the test information function (TIF) for the entire item bank (i.e., 80 items).\r\n\r\n\r\n# Test information function\r\nwith(data, model_3pl_plot(a, b, c, type=\"info\", total = TRUE))\r\n\r\n\r\n\r\n\r\nFinally, we will save the ability parameters and responses as separate data sets.\r\n\r\n\r\n# Ability\r\ntheta <- with(data, data.frame(theta = t))\r\n\r\n# Responses\r\nresp <- with(data, as.data.frame(u))\r\nnames(resp) <- paste0(\"item\", 1:80)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAutomatic Selection of Items\r\nGoal: Assume that using the item bank (i.e., full test with 80 items) generated above, we want to create a short form with only 20 items.\r\nAssumptions: We will assume that: (1) all of the items on the test measure the same latent trait (i.e., the test is unidimensional); (2) the short form will be used for the same student population; and (3) the test does not exhibit any psychometric issues (e.g., mode effect, context effect, or differential item functioning).\r\nConditions: In the short form, we want to maintain the same content distribution (i.e., an equal number of items from each content domain). Therefore, we will select five items from each content domain (i.e., A, B, C, and D). Furthermore, we want the short form to resemble the full test in terms of reliability and (raw) test scores.\r\nMethodology: With the traditional test assembly approach, we would go through all of the items one by one and pick the appropriate ones based on our judgment. However, this would be highly laborious and inefficient in practice. Therefore, we will use two approaches to automatically select the items:\r\nAutomated test assembly as a psychometric approach (IRT parameters are required)2\r\nRecursive feature elimination as a data-driven approach (raw responses are required)\r\nAutomated Test Assembly\r\nAutomated test assembly (or shortly, ATA) is a mathematical optimization approach that allows us to automatically select items from a large item bank (or, item pool) based on pre-defined psychometric, content, and test administration features3. To solve an ATA task, we can use either a mixed integer programming (MIP) algorithm or a heuristic algorithm. In this example, we will use MIP to look for the optimal test form that meets the psychometric and content requirements (i.e., constraints) that we have identified.\r\nTo utilize MIP, we need a solver that will look for the optimal solution based on an objective function (e.g., maximizing test information function) and a set of constraints. In this example, we will use ata_obj_relative to maximize the test information between \\(\\theta=-0.5\\) and \\(\\theta=0.5\\) to mimic the TIF distribution from the full test (i.e, 80 items). In addition, we will use ata_constraint to select five items from each content domain (i.e., A, B, C, and D). For the MIP solver, we will select the open-source solver lp_solve, which is already included in the xxIRT package.\r\n\r\n\r\n# Define the ATA problem (one form with 20 items)\r\nx <- ata(pool = items, num_form = 1, len = 20, max_use = 1)\r\n\r\n# Identify the objective function (maximizing TIF)\r\nx <- ata_obj_relative(x, seq(-0.5, 0.5, .5), 'max')\r\n\r\n# Set the content constraints (5 items from each content domain)\r\nx <- ata_constraint(x, 'content', min = 5, max = 5, level = \"A\")\r\nx <- ata_constraint(x, 'content', min = 5, max = 5, level = \"B\")\r\nx <- ata_constraint(x, 'content', min = 5, max = 5, level = \"C\")\r\nx <- ata_constraint(x, 'content', min = 5, max = 5, level = \"D\")\r\n\r\n# Solve the optimization problem\r\nx <- ata_solve(x, 'lpsolve')\r\n\r\n\r\noptimal solution found, optimum: 9.931 (11.616, 1.685)\r\n\r\nOnce the test assembly is complete, we can see which items have been selected by ATA.\r\n\r\n\r\n# Selected items\r\nprint(x$items)\r\n\r\n\r\n[[1]]\r\n       id a         b c content form\r\n3   item3 1 -0.052065 0       C    1\r\n6   item6 1  0.335703 0       D    1\r\n8   item8 1 -0.053997 0       B    1\r\n9   item9 1  0.183544 0       C    1\r\n14 item14 1  0.190379 0       B    1\r\n18 item18 1 -0.439206 0       D    1\r\n19 item19 1 -0.460572 0       B    1\r\n23 item23 1  0.408673 0       B    1\r\n33 item33 1  0.024181 0       C    1\r\n34 item34 1  0.351346 0       A    1\r\n35 item35 1 -0.279468 0       B    1\r\n36 item36 1  0.686681 0       D    1\r\n43 item43 1 -0.371619 0       A    1\r\n46 item46 1 -0.004551 0       D    1\r\n49 item49 1  0.561348 0       D    1\r\n53 item53 1 -0.437516 0       A    1\r\n59 item59 1 -0.416622 0       A    1\r\n68 item68 1 -0.416855 0       A    1\r\n76 item76 1  0.192465 0       C    1\r\n78 item78 1  0.072045 0       C    1\r\n\r\nIn addition, we can draw a plot to see whether the TIF distribution for the short form is similar to the one from the full test.\r\n\r\n\r\n# TIF for the selected items\r\nwith(x$items[[1]], \r\n     model_3pl_plot(a, b, c, type=\"info\", total = TRUE))\r\n\r\n\r\n\r\n\r\nNow let’s check the reliability of our new short form as well as the correlation among raw scores (i.e., scores from the whole test vs. scores from the shortened test). We will use the alpha function from the psych package (Revelle, 2019) to compute coefficient alpha for the short form.\r\n\r\n\r\nitems_ATA <- x$items[[1]]$id\r\n\r\n# Coefficient alpha\r\npsych::alpha(resp[,c(items_ATA)])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N     ase   mean     sd\r\n    0.9133    0.9133  0.9132    0.3449 10.53 0.00562 0.4974 0.3017\r\n median_r\r\n   0.3481\r\n\r\n# Correlation between raw scores\r\nscore <- rowSums(resp)\r\nscore_ATA <- rowSums(resp[,c(items_ATA)])\r\ncor(score, score_ATA)  \r\n\r\n\r\n[1] 0.969\r\n\r\nThe reliability of the short form is quite high. Also, there is a very high correlation between the raw scores. Lastly, we will check the scatterplot of the raw scores from the full test (out of 80 points) and the short form (out of 20 points). We will use the ggplot2 package (Wickham, 2016) to create the scatterplot.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(\"ggplot2\")\r\n\r\n# Combine the scores\r\nscore1 <- as.data.frame(cbind(score_ATA, score))\r\n\r\n# Draw a scatterplot\r\nggplot(score1, aes(x = score_ATA, y = score)) + \r\n  geom_point() +\r\n  geom_smooth() +\r\n  labs(x = \"Short Form (ATA)\", y = \"Full Test\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nRecursive Feature Elimination\r\nThe term feature selection may sound unfamiliar to those of us who have studied traditional psychometrics but it is one of the core concepts for data scientists dealing with data mining, predictive modeling, and machine learning.\r\n\r\nFeature selection is a process of automatic selection of a subset of relevant features or variables from a set of all features, used in the process of model building. (Dataaspirant)\r\n\r\nThe primary goal of feature selection is to select the most important predictors by filtering out irrelevant or partially relevant predictors. By selecting the most important predictors, we can build a simple and yet accurate model and avoid problems such as overfitting. There are a number of methods for both supervised and unsupervised feature selection. Feature Engineering and Selection: A Practical Approach for Predictive Models by Kuhn & Johnson (2019) is a great resource for those who want to learn more about feature selection4.\r\nIn this example, we will use recursive feature elimination (or shortly, RFE) for automatically selecting items. As a greedy wrapper method, RFE applies backward selection to find the optimal combination of features (i.e., predictors). First, it builds a model based on the entire set of predictors. Then, it removes predictors with the least importance iteratively until a smaller subset of predictors is retained in the model5. In our example, we will use the full set of items (i.e., 80 items) to predict raw scores. RFE will help us eliminate the items that may not be important for the prediction of raw scores6.\r\nTo implement RFE for automatic item selection, we will use the randomForest (Liaw & Wiener, 2002) and caret (Kuhn, 2020) packages in R.\r\n\r\n\r\nlibrary(\"caret\")\r\nlibrary(\"randomForest\")\r\n\r\n\r\n\r\nThe rfe function from caret requires four parameters:\r\nx: A matrix or data frame of features (i.e., predictors)\r\ny: The outcome variable to be predicted\r\nsizes: The number of features that should be retained in the feature selection process\r\nrfeControl: A list of control options for the feature selection algorithms\r\nBefore moving to rfe, we first need to set up the options for rfeControl. The caret package includes a number of functions, such as random forest, naive Bayes, bagged trees, and linear regression7. In this example, we will use the random forest algorithm for model estimation because random forest includes an effective mechanism for measuring feature importance (Kuhn & Johnson, 2019): functions = rfFuncs. In addition, we will add 10-fold cross-validation by using method = “cv” and number = 10.\r\n\r\n\r\n# Define the control using a random forest selection function\r\ncontrol <- rfeControl(functions = rfFuncs, # random forest\r\n                      method = \"cv\", # cross-validation\r\n                      number = 10) # the number of folds\r\n\r\n\r\n\r\nNext, we will run the rfe function with the options we have selected above. For this step, we could simply include all of the items together in a single model and select the best 20 items. However, this process may not yield five items from each content domain, which is the content constraint we have specified for our short form. Therefore, we will apply the rfe function to each content domain separately, use sizes = 5 to retain the top five items (i.e., most important items) for each content domain, and then combine the results at the end.\r\n\r\n\r\n# Run RFE for each content domain within a loop\r\nresult <- list() \r\n\r\nfor(i in c(\"A\", \"B\", \"C\", \"D\")) {\r\n  result[[i]] <- rfe(x = resp[,items[items$content==i,\"id\"]], \r\n                     y = score, \r\n                     sizes = 5, \r\n                     rfeControl = control)\r\n}\r\n\r\n# Extract the results (i.e., first 5 items for each content domain)\r\nitems_RFE <- unlist(lapply(result, function(x) predictors(x)[1:5]))\r\n\r\n\r\n\r\nIn the following section, we will view the items recommended by RFE and then draw a TIF plot for these items.\r\n\r\n\r\n# Selected items\r\nprint(items[items$id %in% items_RFE, ])\r\n\r\n\r\n       id a         b c content\r\n4   item4 1 -0.795227 0       D\r\n6   item6 1  0.335703 0       D\r\n9   item9 1  0.183544 0       C\r\n17 item17 1 -0.641461 0       C\r\n18 item18 1 -0.439206 0       D\r\n19 item19 1 -0.460572 0       B\r\n31 item31 1 -0.106790 0       A\r\n34 item34 1  0.351346 0       A\r\n35 item35 1 -0.279468 0       B\r\n41 item41 1 -0.249472 0       B\r\n42 item42 1 -0.558894 0       C\r\n43 item43 1 -0.371619 0       A\r\n46 item46 1 -0.004551 0       D\r\n53 item53 1 -0.437516 0       A\r\n56 item56 1 -0.499666 0       D\r\n57 item57 1  0.041591 0       C\r\n58 item58 1 -0.551148 0       B\r\n60 item60 1 -0.083425 0       A\r\n72 item72 1  0.529437 0       B\r\n76 item76 1  0.192465 0       C\r\n\r\n# TIF for the selected items\r\nwith(items[items$id %in% items_RFE, ], \r\n     model_3pl_plot(a, b, c, type=\"info\", total = TRUE))\r\n\r\n\r\n\r\n\r\nAlso, we will check the reliability of the short form and correlations between raw scores for the RFE method.\r\n\r\n\r\n# Coefficient alpha\r\npsych::alpha(resp[,c(items_RFE)])$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r   S/N      ase   mean     sd\r\n    0.9193    0.9193  0.9188    0.3628 11.39 0.005233 0.5495 0.3068\r\n median_r\r\n   0.3659\r\n\r\n# Correlation between raw scores\r\nscore_RFE <- rowSums(resp[,c(items_RFE)])\r\ncor(score, score_RFE)  \r\n\r\n\r\n[1] 0.9681\r\n\r\nAs for the ATA method, the correlation between the raw scores is very high for the RFE method. We will also check out the scatterplot of the raw scores.\r\n\r\n\r\n\r\n\r\n\r\n# Combine the scores\r\nscore2 <- as.data.frame(cbind(score_RFE, score))\r\n\r\n# Draw a scatterplot\r\nggplot(score2, aes(x = score_RFE, y = score)) + \r\n  geom_point() +\r\n  geom_smooth() +\r\n  labs(x = \"Short Form (RFE)\", y = \"Full Test\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nLastly, we can compute percent-correct scores for the full test and the short forms generated with ATA and RFE to make an overall score comparison. We will use the ggplot2 package to draw a violin plot and check out the score distributions.\r\n\r\n\r\n# Compute percent-correct scores\r\nscore_all <- data.frame(\r\n  test = as.factor(c(rep(\"Full Test\", 500), rep(\"ATA\", 500), rep(\"RFE\", 500))),\r\n  score = c(score/80*100, score_ATA/20*100, score_RFE/20*100)\r\n)\r\n\r\n# Make \"full test\" the first category of the test variable (optional)\r\nscore_all$test <- relevel(score_all$test, \"Full Test\")\r\n\r\n# Draw a violin plot (with a boxplot inside)\r\nggplot(data = score_all, aes(x = test, y = score)) +\r\n  geom_violin(aes(fill = test), trim = FALSE) + \r\n  geom_boxplot(width = 0.2)+\r\n  scale_fill_manual(values = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\r\n  labs(x = \"\", y = \"Percent-Correct Scores\") +\r\n  theme_bw() + theme(legend.position=\"none\")\r\n\r\n\r\n\r\n\r\nThe violin plot shows that the score distributions are not exactly the same but they are pretty similar.\r\nConclusion\r\nBoth methods (i.e., ATA and RFE) appear to yield very similar (and very good!) results in terms of reliability and correlations between raw scores, although they recommended different sets of items for the short form. This may not be a surprising finding because we generated a nice and clean data set (e.g., no missing responses, no outliers, etc.) based on a psychometric model (i.e., Rasch). Automatic item selection with ATA and RFE may produce less desirable results if:\r\nthe sample size is smaller,\r\nnon-random missingness is present in the response data set, and\r\nthere are problematic items (e.g., items with low discrimination or high guessing).\r\n\r\n\r\n\r\nKuhfeld, M., Soland, J., Tarasawa, B., Johnson, A., Ruzek, E., & Liu, J. (2020). Projecting the potential impacts of COVID-19 school closures on academic achievement. EdWorkingPaper.\r\n\r\n\r\nKuhn, M. (2020). caret: Classification and regression training. https://CRAN.R-project.org/package=caret\r\n\r\n\r\nKuhn, M., & Johnson, K. (2019). Feature engineering and selection: A practical approach for predictive models. CRC Press.\r\n\r\n\r\nLiaw, A., & Wiener, M. (2002). Classification and regression by randomForest. R News, 2(3), 18–22. https://CRAN.R-project.org/doc/Rnews/\r\n\r\n\r\nLuo, X. (2019). xxIRT: Item response theory and computer-based testing in r. https://CRAN.R-project.org/package=xxIRT\r\n\r\n\r\nRevelle, W. (2019). psych: Procedures for psychological, psychometric, and personality research. Northwestern University. https://CRAN.R-project.org/package=psych\r\n\r\n\r\nVan der Linden, W. J. (2006). Linear models for optimal test design. Springer Science & Business Media.\r\n\r\n\r\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\r\n\r\n\r\nThe model_3pl_estimate_mmle function in the same package can be used to estimate item parameters based on raw response data.↩︎\r\nItem response theory (IRT) is a psychometric framework for the design, analysis, and scoring of tests, questionnaires, and similar instruments. If you are not familiar with IRT, you can move to the second approach: Recursive feature elimination.↩︎\r\nYou can check out Van der Linden (2006)’s “Linear Models for Optimal Test Design” for further information on ATA.↩︎\r\nMax Kuhn is also the author of the famous caret package.↩︎\r\nSee http://www.feat.engineering/recursive-feature-elimination.html for more information on RFE↩︎\r\nThis could also be used for predicting person ability estimates in IRT or another external criterion.↩︎\r\nSee https://topepo.github.io/caret/recursive-feature-elimination.html for further information.↩︎\r\n",
    "preview": "posts/2021-01-04-how-to-shorten-a-measurement-instrument-automatically-part-i/photo_unsplash2.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-21-testing-for-measurement-invariance-in-r/",
    "title": "Testing for Measurement Invariance in R",
    "description": "Researchers conduct measurement invariance analysis to ensure that the interpretations of \nlatent construct(s) being measured with their measurement instruments (e.g., scales, surveys, \nand questionnaires) are valid across subgroups of a target population or multiple time points. \nIn this post, I demonstrate how to test for measurement invariance (i.e., configural, metric, scalar,\nand strict invariance) of an instrument using R.\n\n(11 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2020-12-21",
    "categories": [
      "psychometrics",
      "factor analysis",
      "measurement invariance"
    ],
    "contents": "\r\n\r\nContents\r\nMeasurement Invariance\r\nExample\r\nConfigural Invariance\r\nMetric Invariance\r\nScalar Invariance\r\nStrict Invariance\r\n\r\nConcluding Remarks\r\n\r\nMeasurement Invariance\r\nIn the social sciences, researchers often use self-reported measurement instruments (e.g., scales, surveys, and questionnaires) to assess different latent constructs (e.g., emotions, attitudes, and preferences). Data collected through such instruments are typically used for comparing groups at a particular time point or comparing the same individuals across multiple time points. To be able to make valid group comparisons, researchers must ensure that the instrument measures the target latent construct(s) with the same factorial structure across groups. If the measurement instrument can sufficiently maintain its factorial structure across groups, it is called measurement invariant (MI). The lack of measurement invariance (i.e., measurement non-variance) indicates that the latent constructs cannot be measured and interpreted in the same way across groups.\r\nFor example, a researcher could design a new questionnaire to measure attitudes toward spirituality and administer the instrument to a sample of respondents in the target population. Depending on the quality of the items in the questionnaire, individuals from different ethnic and religious groups may perceive and interpret spirituality differently. Thus, individuals from a particular ethnic group might be less (or more) likely to endorse the items on the questionnaire. If this situation significantly affects individuals’ response behaviors, then it is also very likely to influence the factorial structure of the questionnaire. If this is the case, the questionnaire is not measurement invariant and therefore group comparisons based on the results of the questionnaire will not be valid.\r\nThis post has been inspired by Van de Schoot et al. (2012)’s checklist for testing for measurement invariance. The authors provide a nice description of measurement invariance along with Mplus syntax for all the analyses described in their paper. Similarly, in this post I will briefly describe the steps for testing measurement invariance and then demonstrate how to conduct measurement invariance analyses in R. To test for measurement invariance, we need to estimate and compare increasingly constrained confirmatory factor analysis (CFA) models with each other:\r\nConfigural invariance model: A multi-group CFA model fitted for each group separately, without any equality constraints. This model allows us to test whether the same factorial structure holds across all groups.\r\nMetric invariance model: A constrained version of the configural model where the factor loadings are assumed to be equal across groups but the intercepts are allowed to vary between groups.\r\nScalar invariance model: A constrained version of the metric model where both the factor loadings and intercepts are assumed to be equal across groups.\r\nStrict invariance model: A constrained version of the scalar model where the factor loadings, intercepts, and residual variances are fixed across groups.\r\nTo evaluate the configural model for each group, we can use Hu & Bentler (1999)’s guidelines for model fit indices: (1) Comparative fit index (CFI) > .95; Tucker-Lewis index (TLI) > .95; and root mean square error of approximation (RMSEA) < .06. With sufficient model fit for configural invariance, we can proceed to metric invariance.\r\nTo test metric invariance, we need to compare the configural model against the metric model using a chi-square difference (\\(\\Delta\\chi^2\\)) test. If the test is significant, then there is a lack of metric invariance and thus there is no need to test scalar and strict invariance. If, however, the test is not significant, then metric invariance is established and thus we can move to the next step, scalar invariance.\r\nTo assess scalar invariance, we need to follow a similar approach by comparing the scalar model against the metric model. A statistically insignificant result for the \\(\\Delta\\chi^2\\) test would indicate scalar invariance of the factorial model. Van de Schoot et al. (2012) suggest that scalar invariance must hold to be able to interpret latent means and correlations across groups. If scalar invariance is not fully satisfied, then partial MI could be established by adjusting factor loadings and/or intercepts (Steenkamp & Baumgartner, 1998).\r\nFinally, if either partial or full scalar variance holds, then we can test strict invariance by comparing the strict model with the scalar model. Van de Schoot et al. (2012) note that with the lack of strict invariance, groups can still be compared on the latent construct(s). Figure 1 shows a summary of the steps for testing different types of MI based on the four invariance models summarized above.\r\nFigure 1: A summary of measurement invariance modelsExample\r\nTo demonstrate the test of measurement invariance, I will be using the Consumer Financial Protection Bureau (CFPB)’s Financial Well-Being Scale. CFPB defines financial well-being as follows:\r\n\r\nFinancial well-being is a state of being wherein a person can fully meet current and ongoing financial obligations, can feel secure in their financial future, and is able to make choices that allow them to enjoy life.\r\n\r\nTo measure the construct of financial well-being, CFPB created the Financial Well-Being Scale that consists of ten rating scale items. The items cover all four elements of the CFPB’s definition of financial well-being: control over finances, capacity to absorb a financial shock, being on track to meet financial goals, and having the financial freedom to enjoy life, using both positive and negative phrasing. Figure 2 shows a list of the items and their response options.\r\nFigure 2: Items in the Financial Well-Being Scale (Source: Consumer Financial Protection Bureau)CFPB’s technical report on the Financial Well-Being Scale indicates that there are two latent dimensions (i.e., factors) associated with the polarity of the items (i.e., whether the items were phrased negatively or positively). Figure 3 demonstrates the factorial structure of the Financial Well-Being Scale.\r\nFigure 3: Factorial structure of the Financial Well-Being ScaleFor this demonstration, I have already cleaned up the original data set and saved it as finance.csv. You can see the codes for data cleaning and preparation here. Also, the cleaned data set (i.e., finance.csv) is available here.\r\n\r\n\r\nfinance <- read.csv(\"finance.csv\", header = TRUE)\r\n\r\n\r\n\r\nThe following table shows the finance data set. There are 3,811 respondents (1701 female, 2110 male) who responded to the items in the scale.\r\n\r\n\r\n\r\n\r\nBefore we begin the measurement invariance analysis, we need to active the R packages that we will utilize in the example. We will use dplyr (Wickham et al., 2020) for organizing data, corrplot (Wei & Simko, 2017) for visualizing the correlation matrix of the items, lavaan (Rosseel, 2012) to estimate multi-group CFA models, and semTools (Jorgensen et al., 2020) to run model comparison tests.\r\n\r\n\r\nlibrary(\"dplyr\")\r\nlibrary(\"corrplot\")\r\nlibrary(\"lavaan\")\r\nlibrary(\"semTools\")\r\n\r\n\r\n\r\nNow we can go ahead and visualize the correlations among the items using the corrplot function. This is a preliminary analysis to check whether the items in the Financial Well-Being Scale are associated with each other. We already know the factorial structure of the scale but this plot will show how strongly the items are associated with each other and whether there are two groups of items (one for positively-worded items and another for negatively-worded items).\r\n\r\n\r\n# Correlation matrix of items\r\ncormat <- finance %>%\r\n  select(starts_with(\"item\")) %>%\r\n  cor()\r\n\r\n# Correlation matrix plot\r\ncorrplot.mixed(cormat)\r\n\r\n\r\n\r\n\r\nThe plot shows that the items are moderately correlated with each other. Due to the polarity of the items, some of the correlations are negative (red), while the others are positive (blue). We can use hierarchical clustering to group the items together in the same plot. The next plot shows that positively-worded items (items 1, 2, 4, and 8) have been grouped together in the upper rectangle, while the remaining six items (i.e., negatively-worded items) have been grouped in the lower rectangle.\r\n\r\n\r\ncorrplot(cormat, order = \"hclust\", addrect = 2)\r\n\r\n\r\n\r\n\r\nNext, using the lavaan package (see https://lavaan.ugent.be/ for more information on the package), we will estimate a series of multi-group CFA models using gender as a group variable. For all of the models, the baseline model is the same: a two-factor model where the positively-worded items define one dimension and the negatively-worded items define another dimension. Therefore, we define model with two factors (positive and negative) and assume that the two factors are correlated with each other: positive ~~ negative\r\n\r\n\r\n# Two-factor CFA model\r\nmodel <- 'positive =~ item1 + item2 + item4 + item8\r\n          negative =~ item3 + item5 + item6 + item7 + item9 + item10\r\n          positive ~~ negative'\r\n\r\n\r\n\r\nConfigural Invariance\r\nOur first invariance model is the configural invariance model. This is a multi-group CFA model separately estimated for male and female respondents in the finance data set. Since the items follow a 5-point rating scale, we will use the WLSMV estimator – which is more suitable for categorical and ordinal data. We will also add group = “gender” to estimate the same CFA model for female and male respondents separately.\r\n\r\n\r\n# Configural model\r\ncfa.config <- cfa(model, data = finance, estimator = \"WLSMV\", group = \"gender\")\r\n\r\nsummary(cfa.config, fit.measures = TRUE, standardized = TRUE)\r\n\r\n\r\n\r\nThe following (partial) output shows the model fit indices. The results show that the fit indices from the configural model are aligned with Hu & Bentler (1999)’s guidelines for good model fit.\r\n\r\n\r\n\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    0.997       0.959\r\n  Tucker-Lewis Index (TLI)                       0.996       0.945\r\n                                                                  \r\n  Robust Comparative Fit Index (CFI)                            NA\r\n  Robust Tucker-Lewis Index (TLI)                               NA\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.028       0.065\r\n  90 Percent confidence interval - lower         0.023       0.060\r\n  90 Percent confidence interval - upper         0.034       0.070\r\n  P-value RMSEA <= 0.05                          1.000       0.000\r\n                                                                  \r\n  Robust RMSEA                                                  NA\r\n  90 Percent confidence interval - lower                        NA\r\n  90 Percent confidence interval - upper                        NA\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.029       0.029\r\n\r\nMetric Invariance\r\nNext, we will create a metric invariance model. The R codes for this model are nearly identical to those from the configural model. However, we add group.equal = “loadings” to fix the factor loadings to be the same for the female and male models. After estimating the metric model, we will compare it against the configural model using the compareFit function from semTools.\r\n\r\n\r\n# Metric model\r\ncfa.metric <- cfa(model, data = finance, estimator = \"WLSMV\", group = \"gender\", \r\n                  group.equal = \"loadings\")\r\n\r\n# Model comparison\r\ncompareFit(cfa.config, cfa.metric)\r\n\r\n\r\n################### Nested Model Comparison #########################\r\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\r\n\r\nlavaan NOTE:\r\n    The \"Chisq\" column contains standard test statistics, not the\r\n    robust test that should be reported per model. A robust difference\r\n    test is a function of two standard (not robust) statistics.\r\n \r\n           Df AIC BIC Chisq Chisq diff Df diff Pr(>Chisq)\r\ncfa.config 68           171                              \r\ncfa.metric 76           190       9.69       8       0.29\r\n\r\n####################### Model Fit Indices ###########################\r\n           chisq.scaled df.scaled pvalue.scaled cfi.scaled tli.scaled\r\ncfa.config     614.530         68          .000      .959       .945 \r\ncfa.metric     359.295†        76          .000      .978†      .975†\r\n           rmsea.scaled  srmr\r\ncfa.config        .065  .029†\r\ncfa.metric        .044† .031 \r\n\r\n################## Differences in Fit Indices #######################\r\n                        df.scaled cfi.scaled tli.scaled rmsea.scaled\r\ncfa.metric - cfa.config         8       0.02      0.029       -0.021\r\n                         srmr\r\ncfa.metric - cfa.config 0.002\r\n\r\nThe results above show that the chi-square difference test was not statistically significant; \\(\\Delta \\chi^2 = 9.69, df = 8, p = 0.29\\). This finding suggests that after constraining the factor loadings to be equal across groups, the model fit did not change substantially. In other words, the constrained model (i.e., metric model) fits the data equally well. The model fit indices also indicate a good fit for the metric model.\r\nScalar Invariance\r\nSince metric invariance has been established, we can test scalar invariance for the Financial Well-Being Scale. This time, in addition to factor loadings, we also add intercepts to the list of parameters to be constrained equal between female and male participants: group.equal = c(“loadings,”“intercepts”)\r\n\r\n\r\n# Scalar model\r\ncfa.scalar <- cfa(model, data = finance, estimator = \"WLSMV\", group = \"gender\", \r\n                  group.equal = c(\"loadings\",\"intercepts\"))\r\n\r\n# Model comparison\r\ncompareFit(cfa.metric, cfa.scalar)\r\n\r\n\r\n################### Nested Model Comparison #########################\r\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\r\n\r\nlavaan NOTE:\r\n    The \"Chisq\" column contains standard test statistics, not the\r\n    robust test that should be reported per model. A robust difference\r\n    test is a function of two standard (not robust) statistics.\r\n \r\n           Df AIC BIC Chisq Chisq diff Df diff Pr(>Chisq)    \r\ncfa.metric 76           190                                  \r\ncfa.scalar 84           209       41.9       8    1.4e-06 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n####################### Model Fit Indices ###########################\r\n           chisq.scaled df.scaled pvalue.scaled cfi.scaled tli.scaled\r\ncfa.metric     359.295†        76          .000      .978†      .975†\r\ncfa.scalar     402.289         84          .000      .976       .974 \r\n           rmsea.scaled  srmr\r\ncfa.metric        .044† .031†\r\ncfa.scalar        .045  .032 \r\n\r\n################## Differences in Fit Indices #######################\r\n                        df.scaled cfi.scaled tli.scaled rmsea.scaled\r\ncfa.scalar - cfa.metric         8     -0.003          0            0\r\n                         srmr\r\ncfa.scalar - cfa.metric 0.001\r\n\r\nThe chi-square difference test was significant; \\(\\Delta \\chi^2 = 41.9, df = 8, p < .001\\). Despite good model-fit indices presented in the rest of the output, the statistically significant result suggests that there is a lack of scalar invariance for the Financial Well-Being Scale. Thus, we will try to establish partial MI. We will use lavTestScore to view which fixed (or constrained) parameters in the model should be released to improve the fit for the scalar model.\r\n\r\n\r\nlavTestScore(cfa.scalar)\r\n\r\n\r\n\r\n\r\n\r\nunivariate score tests:\r\n\r\n     lhs op   rhs    X2 df p.value\r\n1   .p2. == .p37. 0.272  1   0.602\r\n2   .p3. == .p38. 1.778  1   0.182\r\n3   .p4. == .p39. 0.169  1   0.681\r\n4   .p6. == .p41. 4.049  1   0.044\r\n5   .p7. == .p42. 1.989  1   0.158\r\n6   .p8. == .p43. 3.753  1   0.053\r\n7   .p9. == .p44. 2.274  1   0.132\r\n8  .p10. == .p45. 0.002  1   0.962\r\n9  .p24. == .p59. 5.206  1   0.023\r\n10 .p25. == .p60. 0.253  1   0.615\r\n11 .p26. == .p61. 5.318  1   0.021\r\n12 .p27. == .p62. 0.269  1   0.604\r\n13 .p28. == .p63. 3.364  1   0.067\r\n14 .p29. == .p64. 0.390  1   0.532\r\n15 .p30. == .p65. 0.122  1   0.727\r\n16 .p31. == .p66. 7.561  1   0.006\r\n17 .p32. == .p67. 0.541  1   0.462\r\n18 .p33. == .p68. 1.728  1   0.189\r\n\r\nThe partial output above shows which parameters should be released between the groups in order to establish partial MI. By reviewing the p.value column, we can identify the parameters that are expected to have a significant impact on model fit (i.e., those with \\(p < .05\\)). For example, these are some influential parameters:\r\n.p24. == .p59.\r\n.p26. == .p61.\r\n.p31. == .p66.\r\nWith the list of potential parameters to adjust, we can make changes in the scalar model. However, we do not know what these parameters refer to in the original scalar model. For example, we know that we want to freely estimate .p24. and .p59. but we do not know what these parameters mean for the model. To better understand these parameters, we will use parTable and print out the list of all parameters in the model.\r\n\r\n\r\nparTable(cfa.scalar)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe plabel column in the output shows the labels for all parameters estimated for the scalar model. To interpret the output more easily, we can refer to the following table on the lavaan website (https://lavaan.ugent.be/tutorial/syntax1.html).\r\n\r\n\r\nFormula Type\r\n\r\n\r\nlavaan Operator\r\n\r\n\r\nMeaning\r\n\r\n\r\nLatent variable\r\n\r\n\r\n=~\r\n\r\n\r\nis measured by\r\n\r\n\r\nRegression\r\n\r\n\r\n~\r\n\r\n\r\nis regressed on\r\n\r\n\r\nResidual co(variance)\r\n\r\n\r\n~~\r\n\r\n\r\nis correlated with\r\n\r\n\r\nIntercept\r\n\r\n\r\n~1\r\n\r\n\r\nintercept\r\n\r\n\r\nGoing back to the influential parameters we have identified earlier, we see that .p24. and .p59. are the intercept parameters of item 1 estimated for females and males, respectively. Instead of fixing these intercept parameters, we can estimate them freely for females and males. Similarly, the parameter couples of .p26.-.p61. and .p31.-.p66. also refer to intercept parameters for items 4 and 7, respectively. That is, we can release the constraints for these parameters to establish partial MI.\r\nIdeally, we should make one change at a time and compare the new model against the metric model until partial MI is satisfied. For the sake of brevity, I will make three changes to the scalar model all at once, estimate the adjusted model, and compare it with the metric model. The group.partial command is where we specify which parameters should be released in the model (i.e., freely estimated for females and males). In our model, we want to release the constraints on the intercept parameters for items 1, 4, and 7.\r\n\r\n\r\n# Adjust the model\r\ncfa.scalar2 <- cfa(model, data = finance, estimator = \"WLSMV\", group = \"gender\", \r\n                   group.equal = c(\"loadings\",\"intercepts\"), \r\n                   group.partial = c(\"item1 ~ 1\", \"item4 ~ 1\", \"item7 ~ 1\"))\r\n\r\n# Model comparison\r\ncompareFit(cfa.metric, cfa.scalar2)\r\n\r\n\r\n################### Nested Model Comparison #########################\r\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\r\n\r\nlavaan NOTE:\r\n    The \"Chisq\" column contains standard test statistics, not the\r\n    robust test that should be reported per model. A robust difference\r\n    test is a function of two standard (not robust) statistics.\r\n \r\n            Df AIC BIC Chisq Chisq diff Df diff Pr(>Chisq)\r\ncfa.metric  76           190                              \r\ncfa.scalar2 81           194       7.84       5       0.17\r\n\r\n####################### Model Fit Indices ###########################\r\n            chisq.scaled df.scaled pvalue.scaled cfi.scaled\r\ncfa.metric      359.295†        76          .000      .978†\r\ncfa.scalar2     369.598         81          .000      .978 \r\n            tli.scaled rmsea.scaled  srmr\r\ncfa.metric       .975         .044  .031†\r\ncfa.scalar2      .976†        .043† .031 \r\n\r\n################## Differences in Fit Indices #######################\r\n                         df.scaled cfi.scaled tli.scaled rmsea.scaled\r\ncfa.scalar2 - cfa.metric         5          0      0.001       -0.001\r\n                         srmr\r\ncfa.scalar2 - cfa.metric    0\r\n\r\nThe comparison of the adjusted scalar model and the metric model indicates that the chi-square difference test is not significant anymore; \\(\\Delta \\chi^2 = 7.84, df = 5, p = 0.17\\). Also, the model fit indices for the adjusted scalar model is very good. Therefore, we can conclude that partial MI (more specifically, partial scalar invariance) is now established for the scale.\r\nStrict Invariance\r\nIn the last step, we will check strict invariance. Using the adjusted scalar model from the previous analysis, we will build a new model where residuals will be constrained to be equal for female and male participants. So, we add residuals into the list of equality constraints (i.e., group.equal). The output below shows that the chi-square difference test is not significant; \\(\\Delta \\chi^2 = 15.9, df = 10, p = 0.1\\). Therefore, we can conclude that the Financial Well-Being Scale indicates strict invariance across the female and male groups.\r\n\r\n\r\n# Strict model\r\ncfa.strict <- cfa(model, data = finance, estimator = \"WLSMV\", group = \"gender\", \r\n                  group.equal = c(\"loadings\",\"intercepts\", \"residuals\"),\r\n                  group.partial = c(\"item1 ~ 1\", \"item4 ~ 1\", \"item7 ~ 1\"))\r\n\r\n\r\ncompareFit(cfa.scalar2, cfa.strict)\r\n\r\n\r\n################### Nested Model Comparison #########################\r\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\r\n\r\nlavaan NOTE:\r\n    The \"Chisq\" column contains standard test statistics, not the\r\n    robust test that should be reported per model. A robust difference\r\n    test is a function of two standard (not robust) statistics.\r\n \r\n            Df AIC BIC Chisq Chisq diff Df diff Pr(>Chisq)\r\ncfa.scalar2 81           194                              \r\ncfa.strict  91           204       15.9      10        0.1\r\n\r\n####################### Model Fit Indices ###########################\r\n            chisq.scaled df.scaled pvalue.scaled cfi.scaled\r\ncfa.scalar2     369.598†        81          .000      .978 \r\ncfa.strict      371.913         91          .000      .979†\r\n            tli.scaled rmsea.scaled  srmr\r\ncfa.scalar2      .976         .043  .031†\r\ncfa.strict       .979†        .040† .032 \r\n\r\n################## Differences in Fit Indices #######################\r\n                         df.scaled cfi.scaled tli.scaled rmsea.scaled\r\ncfa.strict - cfa.scalar2        10      0.001      0.003       -0.003\r\n                          srmr\r\ncfa.strict - cfa.scalar2 0.001\r\n\r\nConcluding Remarks\r\nTo test for measurement invariance, sample size should be sufficient for both groups. In this example, I used a large data set with similar numbers of respondents for the gender groups. If sample size is quite small for a particular group, it could lead to higher levels of standard error and convergence issues depending on the complexity of the model (e.g., number of factors).\r\nStrict invariance is often very difficult to establish in practice. The Financial Well-Being Scale used in the above example is a high-quality instrument that was already pilot-tested and refined by the researchers. Therefore, it is not surprising that strict invariance could be established for the scale. However, especially for new instruments that have not been validated yet, establishing scalar invariance should be the main priority.\r\nMeasurement invariance analyses demonstrated here can be summarized using a table with model fit indices, chi-square statistics, and so on. Also, important changes in the models (e.g., those that we made to establish partial MI) should be explained in the text. Van de Schoot et al. (2012) describe how to report the results of measurement invariance analyses.\r\nTesting for measurement invariance can help researchers identify which item(s) might be problematic and thus need to be altered in the future (Lugtig et al., 2012). To identify problematic items within the factor analysis framework, multiple indicators, multiple causes (MIMIC) models can be used (Bulut & Suh, 2017).\r\n\r\n\r\n\r\nBulut, O., & Suh, Y. (2017). Detecting multidimensional differential item functioning with the multiple indicators multiple causes model, the item response theory likelihood ratio test, and logistic regression. Frontiers in Education, 2, 51. https://doi.org/10.3389/feduc.2017.00051\r\n\r\n\r\nHu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal, 6(1), 1–55.\r\n\r\n\r\nJorgensen, T. D., Pornprasertmanit, S., Schoemann, A. M., & Rosseel, Y. (2020). semTools: Useful tools for structural equation modeling. https://CRAN.R-project.org/package=semTools\r\n\r\n\r\nLugtig, P., Boeije, H. R., & Lensvelt-Mulders, G. J. (2012). Change? What change? Methodology, 8, 115–123. https://doi.org/10.1027/1614-2241/a000043\r\n\r\n\r\nRosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36. http://www.jstatsoft.org/v48/i02/\r\n\r\n\r\nSteenkamp, J.-B. E., & Baumgartner, H. (1998). Assessing measurement invariance in cross-national consumer research. Journal of Consumer Research, 25(1), 78–90.\r\n\r\n\r\nVan de Schoot, R., Lugtig, P., & Hox, J. (2012). A checklist for testing measurement invariance. European Journal of Developmental Psychology, 9(4), 486–492.\r\n\r\n\r\nWei, T., & Simko, V. (2017). R package \"corrplot\": Visualization of a correlation matrix. https://github.com/taiyun/corrplot\r\n\r\n\r\nWickham, H., François, R., Henry, L., & Müller, K. (2020). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-21-testing-for-measurement-invariance-in-r/mi2.jpg",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-14-explanatory-irt-models-in-r/",
    "title": "Explanatory IRT Models in R",
    "description": "Explanatory item response modeling (EIRM) is a flexible framework that \nallows researchers to model both person and item characteristics as predictors together. \nIn this post, I demonstrate how to use the `eirm` package to estimate explanatory IRT \nmodels in R. \n\n(4 min read)",
    "author": [
      {
        "name": "Okan Bulut",
        "url": "http://www.okanbulut.com/"
      }
    ],
    "date": "2020-12-14",
    "categories": [
      "psychometrics",
      "IRT",
      "eirm"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nExample\r\nConclusion\r\n\r\nIntroduction\r\nExplanatory item response modeling (shortly EIRM) was first proposed by De Boeck & Wilson (2004) as an alternative to traditional item response model. This framework allows us to turn traditional item response theory (IRT) models into explanatory models with additional predictors. Explanatory IRT models aim to explain how item properties (e.g., content, cognitive complexity, and presence of visual elements) and person characteristics (e.g., gender, race, and English language learner (ELL) status) affect the persons’ responses to items. For a brief introduction to EIRM, you can check out Wilson, De Boeck, and Carstensen’s chapter here.\r\nIn De Boeck & Wilson (2004)’s book, the authors describe how to estimate explanatory IRT models using the SAS statistical software program. Given the growing popularity of R in the psychometric/measurement community, De Boeck et al. (2011) published a nice article that describes how to estimate the same models using the lme4 package (Bates et al., 2015) in R. Since then, I have been using the lme4 package to estimate explanatory IRT models for my research projects. For example, we evaluated measurement invariance of two developmental assets, Support and Positive Identity, across grade levels and ELL subgroups of Latino students in 6th through 12th grade (Bulut et al., 2015). Also, we investigated whether the linguistic complexity of items leads to gender differential item functioning (DIF) in mathematics assessments (Kan & Bulut, 2014).\r\nOne problem that we often had to deal with when using lme4 was the format of item responses. The lme4 package assumes a binomial distribution for item responses, which means responses must be dichotomous (i.e., 0 and 1). Although this is not a problem for dichotomous data based on multiple-choice items, it is a major drawback for polytomous data based on rating scale items (e.g., Likert scales). Therefore, we had to develop a new way to transform polytomous responses into dichotomous responses without losing the original response structure (Stanke & Bulut, 2019). Recently, I decided to share this method in a new package that would faciliate the estimation of explanatory IRT models for both dichotomous and polytomous response data. Therefore, I created the eirm package (Bulut, 2021), which is essentially a wrapper for the lme4 package (Bates et al., 2015). The eirm function in the package allows users to define an explanatory IRT model based on item-level and person-level predictors.\r\nExample\r\nIn this post, I will show how to use the eirm package to estimate explanatory IRT models with polytomous response data. For more examples, you can check out the eirm website: https://okanbulut.github.io/eirm/. The eirm package is currently available on CRAN. It can be downloaded and installed using install.packages(“eirm”).\r\nI will use the verbal aggression dataset in the lme4 package to demonstrate the estimation process. The dataset includes responses to a questionnaire on verbal aggression and additional variables related to items and persons. A preview of the data set is shown below:\r\n\r\n\r\nlibrary(\"eirm\")\r\n\r\ndata(\"VerbAgg\")\r\n\r\nhead(VerbAgg)\r\n\r\n\r\n  Anger Gender        item    resp id btype  situ mode r2\r\n1    20      M S1WantCurse      no  1 curse other want  N\r\n2    11      M S1WantCurse      no  2 curse other want  N\r\n3    17      F S1WantCurse perhaps  3 curse other want  Y\r\n4    21      F S1WantCurse perhaps  4 curse other want  Y\r\n5    17      F S1WantCurse perhaps  5 curse other want  Y\r\n6    21      F S1WantCurse     yes  6 curse other want  Y\r\n\r\nIn the dataset, the variable resp represents each person’s response to the items on the questionnaire. There are three values for resp: yes, perhaps, and no. Therefore, we need to transform these poytomous responses into dichotomous responses. To reformat the data, we will use the polyreformat function in the eirm package. The following example demonstrates how polytomous responses in the verbal aggression dataset can be reformatted into a dichotomous form:\r\n\r\n\r\nVerbAgg2 <- polyreformat(data=VerbAgg, id.var = \"id\", var.name = \"item\", val.name = \"resp\")\r\n\r\nhead(VerbAgg2)\r\n\r\n\r\n  Anger Gender        item    resp id btype  situ mode r2\r\n1    20      M S1WantCurse      no  1 curse other want  N\r\n2    11      M S1WantCurse      no  2 curse other want  N\r\n3    17      F S1WantCurse perhaps  3 curse other want  Y\r\n4    21      F S1WantCurse perhaps  4 curse other want  Y\r\n5    17      F S1WantCurse perhaps  5 curse other want  Y\r\n6    21      F S1WantCurse     yes  6 curse other want  Y\r\n  polycategory polyresponse                polyitem\r\n1  cat_perhaps            0 S1WantCurse.cat_perhaps\r\n2  cat_perhaps            0 S1WantCurse.cat_perhaps\r\n3  cat_perhaps            1 S1WantCurse.cat_perhaps\r\n4  cat_perhaps            1 S1WantCurse.cat_perhaps\r\n5  cat_perhaps            1 S1WantCurse.cat_perhaps\r\n6  cat_perhaps           NA S1WantCurse.cat_perhaps\r\n\r\nIn the reformatted data, polyresponse is the new response variable (i.e., pseudo-dichotomous version of the original response variable resp) and polycategory represents the response categories. Based on the reformatted data, each item has two rows based on the following rules:\r\nIf polycategory = “cat_perhaps” and resp = “no,” then polyresponse = 0\r\nIf polycategory = “cat_perhaps” and resp = “perhaps,” then polyresponse = 1\r\nIf polycategory = “cat_perhaps” and resp = “yes,” then polyresponse = NA\r\nand\r\nIf polycategory = “cat_yes” and resp = “no,” then polyresponse = NA\r\nIf polycategory = “cat_yes” and resp = “perhaps,” then polyresponse = 0\r\nIf polycategory = “cat_yes” and resp = “yes,” then polyresponse = 1\r\nNow we can estimate an explanatory IRT model using the reformatted data. The following model explains only the first threshold (i.e., threshold from no to perhaps) based on three predictors:\r\n\r\n\r\nmod1 <- eirm(formula = \"polyresponse ~ -1 + situ + btype + mode + (1|id)\", \r\n             data = VerbAgg2)\r\n\r\n\r\n\r\nThe regression-like formula polyresponse ~ -1 + situ + btype + mode + (1|id) describes the dependent variable (polyresponse) and predictors (situ, btype, and mode). Also, (1|id) refers to the random effects for persons represented by the id column in the data set. Using the print function or simply typing mod1 in the R console, we can print out the estimated coefficients for the predictors:\r\n\r\n\r\nprint(mod1)\r\n\r\n\r\nEIRM formula: \"polyresponse ~ -1 + situ + btype + mode + (1|id)\" \r\n\r\nNumber of persons: 316 \r\n\r\nNumber of observations: 9665 \r\n\r\nNumber of predictors: 5 \r\n\r\nParameter Estimates:\r\n\r\n           Easiness    S.E. z-value    p-value\r\nsituother    0.4846 0.06628   7.312  2.626e-13\r\nsituself    -0.2239 0.06548  -3.419  6.277e-04\r\nbtypescold  -0.5936 0.05334 -11.128  9.208e-29\r\nbtypeshout  -1.2387 0.05810 -21.319 7.562e-101\r\nmodedo      -0.4264 0.04576  -9.319  1.179e-20\r\n\r\nNote: The estimated parameters above represent 'easiness'.\r\nUse difficulty = TRUE to get difficulty parameters.\r\n\r\nThe mod1 object is essentially a glmerMod-class object from the lme4 package (Bates et al., 2015). All glmerMod results for the estimated model can seen with mod1$model. For example, estimated random effects for persons (i.e., theta estimates) can be obtained using:\r\n\r\n\r\ntheta <- ranef(mod1$model)$id\r\nhead(theta)\r\n\r\n\r\n\r\nIt is possible to visualize the parameters using an item-person map. Note that this plot is a modified version of the plotPImap function from the eRm package (Mair et al., 2020). Aesthetic elements such as axis labels and plot title can be added to the plot. Please run ?plot.eirm for further details on the plot option.\r\n\r\n\r\nplot(mod1)\r\n\r\n\r\n\r\n\r\nThe previous model could be modified to explain the first threshold (i.e., threshold from no to perhaps) and second threshold (perhaps to yes) based on explanatory variables using the following codes:\r\n\r\n\r\nmod2 <- eirm(formula = \"polyresponse ~ -1 + btype + situ + mode + \r\n             polycategory + polycategory:btype + (1|id)\", data = VerbAgg2)\r\n\r\n\r\n\r\nConclusion\r\nBefore I conclude this post, I want to mention a few caveats with the eirm package. First, the eirm package is using a different optimizer to speed up the estimation process. However, as the size of response data (i.e., items and persons) increases, the estimation duration may increase significantly. Second, it is possible to use dichotomous and polytomous items within the same model. In that case, a single difficulty is estimated for dichotomous items (i.e., Rasch model) and multiple thresholds for polytomous items (i.e., either Partial Credit model or Rating Scale model). Lastly, the eirm package is not capable of estimating a discrimination parameter for the items (just like lme4 can’t). Structural equation modeling could be a good alternative for those who want to estimate both discrimination and difficulty parameters for the items.\r\n\r\n\r\n\r\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\r\n\r\n\r\nBulut, O. (2021). eirm: Explanatory item response modeling for dichotomous and polytomous item responses. https://doi.org/10.5281/zenodo.4556285\r\n\r\n\r\nBulut, O., Palma, J., Rodriguez, M. C., & Stanke, L. (2015). Evaluating measurement invariance in the measurement of developmental assets in latino english language groups across developmental stages. SAGE Open, 5(2), 2158244015586238. https://doi.org/10.1177/2158244015586238\r\n\r\n\r\nDe Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx, F., Partchev, I., & others. (2011). The estimation of item response models with the lmer function from the lme4 package in r. Journal of Statistical Software, 39(12), 1–28.\r\n\r\n\r\nDe Boeck, P., & Wilson, M. (2004). Explanatory item response models: A generalized linear and nonlinear approach. Springer Science & Business Media.\r\n\r\n\r\nKan, A., & Bulut, O. (2014). Examining the relationship between gender DIF and language complexity in mathematics assessments. International Journal of Testing, 14(3), 245–264. https://doi.org/10.1080/15305058.2013.877911\r\n\r\n\r\nMair, P., Hatzinger, R., & Maier, M. J. (2020). eRm: Extended Rasch Modeling. https://cran.r-project.org/package=eRm\r\n\r\n\r\nStanke, L., & Bulut, O. (2019). Explanatory item response models for polytomous item responses. International Journal of Assessment Tools in Education, 6(2), 259–278.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-14-explanatory-irt-models-in-r/explanatory-irt-models-in-r_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-12-09T16:34:10-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
