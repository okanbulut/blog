---
title: "Lexicon-Based Sentiment Analysis Using R"
description: |
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed vehicula tempor bibendum. Sed augue turpis, efficitur ut ipsum quis, fermentum pulvinar quam. Proin molestie orci erat, et condimentum dui ornare ac. Quisque sit amet lacus id nisl pellentesque sagittis. Vestibulum non urna non eros condimentum commodo. Proin vitae nulla fermentum.
  ```{r, include=FALSE}
  bytes <- file.size("lexicon-based-sentiment-analysis-using-r.Rmd")
  words <- bytes/10
  minutes <- words/200
  ``` 
  (`r round(minutes)` min read)
author:
  - name: Okan Bulut
    url: http://www.okanbulut.com/
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0001-5853-1267
date: 2024-01-31
categories:
  - data science
  - natural language processing
  - text mining
  - sentiment
preview: emoji.jpg
bibliography: sentiment.bib
csl: apa.csl
output:
  distill::distill_article:
    self_contained: false
    toc: true
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
suppressWarnings({
  # Add all the packages you will use in the article here
  library("rmarkdown")
  library("kableExtra")
  # https://github.com/hadley/emo
  library("emo")
  library("dplyr")
  library("tidytext")
  library("ggplot2")
  library("tidyr")
  library("sentimentr")
  library("magrittr")
})
```

![Photo by [Roman Odintsov](https://www.pexels.com/@roman-odintsov/) on [Pexels](https://www.pexels.com/)](emoji.jpg)

## Introduction

During the COVID-19 pandemic, I decided to learn a new statistical technique to keep my mind occupied instead of watching the news all the time. Among the several techniques I reviewed, those related to natural language processing (NLP) were the most interesting ones. So, I decided to choose one technique from this field and learn more about it. This was sentiment analysis, which is also referred to as "opinion mining" in the literature. This analytical approach enables researchers to extract and interpret emotions expressed towards a particular subject in a written text. With sentiment analysis, one can discern the polarity (positive or negative), nature, and intensity of sentiments expressed across various textual formats such as documents, customer reviews, and social media posts.

Amidst the COVID-19 pandemic, numerous researchers turned to sentiment analysis to gauge public responses to news and developments surrounding the virus. This included scrutinizing user-generated content on popular social media platforms like Twitter, YouTube, and Instagram. Inspired by this approach, my colleagues and I aimed to expand upon existing research by analyzing the daily briefings delivered by public health authorities. In Alberta, Dr. Deena Hinshaw, the province's chief medical officer of health, regularly provided [updates on the region's response to the ongoing pandemic](https://www.youtube.com/watch?v=fvw_USRfXgY), Through our analysis of these public health announcements, we sought to evaluate Alberta's effectiveness in employing communication strategies during this complex public health crisis [@sentimentbulut; @sentimentpoth].

In this post, my aim is to guide you through the process of conducting sentiment analysis using R. Specifically, I'll delve into "lexicon-based sentiment analysis," which I'll explain more in the next section. You'll find examples of lexicon-based sentiment analysis that we've incorporated into our publications mentioned above. In upcoming posts, I'll also explore more sophisticated forms of sentiment analysis leveraging cutting-edge pre-trained models available on [Hugging Face](https://huggingface.co/docs/transformers/en/index).

## Lexicon-Based Sentiment Analysis

As I began to explore sentiment analysis, I found that the most common method of extracting sentiments was through lexicon-based sentiment analysis. This technique involves the use of a specific lexicon, which is essentially the vocabulary of a language or subject, to determine the direction and strength of sentiments expressed in a given text. Some lexicons, such as the Bing lexicon [@hu2004mining], categorize words as either positive or negative. Other lexicons offer more descriptive labels for the sentiments, such as the NRC Emotion Lexicon [@mohammad2013crowdsourcing], which categorizes words based on both positive and negative sentiments and Plutchik’s [@plutchik1980general] psych evolutionary theory of basic emotions (i.e., anger, fear, anticipation, trust, surprise, sadness, joy, and disgust).

Lexicon-based sentiment analysis is a process that involves matching words in a given text with the words present in general-purpose lexicons like NRC and Bing. Each word is assigned a specific sentiment, such as positive or negative. The overall sentiment score of the text is then calculated by adding up the individual sentiment scores of each word. For example, if a text contains 50 positive and 30 negative words based on the Bing lexicon, the total sentiment score would be 20, which indicates that the text expresses mostly positive sentiments. Conversely, a negative value would suggest the opposite sentiment.

Performing lexicon-based sentiment analysis using R can be both fun and tricky at the same time. While analyzing public health announcements in terms of sentiments, I found Julia Silge and David Robinson's book, [Text Mining with R](https://www.tidytextmining.com/), to be very helpful. The book has [a chapter dedicated to sentiment analysis](https://www.tidytextmining.com/sentiment), where the authors demonstrate how to conduct sentiment analysis using general-purpose lexicons like Bing and NRC. However, Julia and David also highlight a major limitation of lexicon-based sentiment analysis. The analysis considers only single words (i.e., unigrams) and does not consider qualifiers before a word. For instance, negation words like "not" in "not true" are ignored, and sentiment analysis processes them as two separate words, "not" and "true". Furthermore, if a particular word (either positive or negative) is repeatedly used throughout the text, this may skew the results depending on the polarity (positive or negative) of this word. Therefore, the results of lexicon-based sentiment analysis should be interpreted carefully. 

Now, let's move to our example where we will conduct lexicon-based sentiment analysis using Dr. Deena Hinshaw's media briefings during the COVID-19 pandemic. My goal is to showcase several R packages capable of running sentiment analysis `r ji("graph")`.

## Example 

For the sake of simplicity, we will focus on the first wave of the pandemic (March 2020 - June 2020). The transcripts of all media briefings were available in the government of Alberta’s COVID-19 pandemic website (<https://www.alberta.ca/covid>). After importing these transcripts into R, I turned all the text into lowercase and then applied word tokenization using the **tidytext** [@R-tidytext] and **tokenizers** [@R-tokenizers] packages. Word tokenization split the sentences in the media briefings into individual words for each entry (i.e., day of media briefings). Next, I applied lemmatization to the tokens to resolve each word into its canonical form using the **textstem** package [@R-textstem]. Finally, I removed common stopwords, such as "my", "for", "that", "with", and "for, using the **stopwords** package [@R-stopwords]. The final dataset is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData). Now, let's import the data into R and then review its content. 

```{r ch1, echo=TRUE, eval=FALSE}
load("wave1_alberta.RData")

head(wave1_alberta, 10)
```

```{r ch2, echo=FALSE, eval=TRUE}
load("wave1_alberta.RData")
paged_table(wave1_alberta, options = list(cols.print = 12))
```

The dataset has three columns: 

* month (the month of the media briefing)
* date (the exact date of the media briefing), and 
* word (words or tokens used in media briefing)

### Descriptve Analysis

Now, we can calculate some descriptive statistics to better understand the content of our dataset. We can begin by finding the most common 5 words for each month. 

```{r ch3, echo=TRUE, eval=TRUE}
library("dplyr")

wave1_alberta %>%
  group_by(month) %>%
  count(word, sort = TRUE) %>%
  slice_head(n = 5) %>%
  as.data.frame()
```

We see that words such as health, continue, and test were commonly used in the media briefings across this 4-month period. We can also expand our list to the most common 10 words and print it visually:

```{r ch4, echo=TRUE, eval=TRUE, fig.cap="Most common words based on frequency"}
library("tidytext")
library("ggplot2")

wave1_alberta %>%
  group_by(month) %>%
  count(word, sort = TRUE) %>%
  # Find the top 10 words
  slice_head(n = 10) %>%
  ungroup() %>%
  # Order the words by their frequency within each month
  mutate(word = reorder_within(word, n, month)) %>%
  # Create a bar graph
  ggplot(aes(x = n, y = word, fill = month)) +
  geom_col() +
  scale_y_reordered() +
  facet_wrap(~ month, scales = "free_y") +
  labs(x = "Frequency", y = NULL) +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11),
        strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 13))
```

Since some words may be common across all four months, the plot above may not necessarily show us the important words that are unique to each month. To find such important words, we can use Term Frequency - Inverse Document Frequency (TF-IDF)--a widely used technique in NLP for measuring how important a term is within a document relative to a collection of documents (for more detailed information about TF-IDF, check out [my previous blog post](https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf)). In our example, we will treat media briefings for each month as a document and calculate TF-IDF for the tokens (i.e., words) within each document. The first part of the R codes below creates a new dataset, *wave1_tf_idf*, by calculating TF-IDF for all tokens and selecting the tokens with the highest TF-IDF values within each month. Next, we will create a bar plot with the TF-IDF values to view the common words unique to each month. 

```{r ch5, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Most common words based on TIF-IDF"}
# Calculate TF-IDF for the words for each month
wave1_tf_idf <- wave1_alberta %>%
  count(month, word, sort = TRUE) %>%
  bind_tf_idf(word, month, n) %>%
  arrange(month, -tf_idf) %>%
  group_by(month) %>%
  top_n(10) %>%
  ungroup

# Visualize the results
wave1_tf_idf %>%
  mutate(word = reorder_within(word, tf_idf, month)) %>%
  ggplot(aes(word, tf_idf, fill = month)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ month, scales = "free", ncol = 2) +
  scale_x_reordered() +
  coord_flip() +
  theme(strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 13),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11)) +
  labs(x = NULL, y = "TF-IDF")
```

Before we move to the sentiment analysis, let's take a look at another descriptive variable: the length of each media briefing. We can see whether the length of the media briefings (i.e., the number of tokens) varied over time.

```{r ch6, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Number of words by days"}
wave1_alberta %>%
  mutate(day = substr(date, 9, 10)) %>%
  group_by(month, day) %>%
  summarize(n = n()) %>%
  ggplot(aes(day, n, color = month, shape = month, group = month)) +
  geom_point(size = 2) + 
  geom_line() + 
  labs(x = "Days", y = "Number of Words") +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90, size = 11),
        strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 11),
        axis.text.y = element_text(size = 11)) +
  ylim(0, 800) +
  facet_wrap(~ month, scales = "free_x")
```

The figure above shows that the length of media briefings varied over time. Especially in March and May, there are larger fluctuations (i.e., very long or short briefings), whereas in June, the daily media briefings are quite similar in terms of length. 

### Sentiment Analysis with **tidytext**

After analyzing the dataset descriptively, we are ready to begin with the sentiment analysis. In the first part, we will use the **tidytext** package for performing sentiment analysis. We will first import the lexicons into R and then merge them with our dataset. Using the Bing lexicon, we need to count the number of positive and negative words to produce a sentiment score (i.e., sentiment = the number of positive words - the number of negative words).

```{r ch7, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Sentiment scores based on the Bing lexicon"}
# From the three lexicons, Bing is already available in the tidytext page
# for AFINN and NRC, install the textdata package by uncommenting the next line
# install.packages("textdata")
get_sentiments("bing") 
get_sentiments("afinn") 
get_sentiments("nrc")

library("tidyr")

# Sentiment scores with bing (based on frequency)
wave1_alberta %>%
  mutate(day = substr(date, 9, 10)) %>%
  group_by(month, day) %>%
  inner_join(get_sentiments("bing")) %>%
  count(month, day, sentiment) %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(day, sentiment, fill = month)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Days", y = "Sentiment Score") +
  ylim(-50, 50) + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +
  facet_wrap(~ month, ncol = 2, scales = "free_x") +
  theme(strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 11),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11)) 
```

The figure shows that the sentiments delivered in the media briefings were generally negative, which is not necessarily surprising since the media briefings were all about how many people passed away, hospitalization rates, potential outbreaks, etc. On certain days (e.g., March 24, 2020 and May 4, 2020), the media briefings were particularly more negative in terms of sentiments. 

Next, we will use the AFINN lexicon. Unlike Bing that labels words as positive or negative, AFINN assigns a numerical weight to each word. The sign of the weight indicates the polarity of sentiments (i.e., positive or negative) while the value indicates the intensity of sentiments. Now, let's see if these weighted values produce different sentiment scores. 


```{r ch8, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Sentiment scores based on the AFINN lexicon"}
wave1_alberta %>%
  mutate(day = substr(date, 9, 10)) %>%
  group_by(month, day) %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(month, day) %>%
  summarize(sentiment = sum(value),
            type = ifelse(sentiment >= 0, "positive", "negative")) %>%
  ggplot(aes(day, sentiment, fill = type)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Days", y = "Sentiment Score") +
  ylim(-100, 100) + 
  facet_wrap(~ month, ncol = 2, scales = "free_x") +
  theme(legend.position = "none", 
        strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 11),
        axis.text.x = element_text(size = 11, angle = 90),
        axis.text.y = element_text(size = 11))
```

The results based on the AFINN lexicon are quite different! It seems that once we take the "weight" of the tokens into account, most media briefings are overall positive (see the green bars), although there are still some day with negative sentiments (see the red bars). The two analyses we have done so far have yielded very different for two reasons. First, as I mentioned above, the Bing lexicon focuses on the polarity of the words but ignore the intensity of the words (dislike and hate are considered negative words with equal intensity). Unlike the Bing lexicon, the AFINN lexicon takes the intensity into account, which impacts the calculation of the sentiment scores. Second, the Bing lexicon (6786 words) is fairly larger than the AFINN lexicon (2477 words). Therefore, it is likely that some tokens in the media briefings are included in the Bing lexicon but not in the AFINN lexicon. Disregarding those tokens may impact the results substantially.

The final lexicon we are going to try using the **tidytext** package is NRC. As I mentioned earlier, this lexicon uses uses Plutchik’s [@plutchik1980general] psych evolutionary theory to label the tokens based on basic emotions such as anger, fear, and anticipation. WE are going to count the number of words or token associated with each emotion and then 

```{r ch9, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Sentiment scores based on the NRC lexicon"}
wave1_alberta %>%
  mutate(day = substr(date, 9, 10)) %>%
  group_by(month, day) %>%
  inner_join(get_sentiments("nrc")) %>%
  count(month, day, sentiment) %>%
  group_by(month, sentiment) %>%
  summarize(n_total = sum(n)) %>%
  ggplot(aes(n_total, sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Frequency", y = "") +
  xlim(0, 2000) + 
  facet_wrap(~ month, ncol = 2, scales = "free_x") +
  theme(strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 11),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11))
```

The figure shows that the media briefings were mostly positive each month. Dr. Hinshaw also used words associated with "trust", "anticipation", and "fear". Overall, the pattern of these emotions seems to remain very similar over time, indicating the consistency of the media briefings in terms of the type and intensity of the emotions delivered. 

### Sentiment Analysis with **sentimentr**

Another package for lexicon-based sentiment analysis is **sentimentr** [@R-sentiment]. Unlike the **tidytext** package, this package takes valence shifters (e.g., negation) into account, which can easily flip the polarity of a sentence with one word. For example, the sentence "I am not unhappy" is actually positive but if we analyze it word by word, the sentence may seem to have a negative sentiment due to the words "not" and "unhappy". Similarly, "I hardly like this book" is a negative sentence but the analysis of individual words, "hardly" and "like", may yield a positive sentiment score. The **sentimentr** package addresses the limitations around sentiment detection with valence shifters (see the package author Tyler Rinker’s Github page for further details on **sentimentr**: <https://github.com/trinker/sentimentr>). 

To benefit from the **sentimentr** package, we need actual sentences in the media briefings rather than the individual tokens. Therefore, I had to create an untokenized version of the dataset, which is available [**here**](https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData). We will first import this dataset into R, get individual sentences for each media briefing using the `get_sentences()` function, and then calculate sentiment scores by day and month via `sentiment_by()`.

```{r ch10, echo=TRUE, eval=FALSE}
library("sentimentr")
library("magrittr")

load("wave1_alberta_sentence.RData")

# Calculate sentiment scores by day and month
wave1_sentimentr <- wave1_alberta_sentence %>%
  mutate(day = substr(date, 9, 10)) %>%
  get_sentences() %$%
  sentiment_by(text, list(month, day))

# View the dataset
head(wave1_sentimentr, 10)
```

```{r ch11, echo=FALSE, eval=TRUE}
load("wave1_alberta_sentence.RData")

# Calculate sentiment scores by day and month
wave1_sentimentr <- wave1_alberta_sentence %>%
  mutate(day = substr(date, 9, 10)) %>%
  get_sentences() %$%
  sentiment_by(text, list(month, day))

paged_table(wave1_sentimentr, options = list(cols.print = 12))
```

In the dataset we created, "ave_sentiment" is the average sentiment score for each day in March, April, May, and June (i.e., days where a media briefing was made). Using this dataset, we can visualize the sentiment scores. 

```{r ch12, echo=TRUE, eval=TRUE, layout="l-body-outset", fig.width=8, fig.height=6, fig.cap="Sentiment scores based on **sentimentr**"}
wave1_sentimentr %>%
  group_by(month, day) %>%
  ggplot(aes(day, ave_sentiment, fill = ave_sentiment)) +
  scale_fill_gradient(low="red", high="blue") + 
  geom_col(show.legend = FALSE) +
  labs(x = "Days", y = "Sentiment Score") +
  ylim(-0.1, 0.3) +
  facet_wrap(~ month, ncol = 2, scales = "free_x") +
  theme(legend.position = "none", 
        strip.background = element_blank(),
        strip.text = element_text(colour = "black", face = "bold", size = 11),
        axis.text.x = element_text(size = 11, angle = 90),
        axis.text.y = element_text(size = 11))
```

In the figure above, blue bars indicate highly positive sentiment scores whereas red bars indicate relatively lower sentiment scores. The patterns of the sentiment scores produced by **sentimentr** are similar to those based on the AFINN lexicon, although this analysis is based on the original media briefings (instead of only tokens) and the valence shifters are also considered in the computation of sentiment scores. 


## Concluding Remarks

xxx
