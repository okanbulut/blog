---
title: "Introduction to Psychometric Network Analysis"
description: |
  Psychometric network analysis (PNA), also known as network psychometrics, has emerged as a new framework to analyze the relationships among observed variables to better understand the underlying structure of psychological phenomena. Psychometric network models are particularly useful when dealing with complex and multidimensional data. In this post, I provide a brief introduction to psychometric network models, particularly Gaussian graphical models (GGMs), and demonstrate how to estimate them using R. 
  ```{r, include=FALSE}
  bytes <- file.size("introduction-to-psychometric-network-analysis.Rmd")
  words <- bytes/10
  minutes <- words/200
  ``` 
  (`r round(minutes)` min read)
author:
  - name: Okan Bulut
    url: http://www.okanbulut.com/
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0001-5853-1267
date: 2024-01-04
categories:
  - psychometrics
  - network
  - correlation
preview: network.jpg
bibliography: network.bib
csl: apa.csl
output:
  distill::distill_article:
    self_contained: false
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
suppressWarnings({
  # Add all the packages you will use in the article here
  library("rmarkdown")
  library("kableExtra")
  # https://github.com/hadley/emo
  # devtools::install_github("hadley/emo")
  library("emo")
  library("dplyr")
  library("psych")
  library("ggcorrplot")
  library("bootnet")
  library("psychonetrics")
  library("qgraph")
})
```

![Photo by [Omar Flores](https://unsplash.com/@designedbyflores) on [Unsplash](https://unsplash.com/)](network.jpg)

## Introduction

Psychometric Network Analysis (PNA), which is also referred to as "network psychometrics", is an emerging field that combines concepts from psychometrics and network analysis to study the relationships among psychological variables. Generally, psychometrics involves the measurement of latent traits, while network analysis focuses on modeling and analyzing complex systems of interconnected elements. PNA aims to represent the relationships between psychological constructs, such as symptoms, traits, or behaviors, as a network of interconnected nodes. The figure below shows a network graph for the Big Five Personality Test:

```{r, echo=FALSE, fig.cap="Figure from [Epskamp (2017)](https://hdl.handle.net/11245.1/a76273c6-6abc-4cc7-a2e9-3b5f1ae3c29e)", , out.extra="class=external", layout="l-body-outset"}
knitr::include_graphics("pna1.png")
```

Each node represents a specific personality trait, and the edges (connections) between nodes reflect the statistical relationships between those variables. The edges can be weighted to represent the strength (e.g., thicker edges represents stronger associations) and direction of the relationships between the corresponding traits (e.g., green lines for positive and red lines for negative associations).

In social sciences (especially in psychology and education), researchers often use "partial correlation" coefficients to understand the associations between the variables. A partial correlation is the association between two [quantitative] variables, after conditioning on all other variables in the dataset. Networks based on partial correlations are known as Gaussian Graphical Models (GGMs; @costantini). A GGM can be mathematically expressed as follows:

$$Σ=Δ(I−Ω)^{−1}Δ,$$
where $Σ$ is the variance-covariance matrix of the observed variables, $Δ$ is a diagonal matrix, $Ω$ is a matrix containing weights linking observed variables, conditioned on all other variables, and $I$ is the identity matrix. 

To prevent over-interpretation in network structures, we want to limit the number of spurious connections (i.e., weak partial correlations due to sampling variation). It is possible to eliminate spurious connections using statistical regularization techniques, such as "least absolute shrinkage and selection operator" (LASSO; @tibshirani). LASSO (L1) regularization shrinks partial correlation coefficients when estimating a network model, which means that small coefficients are estimated to be exactly zero. 

As an extension of LASSO, graphical LASSO, or shortly glasso [@friedman], involves a penalty parameter ($\lambda$) to remove weak associations within a network. The following figure demonstrates the impact of the penalty parameter. 

```{r, echo=FALSE, fig.cap="Figure Adapted from [Epskamp and Fried (2017)](https://doi.org/10.1037/met0000167)", out.extra="class=external", layout="l-body-outset"}
knitr::include_graphics("pna2.png")
```

When estimating network models, glasso is typically combined with the extended Bayesian information criterion (EBIC; @chen2008) for tuning parameter selection, resulting in EBICglasso [@epskamp2018tutorial]. If the dataset for PNA consists of continuous variables that are multivariate normally distributed, then we can estimate a GGM based on partial correlations with glasso or EBICglasso. GGM can also be used for ordinal data (e.g., Likert-scale data) wherein the network is based on the polychoric correlations instead of partial correlations.

In this blog post, I want to demonstrate how to perform PNA and visualize resulting network models (specifically, GGMs) using R `r ji("web")` `r ji("coder")`. I highly encourage readers to check out [Network Psychometrics with R](https://doi.org/10.4324/9781003111238) by @isvoranu2022 for a comprehensive discussion of network models and their estimation using R. 

Let's get started `r emo::ji("biceps")`.

## Example

In our example, we will use a subset of the Synthetic Aperture Personality Assessment (SAPA)--a web based personality assessment project (<https://www.sapa-project.org/>). The purpose of SAPA is to find patterns among the vast number of ways that people differ from one another in terms of their thoughts, feelings, interests, abilities, desires, values, and preferences [@condon2014; @revelle2010]. The dataset consists of 16 SAPA items sampled from the full instrument (80 items). These items measure four subskills (i.e., verbal reasoning, letter series, matrix reasoning, and spatial rotations) as part of the general intelligence, also known as *g* factor. The "sapa_data.csv" dataset is a data frame with 1525 individuals who responded to 16 multiple-choice items in SAPA. The original dataset is included in the **psych** package [@R-psych]. The dataset can be downloaded from [**here**](https://raw.githubusercontent.com/okanbulut/psychometrics/master/sapa_data.csv). Now, let's import the data into R and then review its content.


```{r ch1, eval=FALSE}
# Read the data in R
sapa <- read.csv("sapa_data.csv", header = TRUE)

# Preview the data
head(data)
```

```{r ch2, echo=FALSE}
# Read the data in R
sapa <- read.csv("sapa_data.csv", header = TRUE)
paged_table(sapa, options = list(cols.print = 12))
```

Next, we will check out the correlations among the SAPA items. Since the items are dichotomous (i.e., binary), we will compute the tetrachoric correlations and then visualize the matrix using a correlation matrix plot.

```{r ch3, fig.width=8, fig.height=5, fig.cap="Correlation matrix plot of the SAPA items", layout="l-body-outset"}
library("psych")
library("ggcorrplot")  

# Save the correlation matrix
cormat <- psych::tetrachoric(x = sapa)$rho

# Correlation matrix plot
ggcorrplot::ggcorrplot(corr = cormat, # correlation matrix
                       type = "lower", # print only the lower part of the correlation matrix
                       hc.order = TRUE, # hierarchical clustering
                       show.diag = TRUE, # show the diagonal values of 1
                       lab = TRUE, # add correlation values as labels
                       lab_size = 3) # Size of the labels

```

The figure above shows that all of the SAPA items are positively correlated with each other. the items associated with the same construct (e.g., rotation) seem to have been clustered together (e.g., see the rotate.4, rotate.3, rotate.6, and rotate.8 as the top cluster). Now, we can go ahead and estimate a GGM based on this correlation matrix. 

We will use the **bootnet** package [@R-bootnet]. This package has many methods to estimate GGMs. In our example, we will use "IsingFit" because the Ising model deal with binary data (see @ising and @marsman2018 for more details on the Ising network models). This model combines L1-regularized logistic regression with model selection based on EBIC.

```{r ch4, fig.width=8, fig.height=8, fig.cap="Network plot for the Ising model", layout="l-body-outset"}
library("bootnet")

network_sapa_1 <- bootnet::estimateNetwork(
  data = sapa, # dataset
  default = "IsingFit", # for estimating GGM with the Ising model and EBIC,
  verbose = FALSE # Ignore real-time updates on the estimation process
)

# View the estimated network
plot(network_sapa_1, layout = "spring") 
```

From the network plot above, we can see that the items associated with the same construct often have stronger edges between the nodes (e.g., see the thick edges between the rotation items). These items also appear to be clustered closely within the network. 


What if we compute the polychoric correlations for the SAPA items using `corMethod = "cor_auto"` and estimate a GGM with glasso and EBIC? 

```{r ch5, fig.width=8, fig.height=8, fig.cap="Network plot for the GGM", layout="l-body-outset"}
network_sapa_2 <- bootnet::estimateNetwork(
  data = sapa,
  corMethod = "cor_auto", # for polychoric correlations
  default = "EBICglasso", # for estimating GGM with glasso and EBIC
  verbose = FALSE
)

# View the estimated network
plot(network_sapa_2, layout = "spring") 
```

By default, the **bootnet** package uses `tuning = 0.5` for the penalty parameter for EBICglasso. We can increase or decrease this parameter to adjust the penalty on the model (**Note:** `tuning = 0` leads to model selection based on BIC instead of EBIC). In the following example, we will use `tuning = 1` to apply a larger penalty parameter to the model. 


```{r ch6, fig.width=8, fig.height=8, fig.cap="Network plot for the GGM after tuning", layout="l-body-outset"}
network_sapa_3 <- bootnet::estimateNetwork(
  data = sapa,
  corMethod = "cor_auto", # for polychoric correlations
  default = "EBICglasso", # for estimating GGM with glasso and EBIC
  tuning = 1, 
  verbose = FALSE
)

# View the estimated network
plot(network_sapa_3, layout = "spring") 
```

The final network plot includes fewer edges compared to the previous one with the default tuning parameter of 0.5. As the tuning parameter gets larger, more edges may be eliminated from the network, leading to a more compact and interpretable model. 

I also want to demonstrate the same analysis using the **psychonetrics** package [@R-psychonetrics]. This package is more versatile than **bootnet** as it is capable of estimating both network models and structural equation models. 

```{r ch7}
library("psychonetrics")
library("dplyr")

network_sapa_4 <- psychonetrics::ggm(data = sapa,
                                     omega = "full", # or "empty" to set all elements to zero
                                     delta = "full", # or "empty" to set all elements to zero
                                     estimator = "FIML", # or "ML", "ULS", or "DWLS"
                                     verbose = FALSE) %>%
  psychonetrics::runmodel() # Run the model

# View the model parameters
network_sapa_4 %>% 
  psychonetrics::parameters()
```

We can also see some information on the model fit, which can be used for making comparisons if we make any changes to the model (e.g., pruning). 

```{r ch8}
# Look at the model fit
network_sapa_4 %>% 
  psychonetrics::fit()
```

Another important feature of the `ggm()` function is that statistically insignificant edges can be removed to have a more compact model. In the following codes, we will eliminate edges that are not statistically significant at the alpha level of $\alpha = .05$, after performing a Benjamini and Hochberg (known as "BH" or "fdr") correction [@BH]. We will then compare this model with the previous model. 

```{r ch9}
# We can prune this model to remove insignificant edges
network_sapa_5 <- psychonetrics::ggm(data = sapa,
                                     omega = "full", # or "empty" to set all elements to zero
                                     delta = "full", # or "empty" to set all elements to zero
                                     estimator = "FIML", # or "ML", "ULS", or "DWLS"
                                     verbose = FALSE) %>%
  psychonetrics::runmodel() %>%
  psychonetrics::prune(adjust = "fdr", alpha = 0.05)

# Compare the models
comparison <- psychonetrics::compare(
  `1. Original model`  = network_sapa_4,
  `2. Compact model after pruning` = network_sapa_5)

print(comparison)
```

The results show that the compact model with pruning fits the data better based on BIC although AIC shows the opposite result. In the final step, we will visualize the last network model resulting from `ggm()` using the famous **qgraph** package [@R-qgraph]:

```{r ch10, fig.width=8, fig.height=8, fig.cap="Network plot using **qgraph**", layout="l-body-outset"}
library("qgraph")

# Obtain the network plot
net5 <- getmatrix(network_sapa_5, "omega")

qgraph::qgraph(net5, 
               layout = "spring", 
               theme = "colorblind",
               labels = colnames(sapa))
```


## Conclusion

This was a very brief introduction to PNA using R. There are still many things to discuss in PNA. Thus, I plan to write several blog posts on PNA over the next few months. For example, I want to discuss centrality indices and demonstrate how to generate centrality indices for network models using R. I will also write a post on the Ising model for binary data to demonstrate the similarities and differences between an Ising network model and an item response theory (IRT) model. 





