---
title: "Visualizing Machine Learning Models"
description: |
 Data visualization plays an important role when evaluating the performance of machine learning models. In this post, we demonstrate how to use the **DALEX** package for visualizing machine learning models effectively. Visualizations with **DALEX** can facilitate the comparison of machine learning models and help researchers understand which model works better and why.
  ```{r, include=FALSE}
  bytes <- file.size("visualizing-machine-learning-models.Rmd")
  words <- bytes/10
  minutes <- words/200
  ``` 
  (`r round(minutes)` min read)
author:
  - name: Okan Bulut
    url: http://www.okanbulut.com/
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0001-5853-1267
  - name: Seyma Nur Yildirim-Erbasli 
    url: https://www.ualberta.ca
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0002-8010-9414
date: "`r Sys.Date()`"
categories:
  - machine learning
  - classification 
  - data visualization
preview: algorithm.jpg
bibliography: references.bib
csl: apa.csl
output:
  distill::distill_article:
    self_contained: false
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
suppressWarnings({
  library("rmarkdown")
  library("kableExtra")
  library("emo")
  library("caret")
  library("DALEX")
  library("ggplot2")
  library("patchwork")
})
```

![Photo by [Gerd Altmann](https://pixabay.com/users/geralt-9301/) on [pixabay](https://pixabay.com/)](algorithm.jpg)

## Introduction

Over the last few years, advanced machine learning algorithms have been widely utilized in different contexts of education. The literature shows that educational researchers typically perform machine learning models for classification (or prediction) problems, such as student engagement [e.g., @hew2018understanding], performance [e.g., @xu2017machine], and dropout [e.g., @tan2015prediction]. Researchers often try different classification algorithms and select the most accurate model based on model evaluation metrics (e.g., recall, precision, accuracy, and area under the curve). However, the comparison and evaluation of machine learning models based on these evaluation metrics are not necessarily easy to use for most researchers. 

In this post, we demonstrate a versatile R package that can be used to visualize and interpret machine learning models: **DALEX** [@dalex]. **DALEX** package stands for mo**D**el **A**gnostic **L**anguage for **E**xploration and e**X**planation. It can be used for both regression and classification tasks in machine learning. With the **DALEX** package, we can examine residual diagnostics, feature importance, the relationship between features and the outcome variable, the accuracy of future predictions, and many other things. Using real data from a large-scale assessment, we will review some of the data visualization tools available in **DALEX**. 

Now, let's get started `r emo::ji("trend")`.

## Example 

In this example, we will use student data from the OECD's [Programme for International Student Assessment (PISA)](https://www.oecd.org/pisa/test/). PISA is an international. large-scale assessment that measures 15-year-old students' competency in reading, mathematics and science. Using the Turkish sample of the PISA 2015 database, we will build a binary classification model that predicts students' reading performance (i.e., high vs. low performance) and then use the **DALEX** package to evaluate and compare different machine learning algorithms. The data set is available [here](https://raw.githubusercontent.com/okanbulut/blog/master/data_and_codes/PISA_Turkey.csv). The variables in the data set are shown below:

Variable      | Description
------------- | -------------
gender        | Gender
grade         | Grade
computer      | Having a vomputer at home
internet      | Having Internet at home
desk          | Having a study desk at home?
own.room      | Owning a room at home
quiet.study   | Owning a quiet study area at home
book.sch      | Having school books
tech.book     | Having technical books
art.book      | Having art books
reading       | Students' reading scores in PISA 2015
   
First, we will import the data into R and preview its content:

```{r ch1, eval=FALSE}
pisa <- read.csv("PISA_Turkey.csv", header = TRUE)
head(pisa)
```

```{r ch2, eval=TRUE, echo=FALSE}
pisa <- read.csv("PISA_Turkey.csv", header = TRUE)
paged_table(pisa, options = list(cols.print = 12))
```

Second, we will remove missing cases from the data.

```{r ch3, echo=TRUE}
pisa <- na.omit(pisa)
```

Next, we will convert gender and grade to numeric variables. Also, in the **DALEX** package, the outcome variable needs to be a numeric vector for both regression and classification tasks. Thus, we will transform students' reading scores into a binary variable based on the average reading score: 1 (i.e., high performance) vs. 0 (i.e., low performance).

```{r ch4, echo=TRUE}
# Convert gender to a numeric variable
pisa$gender = (as.numeric(sapply(pisa$gender, function(x) {
  if(x=="Female") "1"
  else if (x=="Male") "0"})))

# Convert grade to a numeric variable
pisa$grade = (as.numeric(sapply(pisa$grade, function(x) {
  if(x=="Grade 7") "7"
  else if (x=="Grade 8") "8"
  else if (x=="Grade 9") "9"
  else if (x=="Grade 10") "10"
  else if (x=="Grade 11") "11"
  else if (x=="Grade 12") "12"})))

# Convert reading performance to a binary variable based on the average score 
# 1 represents high performance and 0 represents low performance
pisa$reading <- factor(ifelse(pisa$reading >= mean(pisa$reading), 1, 0))

# View the frequencies for high and low performance groups
table(pisa$reading)
```

Now, we will build a machine learning model using three different algorithms: random forest, logistic regression, and support vector machines. Since the focus of our post is on how to visualize machine learning models, we will build the machine learning models without additional hyperparameter tuning. We use the `createDataPartition()` function from the **caret** package [@caret] to create training (70%) and testing (30%) sets.

```{r ch5, echo=TRUE}
# Activate the caret package
library("caret")

# Set the seed to ensure reproducibility
set.seed(1)

# Split the data into training and testing sets
index <- createDataPartition(pisa$reading, p = 0.7, list = FALSE)
train <- pisa[index, ]
test  <- pisa[-index, ]
```

Next, we use the `train()` function from the **caret** package to create three classification models through 5-fold cross-validation. In each model, reading ~ . indicates that the outcome variable is reading (1 = high performance, 0 = low performance) and the remaining variables are the predictors. 

```{r ch6, echo=TRUE}
# 5-fold cross-validation
control = trainControl(method="repeatedcv", number = 5, savePredictions=TRUE)

# Random Forest
mod_rf = train(reading ~ .,
               data = train, method='rf', trControl = control)

# Generalized linear model (i.e., Logistic Regression)
mod_glm = train(reading ~ .,
                data = train, method="glm", family = "binomial", trControl = control)

# Support Vector Machines
mod_svm <- train(reading ~.,
                 data = train, method = "svmRadial", prob.model = TRUE, trControl=control)
```

Now, we are ready to explore the **DALEX** package. The first step of using the **DALEX** package is to define explainers for machine learning models. For this, we write a custom `predict` function with two arguments: model and newdata. This function returns a vector of predicted probabilities for each class of the binary outcome variable. 

In the second step, we create an explainer for each machine learning model using the `explainer()` function from the **DALEX** package, the testing data set, and the `predict` function. When we convert machine learning models to an *explainer* object, they contain a list of the training and metadata on the machine learning model.

```{r ch7, echo=TRUE}
# Activate the DALEX package
library("DALEX")

# Create a custom predict function
p_fun <- function(object, newdata){
  predict(object, newdata=newdata, type="prob")[,2]
  }

# Convert the outcome variable to a numeric binary vector
yTest <- as.numeric(as.character(test$reading))

# Create explainer objects for each machine learning model
explainer_rf  <- explain(mod_rf, label = "RF",
                         data = test, y = yTest,
                         predict_function = p_fun,
                         verbose = FALSE)

explainer_glm <- explain(mod_glm, label = "GLM",
                         data = test, y = yTest,
                         predict_function = p_fun,
                         verbose = FALSE)

explainer_svm <- explain(mod_svm, label = "SVM",
                         data = test, y = yTest,
                         predict_function = p_fun,
                         verbose = FALSE)
```

### Model Performance

With the **DALEX** package, we can analyze model performance based on the distribution of residuals. Here, we consider the differences between observed and predicted probabilities as residuals. The `model_performance()` function calculates predictions and residuals for the testing data set.

```{r ch8, echo=TRUE}
# Calculate model performance and residuals
mp_rf  <- model_performance(explainer_rf)
mp_glm <- model_performance(explainer_glm)
mp_svm <- model_performance(explainer_svm)

# Random Forest
mp_rf

# Logistic Regression
mp_glm

# Support Vector Machines
mp_svm
```

Based on the performance measures of these three models (i.e., recall, precision, f1, accuracy, and AUC) from the above output, we can say that the models seem to perform very similarly. However, when we check the residual plots, we see how similar or different they are in terms of the residuals. Residual plots show the cumulative distribution function for absolute values from residuals and they can be generated for one or more models. Here, we use the `plot()` function to generate a single plot that summarizes all three models. This plot allows us to make an easy comparison of absolute residual values across models.

```{r ch9, echo=TRUE, fig.cap="Plot of reserve cumulative distribution of residuals"}
# Activate the ggplot2 package
library("ggplot2")

p1 <- plot(mp_rf, mp_glm, mp_svm)
p1
```

From the reverse cumulative of the absolute residual plot, we can see that there is a higher number of residuals in the left tail of the SVM residual distribution. It shows a higher number of large residuals compared to the other two models. However, RF has a higher number of large residuals than the other models in the right tail of the residual distribution.

In addition to the cumulative distributions of absolute residuals, we can also compare the distribution of residuals with boxplots by using geom = "boxplot" inside the `plot` function.

```{r ch10, echo=TRUE, fig.cap="Boxplots of residuals"}
p2 <- plot(mp_rf, mp_glm, mp_svm, geom = "boxplot")
p2
```


Figure \@ref(fig:ch10) shows that RF has the lowest median absolute residual value. Although the GLM model has the highest AUC score, the RF model performs best when considering the median absolute residuals. We can also plot the distribution of residuals with histograms by using geom="histogram" and the precision recall curve by using geom="prc".

```{r ch11, echo=TRUE, fig.cap="Histograms for residuals and precision-recall curve", fig.width=9, fig.height=6, layout="l-body-outset"}
# Activate the patchwork package to combine plots
library("patchwork")

p1 <- plot(mp_rf, mp_glm, mp_svm, geom = "histogram") 
p2 <- plot(mp_rf, mp_glm, mp_svm, geom = "prc") 
p1 + p2
```

### Variable Importance

When using machine learning models, it is important to understand which predictors are more influential on the outcome variable. Using the **DALEX** package, we can see which variables are more influential on the predicted outcome. The `variable_importance()` function computes variable importance values through permutation, which then can be visually examined using the `plot` function. 

```{r ch12, echo=TRUE, fig.cap="Feature importance plots", fig.width=8, fig.height=6, layout="l-body-outset"}
vi_rf <- variable_importance(explainer_rf, loss_function = loss_root_mean_square)
vi_glm <- variable_importance(explainer_glm, loss_function = loss_root_mean_square)
vi_svm <- variable_importance(explainer_svm, loss_function = loss_root_mean_square)

plot(vi_rf, vi_glm, vi_svm)
```

In Figure \@ref(fig:ch12), the width of the interval bands (i.e., lines) corresponds to variable importance, while the bars indicate RMSE loss after permutations. Overall, the GLM model seems to have the lowest RMSE, whereas the RF model has the highest RMSE. The results also show that if we list the first two most influential variables on the outcome variable, grade and having school books seem to influence all three models significantly.

Another function that calculates the importance of variables using permutations is `model_parts()`. We will use the default loss_fuction - One minus AUC - and set show_boxplots = FALSE this time. Also, we limit the number of variables on the plot with max_vars to show make the plots more readable if there is a large number of predictors in the model. 

```{r ch13, echo=TRUE, fig.cap="Mean variable importance for some predictors"}
vip_rf  <- model_parts(explainer = explainer_rf,  B = 50, N = NULL)
vip_glm  <- model_parts(explainer = explainer_glm,  B = 50, N = NULL)
vip_svm <- model_parts(explainer = explainer_svm, B = 50, N = NULL)

plot(vip_rf, vip_glm, vip_svm, max_vars = 4, show_boxplots = FALSE) +
  ggtitle("Mean variable-importance over 50 permutations", "") 
```

After identifying the influential variables, we can show how the machine learning models perform based on different combinations of the predictors.

### Partial Dependence Plot

With the **DALEX** package, we can also create explainers that show the relationship between a predictor and model output through Partial Dependence Plots (PDP) and Accumulated Local Effects (ALE). These plots show whether or not the relationship between the outcome variable and a predictor is linear and how each predictor affects the prediction process. Therefore, these plots can be created for one predictor at a time. The `model_profile()` function with the parameter type = "partial" calculates PDP. We will use the grade variable to create a partial dependence plot.

```{r ch14, echo=TRUE, fig.cap="Partial dependence of grade in the models"}
pdp_rf <- model_profile(explainer_rf, variable = "grade", type = "partial")
pdp_glm <- model_profile(explainer_glm, variable = "grade", type = "partial")
pdp_svm <- model_profile(explainer_svm, variable = "grade", type = "partial")

plot(pdp_rf, pdp_glm, pdp_svm)
```

Figure \@ref(fig:ch14) can helps us understand how grade affects the classification of reading performance. The plot shows that the probability (see the y-axis) is low until grade 9 (see the x-axis) but then increases for all of the models. However, it decreases after grade 10 for the RF and SVM models.

### Accumulated Local Effects Plot

ALE plots are the extension of PDP, which is more suited for correlated variables. The `model_profile()` function with the parameter type = "accumulated" calculates the ALE curve. Compared with PDP plots, ALE plots are more useful because predictors in machine learning models are often correlated to some extent, and ALE plots take the correlations into account.

```{r ch15, echo=TRUE, fig.cap="Accumulated local effect of grade in the models"}
ale_rf  <- model_profile(explainer_rf, variable = "grade", type = "accumulated")
ale_glm  <- model_profile(explainer_glm, variable = "grade", type = "accumulated")
ale_svm  <- model_profile(explainer_svm, variable = "grade", type = "accumulated")

plot(ale_rf, ale_glm, ale_svm)
```

### Instance Level Explanation

Using **DALEX**, we can also see how the models behave for a single observation. We can select a particular observation from the data set or define a new observation. We investigate this using the `predict_parts()` function. This function is a special case of the `model_parts()`. It calculates the importance of the variables for a single observation while `model_parts()` calculates it for all observations in the data set.

We show this single observation level explanation by using the RF model. We could also create the plots for each model and compare the importance of a selected variable across the models. We will use an existing observation (i.e., student 1) from the testing data set.

```{r ch16, echo=TRUE, fig.cap="Prediction results for student 1"}
student1 <- test[1, 1:11]
pp_rf <- predict_parts(explainer_rf, new_observation = student1)

plot(pp_rf)
```

Figure \@ref(fig:ch16) shows that the prediction probability for the selected observation is 0.34. Also, grade seems to be the most important predictor. Next, we will define a hypothetical student and investigate how the RF model behaves for this student.

```{r ch17, echo=TRUE, fig.cap="Prediction results for a hypothetical student"}
new_student <- data.frame(gender = 0,
                          grade = 10,
                          computer = 0,
                          internet = 0,
                          desk=1,
                          own.room=1,
                          quiet.study=1,
                          book.sch = 1,
                          tech.book=1,
                          art.book=1)

pp_rf_new <- predict_parts(explainer_rf, new_observation = new_student)
plot(pp_rf_new)
```

For the new student we have defined, the most important variable that affects the prediction is computer. Setting type="shap", we can inspect the contribution of the predictors for a single observation.

```{r ch18, echo=TRUE, fig.cap="Contributions of the predictors to the prediction process"}
pp_rf_shap <- predict_parts(explainer_rf, new_observation = new_student, type = "shap")
plot(pp_rf_shap)
```

### Ceteris Paribus Profiles

In the previous section, we have discussed the PDP plots. Ceteris Paribus Profiles (CPP) is the single observation level version of the PDP plots. To create this plot, we can use `predict_profile()` function in the **DALEX** package. In the following example, we select two predictors for the same observation (i.e., student 1) and create a CPP plot for the RF model. In the plot, blue dots represent the original values for the selected observation.

```{r ch19, echo=TRUE, fig.cap="CPP plot for student 1"}
selected_variables <- c("grade", "gender")
cpp_rf <- predict_profile(explainer_rf, student1, variables = selected_variables)

plot(cpp_rf, variables = selected_variables)
```

## Conclusion

In this post, we wanted to demonstrate how to use data visualizations to evaluate the performance machine learning models beyond the conventional performance measures. Data visualization tools in the **DALEX** package enable residual diagnostics of the machine learning models, a comparison of variable importance, and a comprehensive evaluation of the relationship between each predictor and the outcome variable. Also, the package offers tools for visualizing the machine learning models based on a particular observation (either real or hypothetical). We hope that these features of the **DALEX** package will help you in the comparison and interpretation of machine learning models. More examples of **DALEX** are available on the **DALEX** authors' book [@dalexbook], which is available online at <http://ema.drwhy.ai/>.


