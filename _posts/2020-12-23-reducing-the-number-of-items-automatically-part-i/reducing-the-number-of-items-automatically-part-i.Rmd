---
title: "How to Shorten a Measurement Instrument Automatically (Part I)"
description: |
  Researchers conduct measurement invariance analysis to ensure that the interpretation of 
  latent construct(s) being measured with their measurement instruments (e.g., scales, surveys, 
  and questionnaires) are valid across subgroups of a target population or time points. 
  In this post, I demonstrate how to test for measurement invariance (i.e., configural, metric, scalar,
  and strict invariance) of an instrument using R.
  ```{r, include=FALSE}
  bytes <- file.size("reducing-the-number-of-items-automatically-part-i.Rmd")
  words <- bytes/10
  minutes <- words/200
  ``` 
  (`r round(minutes)` min read)
author:
  - name: Okan Bulut
    url: http://www.okanbulut.com/
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0001-5853-1267
date: 12-23-2020
categories:
  - psychometrics
  - data science
  - IRT
bibliography: reducingitems1.bib
preview: photo_unsplash2.jpg
output:
  distill::distill_article:
    self_contained: false
    toc: true
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
suppressWarnings({
library("rmarkdown")
library("kableExtra")
library("emo")
})
```

![Photo by [Maxime Agnelli](https://unsplash.com/@maxa) on [Unsplash](https://unsplash.com/)](photo_unsplash2.jpg)

## Introduction

Since the COVID-19 pandemic started, many academic institutions around the world have promptly shifted in-person educational activities to the remote learning format. With the mandatory transition to remote learning, educators need to develop and administer online assessments (or e-assessments) because students are not able to physically attend classes in order to avoid the spread of the virus. When assessing student learning with online assessments, educators are recommended to avoid long, heavily-weighted exams and instead use shorter exams more frequently throughout the semester [e.g., @kuhfeld2020projecting]. Although this sounds like a good idea in theory, it is easier said than done in practice. 

To build shorter exams, educators first need to determine which items should be removed from the exams. In addition, they need to ensure that the reliability and other psychometric qualities (e.g., content distribution) of the shortened exam are acceptable. However, making such adjustments manually could be a tedious and time-consuming task. In this two-part blog series, I want to demonstrate how to shorten exams (or, any measurement instrument) by automatically selecting the most appropriate items. 

**In Part I**, I will show how to employ automated test assembly and recursive feature elimination as alternative methods to automatically build shorter versions of educational assessments (e.g., multiple-choice exams, tests, and quizzes). 

**In Part II**, I will demonstrate how to use more advanced algorithms, such as the ant colony optimization (ACO; `r ji("ant")`) and genetic algorithm (GA; `r ji("gene")`), for creating short forms of other types of instruments (e.g., psychological scales).  


Let's get started `r emo::ji("biceps")`.


## Example

In this example, we will create a hypothetical assessment with 80 dichotomously-scored items. The items are associated with four content domains labeled as 'A', 'B', 'C', and 'D' (with 20 items per content domain). For the sake of simplicity, we will simulate the items based on the Rasch model. We will also assume that a sample of 500 examinees responded to the generated items. The ability distribution will be $\theta \sim N(0, 1)$. Finally, a response matrix with dichotomous responses will be generated based on the item and ability parameters. We will use the `xxIRT` package [@xxirt] for data generation.

```{r, echo=TRUE, eval=TRUE}
library("xxIRT")

# Generate item parameters, abilities, and responses
data <- model_3pl_gendata(
  n_p = 500, # number of examinees
  n_i = 80, # number of items
  t_dist = c(0, 1), # theta distribution as N(0, 1)
  #a_dist = c(-0.1, 0.2), # a parameter distribution
  a = 1, 
  b_dist = c(0, 0.7), # b parameter distribution
  c = 0 # fix c to zero (i.e., no guessing)
)

# Save the item parameters as a separate data set
items <- with(data, data.frame(id=paste0("item", 1:80), a=a, b=b, c=c))
items$content <- sample(LETTERS[1:4], 80, replace=TRUE)
```

Let's see the item bank that we have created.

```{r, echo=FALSE}
paged_table(items, options = list(cols.print = 10))
```

Based on the generated items, we can check out the test information function (TIF) for the entire item bank (i.e., 80 items).

```{r, echo=TRUE, eval=TRUE}
# Test information function
with(data, model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Finally, we will save the ability parameters and responses as separate data sets.

```{r, echo=TRUE, eval=TRUE}
# Ability
theta <- with(data, data.frame(theta = t))
hist(theta$theta, main = "Ability Distribution", xlab = "Theta")

# Responses
resp <- with(data, as.data.frame(u))
names(resp) <- paste0("item", 1:80)
```

```{r, echo=FALSE}
paged_table(resp, options = list(cols.print = 10))
```

### Automatic Selection of Items

**Goal:** Assume that using the item bank (i.e., full test) generated above, we want to create a short form with only 20 items. 

**Conditions:** In the short form, we want to maintain the same content distribution. Therefore, we will have 5 items from each content domain (i.e., A, B, C, and D). Furthermore, we want the short form to resemble the full test in terms of reliability and test scores. 

**Methodology:** With the traditional test assembly approach, we would go through all of the items one by one and pick the appropriate ones based on our judgment. However, this would be highly laborious and inefficient in practice. Therefore, we will use two approaches to automatically select the items:

1. Automated test assembly as a psychometric approach

2. Recursive feature elimination as a data science approach

### Automated Test Assembly

Automated test assembly (or shortly, ATA) is a mathematical optimization approach that allows us to automatically select items from a large item bank (or, item pool) based on pre-defined psychometric, content, and test administration features^[You can check out @van2006linear's "Linear Models for Optimal Test Design" for further information on ATA.]. To solve an ATA task, we can use either a mixed integer programming (MIP) algorithm or a heuristic algorithm. In this example, we will use MIP to look for the most optimal test form that meets the psychometric and content requirements (i.e., constraints) that we have identified. 

To utilize MIP, we need a solver that will look for the optimal solution based on an objective function (e.g., maximizing test information function) and a set of constraints. In this example, we will use **ata_obj_relative** to maximize the test information between $\theta=-0.5$ and $\theta=0.5$ to mimic the TIF distribution from the full test (i.e,, 80 items). In addition, we will use **ata_constraint** to select five items from each content domain (i.e., A, B, C, and D). For the MIP solver, we will select [lp_solve](http://lpsolve.sourceforge.net/5.5/), which is already included in the `xxIRT` package. 

```{r, echo=TRUE, eval=TRUE}
# Define the ATA problem
x <- ata(items, 1, len = 20, max_use = 1)

# Identify the objective function
x <- ata_obj_relative(x, seq(-0.5, 0.5, .5), 'max')

# Set the content constraints
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "A")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "B")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "C")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "D")

# Solve the optimization problem
x <- ata_solve(x, 'lpsolve')
```

Once the test assembly is complete, we can see which items have been selected by ATA.

```{r, echo=TRUE, eval=TRUE}
# Selected items
print(x$items)
```

In addition, we can draw a TIF plot to see whether the TIF distribution for the short form is similar to the one from the full test. 

```{r, echo=TRUE, eval=TRUE}
# TIF for the selected items
with(x$items[[1]], 
     model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Now let's check the reliability of our new shortened form as well as the correlation among raw scores (i.e., scores from the whole test vs. scores from the shortened test). We will use the **alpha** function from the `psych` package [@psych] to compute coefficient alpha for the shortened test. 

```{r, echo=TRUE, eval=TRUE}
items_ATA <- x$items[[1]]$id

# Coefficient alpha
psych::alpha(resp[,c(items_ATA)])$total

# Correlation between raw scores
score <- rowSums(resp)
score_ATA <- rowSums(resp[,c(items_ATA)])
cor(score, score_ATA)  

plot(score_ATA, score, xlab = "Short Form (ATA)", ylab = "Full Test")
abline(lm(score ~ score_ATA), col = "red", lty = 2, lwd = 2)
```

### Recursive Feature Elimination

Recursive feature elimination (or shortly, RFE).

To implement RFE for item selection, we will use the `randomForest` [@rf] and `caret` [@caret] packages in R. 

```{r, echo=TRUE, eval=TRUE}
library("caret")
library("randomForest")

# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, 
                      method = "cv", # cross-validation
                      number = 10) # the number of folds

# Run random forest for each content domain
results_A <- rfe(x = resp[,items[items$content=="A","id"]], 
                 y = score, 
                 sizes = 5, 
                 rfeControl = control)

results_B <- rfe(x = resp[,items[items$content=="B","id"]], 
                 y = score, 
                 sizes = 5, 
                 rfeControl = control)

results_C <- rfe(x = resp[,items[items$content=="C","id"]], 
                 y = score, 
                 sizes = 5, 
                 rfeControl = control)

results_D <- rfe(x = resp[,items[items$content=="D","id"]], 
                 y = score, 
                 sizes = 5, 
                 rfeControl = control)

items_RFE <- c(predictors(results_A)[1:5],
               predictors(results_B)[1:5],
               predictors(results_C)[1:5],
               predictors(results_D)[1:5])

# Selected items
print(items[items$id %in% items_RFE, ])

# TIF for the selected items
with(items[items$id %in% items_RFE, ], 
     model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Also, we will check the reliability of the shortened form and correlations among raw scores for the RFE method. 

```{r, echo=TRUE, eval=TRUE}
# Coefficient alpha
psych::alpha(resp[,c(items_RFE)])$total

# Correlation between raw scores
score_RFE <- rowSums(resp[,c(items_RFE)])
cor(score, score_RFE)  

plot(score_RFE, score, xlab = "Short Form (RFE)", ylab = "Full Test")
abline(lm(score ~ score_RFE), col = "red", lty = 2, lwd = 2)
```


## Conclusion

Xxx. 












