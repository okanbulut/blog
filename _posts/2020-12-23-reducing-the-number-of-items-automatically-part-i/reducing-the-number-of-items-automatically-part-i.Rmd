---
title: "How to Shorten a Measurement Instrument Automatically (Part I)"
description: |
  Reducing the number of items in an existing measurement instrument (e.g., tests, surveys,
  questionnaires) is almost as tedious as creating a new instrument. Going through all of 
  the items one by one and choosing the appropriate ones based on our judgment could highly 
  laborious and inefficient. In this two-part series, I will demonstrate how to shorten a
  measurement instrument automatically in R. The first part focuses on a psychometric method 
  (automated test assembly) and a data-driven method (recursive feature elimination) for
  automatic selection of items.
  ```{r, include=FALSE}
  bytes <- file.size("reducing-the-number-of-items-automatically-part-i.Rmd")
  words <- bytes/10
  minutes <- words/200
  ``` 
  (`r round(minutes)` min read)
author:
  - name: Okan Bulut
    url: http://www.okanbulut.com/
    affiliation: University of Alberta
    affiliation_url: https://www.ualberta.ca
    orcid_id: 0000-0001-5853-1267
date: 01-05-2021
categories:
  - psychometrics
  - machine learning
  - test development
bibliography: reducingitems1.bib
preview: photo_unsplash2.jpg
output:
  distill::distill_article:
    self_contained: false
    toc: true
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE)
suppressWarnings({
library("rmarkdown")
library("kableExtra")
library("emo")
})
```

![Photo by [Maxime Agnelli](https://unsplash.com/@maxa) on [Unsplash](https://unsplash.com/)](photo_unsplash2.jpg)

## Introduction

Since the COVID-19 pandemic started, many academic institutions around the world have promptly shifted in-person educational activities to the remote learning format. With the mandatory transition to remote learning, educators need to develop and administer online assessments (or e-assessments) because students are not able to physically attend classes in order to avoid the spread of the virus. When assessing student learning with online assessments, educators are recommended to avoid long, heavily-weighted exams and instead use shorter exams more frequently throughout the semester [e.g., @kuhfeld2020projecting]. Although this sounds like a good idea in theory, it is easier said than done in practice. 

To build shorter exams, educators first need to determine which items should be removed from the exams. In addition, they need to ensure that the reliability and other psychometric qualities (e.g., content distribution) of the shortened exam are acceptable. However, making such adjustments manually could be a tedious and time-consuming task. In this two-part series, I want to demonstrate how to shorten exams (or, any measurement instrument) by automatically selecting the most appropriate items. 

**In Part I**, I will show how to utilize automated test assembly and recursive feature elimination as alternative methods to automatically build shorter versions of educational assessments (e.g., multiple-choice exams, tests, and quizzes). 

**In Part II**, I will demonstrate how to use more advanced algorithms, such as the ant colony optimization (ACO; `r ji("ant")`) and genetic algorithm (GA; `r ji("gene")`), for creating short forms of other types of instruments (e.g., psychological scales and surveys).  

Let's get started `r emo::ji("biceps")`.


## Example

In this example, we will create a hypothetical exam with 80 dichotomously-scored items (i.e., 0 = Incorrect, 1 = Correct). The items are associated with four content domains labeled as 'A', 'B', 'C', and 'D' (with 20 items per content domain). For the sake of simplicity, we will simulate the items based on the Rasch model. We will also assume that a sample of 500 examinees responded to the generated items thus far. The ability distribution will be $\theta \sim N(0, 1)$. Finally, a response matrix with dichotomous responses will be generated based on the item and ability parameters. We will use the `xxIRT` package [@xxirt] for data generation.

```{r, echo=TRUE, eval=TRUE}
library("xxIRT")

set.seed(2021)

# Generate item parameters, abilities, and responses
data <- model_3pl_gendata(
  n_p = 500, # number of examinees
  n_i = 80, # number of items
  t_dist = c(0, 1), # theta distribution as N(0, 1)
  a = 1, # fix discrimination to 1
  b_dist = c(0, 0.7), # b parameter distribution
  c = 0 # fix c to zero (i.e., no guessing)
)

# Save the item parameters as a separate data set
items <- with(data, data.frame(id=paste0("item", 1:80), a=a, b=b, c=c))

# Randomly assign four content domains (A, B, C, or D) to the items
items$content <- sample(LETTERS[1:4], 80, replace=TRUE)
```

Let's see the item bank that we have created.

```{r, echo=FALSE}
paged_table(items, options = list(cols.print = 10))
```

Based on the generated items, we can check out the test information function (TIF) for the entire item bank (i.e., 80 items).

```{r, echo=TRUE, eval=TRUE}
# Test information function
with(data, model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Finally, we will save the ability parameters and responses as separate data sets.

```{r, echo=TRUE, eval=TRUE}
# Ability
theta <- with(data, data.frame(theta = t))
hist(theta$theta, main = "Ability Distribution", xlab = "Theta")

# Responses
resp <- with(data, as.data.frame(u))
names(resp) <- paste0("item", 1:80)
```

```{r, echo=FALSE}
paged_table(resp, options = list(cols.print = 10))
```

### Automatic Selection of Items

**Goal:** Assume that using the item bank (i.e., full test with 80 items) generated above, we want to create a short form with only 20 items.

**Assumptions:** We will assume that: (1) all of the items on the test measure the same latent trait (i.e., the test is unidimensional); (2) the short form will be used for the same examinee population; and (3) the test does not exhibit any psychometric issues (e.g., mode effect, context effect, or differential item functioning).

**Conditions:** In the short form, we want to maintain the same content distribution (i.e., equal number of items from each content domain). Therefore, we will have five items from each content domain (i.e., A, B, C, and D). Furthermore, we want the short form to resemble the full test in terms of reliability and (raw) test scores. 

**Methodology:** With the traditional test assembly approach, we would go through all of the items one by one and pick the appropriate ones based on our judgment. However, this would be highly laborious and inefficient in practice. Therefore, we will use two approaches to automatically select the items:

1. Automated test assembly as a psychometric approach (IRT parameters are required)^[Item response theory (IRT) is a psychometric framework for the design, analysis, and scoring of tests, questionnaires, and similar instruments. If you are not familiar with IRT, you can move to the second approach: Recursive feature elimination.]

2. Recursive feature elimination as a data-driven approach (raw responses are required)

### Automated Test Assembly

Automated test assembly (or shortly, ATA) is a mathematical optimization approach that allows us to automatically select items from a large item bank (or, item pool) based on pre-defined psychometric, content, and test administration features^[You can check out @van2006linear's "Linear Models for Optimal Test Design" for further information on ATA.]. To solve an ATA task, we can use either a mixed integer programming (MIP) algorithm or a heuristic algorithm. In this example, we will use MIP to look for the optimal test form that meets the psychometric and content requirements (i.e., constraints) that we have identified. 

To utilize MIP, we need a solver that will look for the optimal solution based on an objective function (e.g., maximizing test information function) and a set of constraints. In this example, we will use **ata_obj_relative** to maximize the test information between $\theta=-0.5$ and $\theta=0.5$ to mimic the TIF distribution from the full test (i.e,, 80 items). In addition, we will use **ata_constraint** to select five items from each content domain (i.e., A, B, C, and D). For the MIP solver, we will select [lp_solve](http://lpsolve.sourceforge.net/5.5/), which is already included in the `xxIRT` package. 

```{r, echo=TRUE, eval=TRUE}
# Define the ATA problem
x <- ata(items, 1, len = 20, max_use = 1)

# Identify the objective function
x <- ata_obj_relative(x, seq(-0.5, 0.5, .5), 'max')

# Set the content constraints
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "A")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "B")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "C")
x <- ata_constraint(x, 'content', min = 5, max = 5, level = "D")

# Solve the optimization problem
x <- ata_solve(x, 'lpsolve')
```

Once the test assembly is complete, we can see which items have been selected by ATA.

```{r, echo=TRUE, eval=TRUE}
# Selected items
print(x$items)
```

In addition, we can draw a plot to see whether the TIF distribution for the short form is similar to the one from the full test. 

```{r, echo=TRUE, eval=TRUE}
# TIF for the selected items
with(x$items[[1]], 
     model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Now let's check the reliability of our new shortened form as well as the correlation among raw scores (i.e., scores from the whole test vs. scores from the shortened test). We will use the **alpha** function from the `psych` package [@psych] to compute coefficient alpha for the shortened test. 

```{r, echo=TRUE, eval=TRUE}
items_ATA <- x$items[[1]]$id

# Coefficient alpha
psych::alpha(resp[,c(items_ATA)])$total

# Correlation between raw scores
score <- rowSums(resp)
score_ATA <- rowSums(resp[,c(items_ATA)])
cor(score, score_ATA)  
```

This is a very high correlation. Let's also check the scatterplot of the raw scores from the full test (out of 80 points) and the short form (out of 20 points). 

```{r, echo=TRUE, eval=TRUE}
# Scatter plot of raw scores
plot(score_ATA, score, xlab = "Short Form (ATA)", ylab = "Full Test")
abline(lm(score ~ score_ATA), col = "red", lty = 2, lwd = 2)
```


### Recursive Feature Elimination

The term *feature selection* may sound unfamiliar to those of us who have originally studied psychometrics but it is one of the core concepts for data scientists dealing data mining, predictive modeling, and machine learning. 

> *Feature selection* is a process of automatic selection of a subset of relevant features or variables from a set of all features, used in the process of model building. ([Dataaspirant](https://dataaspirant.com/feature-selection-methods-machine-learning/))

The primary goal of feature selection is to select the most important predictors by filtering out irrelevant or partially relevant predictors. By selecting the most important predictors, we can build a simple and yet accurate model and avoid problems such as overfitting. There are a number of methods for both supervised and unsupervised feature selection. [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by @kuhn2019feature is a great resource for those who want to learn more about feature selection^[Max Kuhn is also the author of the famous `caret` package]. 

In this example, we will use *recursive feature elimination* (or shortly, RFE) for automatically selecting items. As a greedy wrapper method, RFE applies backwards selection to find the optimal combination of features (i.e., predictors). First, it builds a model based on the entire set of predictors. Then, it removes predictors with the least importance iteratively until a smaller subset of predictors are retained in the model^[See <http://www.feat.engineering/recursive-feature-elimination.html> for more information on RFE]. In our example, we will use the full set of items (i.e., 80 items) to predict raw scores. RFE will help us eliminate the items that may not be important for the prediction of raw scores^[This could also be used for predicting person ability estimates in IRT or another external criterion]. 

To implement RFE for automatic item selection, we will use the `randomForest` [@rf] and `caret` [@caret] packages in R. 

```{r, echo=TRUE, eval=TRUE}
library("caret")
library("randomForest")
```

The **rfe** function the from `caret` requires four parameters:

* **x**: A matrix or data frame of features (i.e., predictors)
* **y**: The outcome variable to be predicted
* **sizes**: The number of features that should be retained in the feature selection process
* **rfeControl**: A list of control options for the feature selection algorithms

Before moving to **rfe**, we first need to set up the options for **rfeControl**. The `caret` package includes a number of functions, such as random forest, naive Bayes, bagged trees, and  linear regression^[See <https://topepo.github.io/caret/recursive-feature-elimination.html> for further information]. In this example, we will use the *random forest* algorithm for model estimation because random forest includes an effective mechanism for measuring feature importance [@kuhn2019feature]: **functions = rfFuncs**. In addition, we will add 10-fold cross-validation by using **method = "cv"** and **number = 10**. 

```{r, echo=TRUE, eval=TRUE}
# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "cv", # cross-validation
                      number = 10) # the number of folds
```

Next, we will run the **rfe** function with the options we have identified above. For this step, we could simply include all of the items together in a single model and select the best 20 items at the end. However, this process may not yield five items from each content domain, which is the content constraint we have specified for our short form. Therefore, we will apply the **rfe** function to each content domain separately, use **sizes = 5** to retain five items from each content domain, and then combine the results at the end.

```{r, echo=TRUE, eval=TRUE}
# Run RFE for each content domain with a loop
result <- list() 

for(i in c("A", "B", "C", "D")) {
  result[[i]] <- rfe(x = resp[,items[items$content==i,"id"]], 
                     y = score, 
                     sizes = 5, 
                     rfeControl = control)
}

# Extract the results (i.e., first 5 predictors for each content domain)
items_RFE <- unlist(lapply(result, function(x) predictors(x)[1:5]))
```

In the following section, we will view the selected items and then draw a TIF plot for the selected items. 

```{r, echo=TRUE, eval=TRUE}
# Selected items
print(items[items$id %in% items_RFE, ])

# TIF for the selected items
with(items[items$id %in% items_RFE, ], 
     model_3pl_plot(a, b, c, type="info", total = TRUE))
```

Also, we will check the reliability of the short form and correlations among raw scores for the RFE method. 

```{r, echo=TRUE, eval=TRUE}
# Coefficient alpha
psych::alpha(resp[,c(items_RFE)])$total

# Correlation between raw scores
score_RFE <- rowSums(resp[,c(items_RFE)])
cor(score, score_RFE)  
```

As for the ATA method, the correlation between the raw scores is very high for the RFE method. We will also check out the scatterplot of the raw scores.

```{r, echo=TRUE, eval=TRUE}
# Scatter plot of raw scores
plot(score_RFE, score, xlab = "Short Form (RFE)", ylab = "Full Test")
abline(lm(score ~ score_RFE), col = "red", lty = 2, lwd = 2)
```

Lastly, we can compute percent-correct scores for the full test and the short forms generated with ATA and RFE to make an overall score comparison. We will use the `ggplot2` package [@ggplot2] to draw a violin plot and check out the score distributions. The violin plot shows that the score distributions are not exactly the same but they are pretty similar.

```{r, echo=TRUE, eval=TRUE}
library("ggplot2")

# Compute percent-correct scores
score_all <- data.frame(
  test = as.factor(c(rep("Full Test", 500), rep("ATA", 500), rep("RFE", 500))),
  score = c(score/80*100, score_ATA/20*100, score_RFE/20*100)
)

# Make "full test" the first category of the test variable (optional)
score_all$test <- relevel(score_all$test, "Full Test")

# Draw a violin plot (with a boxplot inside)
ggplot(data = score_all, aes(x = test, y = score)) +
  geom_violin(aes(fill = test), trim = FALSE) + 
  geom_boxplot(width = 0.2)+
  scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07")) +
  labs(x = "", y = "Percent-Correct Scores") +
  theme(legend.position="none")
```


## Conclusion

In our example, both methods (i.e., ATA and RFE) appear to yield very similar (and very good!) results in terms of reliability and correlations between raw scores, although they recommended different sets of items for the short form. This may not be a surprising finding because we generated a nice data set (e.g., no missing responses, no outliers, etc.) based on a psychometric model (i.e., Rasch). ATA and RFE may produce less desirable results if:

* the sample size is smaller,
* non-random missingness is present in the response data set, and
* there are problematic items (e.g., items with low discrimination or high guessing).



